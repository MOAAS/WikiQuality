Id,Title,Databases,Year,Authors,Publication Type,Published In,Publisher,Address,Refs.,Cits.,Abstract,Keywords,PDF,URL,"Q1 [0, 3]","Q2 [0, 3]","Q3 [0, 3]","Q4 [0, 1]","Total [0, 10]",Backward Tracked,Forward Tracked
1,Measuring article quality in wikipedia: models and evaluation,"ACM, Google Scholar",2007,Meiqun Hu; Ee-Peng Lim; Aixin Sun; Hady Wirawan Lauw; Ba-Quy Vuong,Conference,"CIKM '07: International Conference on Information and Knowledge Management, pp. 243-252",Association for Computing Machinery,"New York City, United States",29,360,"Wikipedia has grown to be the world largest and busiest free encyclopedia, in which articles are collaboratively written and maintained by volunteers online. Despite its success as a means of knowledge sharing and collaboration, the public has never stopped criticizing the quality of Wikipedia articles edited by non-experts and inexperienced contributors. In this paper, we investigate the problem of assessing the quality of articles in collaborative authoring of Wikipedia. We propose three article quality measurement models that make use of the interaction data between articles and their contributors derived from the article edit history. Our Basic model is designed based on the mutual dependency between article quality and their author authority. The PeerReview model introduces the review behavior into measuring article quality. Finally, our ProbReview models extend PeerReview with partial reviewership of contributors as they edit various portions of the articles. We conduct experiments on a set of well-labeled Wikipedia articles to evaluate the effectiveness of our quality measurement models in resembling human judgement.",Wikipedia; article quality; collaborative authoring; authority; peer review,https://dl.acm.org/doi/pdf/10.1145/1321440.1321476,https://dl.acm.org/doi/10.1145/1321440.1321476,2,0,1,0,3,No,No
2,Cooperation and quality in wikipedia,"ACM, Google Scholar",2007,Dennis Wilkinson; Bernardo Huberman,Conference,"WikiSym '07: International Symposium on Wikis, pp. 157-164",Association for Computing Machinery,"New York City, United States",34,390,"The rise of the Internet has enabled collaboration and cooperation on anunprecedentedly large scale. The online encyclopedia Wikipedia, which presently comprises 7.2 million articles created by 7.04 million distinct editors, provides a consummate example. We examined all 50 million edits made tothe 1.5 million English-language Wikipedia articles and found that the high-quality articles are distinguished by a marked increase in number of edits, number of editors, and intensity of cooperative behavior, as compared to other articles of similar visibility and age. This is significant because in other domains, fruitful cooperation has proven to be difficult to sustain as the size of the collaboration increases. Furthermore, in spite of the vagaries of human behavior, we show that Wikipedia articles accrete edits according to a simple stochastic mechanism in which edits beget edits. Topics of high interest or relevance are thus naturally brought to the forefront of quality.",Cooperation; Wikipedia,https://dl.acm.org/doi/pdf/10.1145/1296951.1296968,https://dl.acm.org/doi/10.1145/1296951.1296968,1,0,1,0,2,No,No
3,Who does what: Collaboration patterns in the wikipedia and their impact on article quality,"ACM, Google Scholar",2011,Jun Liu; Sudha Ram,Journal,"ACM Transactions on Management Information Systems, vol. 2(2), no. 11, pp. 1-23",N/A,N/A,52,172,"The quality of Wikipedia articles is debatable. On the one hand, existing research indicates that not only are people willing to contribute articles but the quality of these articles is close to that found in conventional encyclopedias. On the other hand, the public has never stopped criticizing the quality of Wikipedia articles, and critics never have trouble finding low-quality Wikipedia articles. Why do Wikipedia articles vary widely in quality? We investigate the relationship between collaboration and Wikipedia article quality. We show that the quality of Wikipedia articles is not only dependent on the different types of contributors but also on how they collaborate. Based on an empirical study, we classify contributors based on their roles in editing individual Wikipedia articles. We identify various patterns of collaboration based on the provenance or, more specifically, who does what to Wikipedia articles. Our research helps identify collaboration patterns that are preferable or detrimental for article quality, thus providing insights for designing tools and mechanisms to improve the quality of Wikipedia articles.",Wikipedia; collaboration pattern; article quality,https://dl.acm.org/doi/pdf/10.1145/1985347.1985352,https://dl.acm.org/doi/10.1145/1985347.1985352,1,0,1,0,2,No,No
4,Size matters: word count as a measure of quality on wikipedia,"ACM, Google Scholar",2008,Joshua E. Blumenstock,Conference,"WWW '08: The Web Conference, pp. 1095-1096",Association for Computing Machinery,"New York City, United States",6,379,"Wikipedia, ""the free encyclopedia"", now contains over two million English articles, and is widely regarded as a high-quality, authoritative encyclopedia. Some Wikipedia articles, however, are of questionable quality, and it is not always apparent to the visitor which articles are good and which are bad. We propose a simple metric -- word count -- for measuring article quality. In spite of its striking simplicity, we show that this metric significantly outperforms the more complex methods described in related work.",Wikipedia; information quality; word count,https://dl.acm.org/doi/pdf/10.1145/1367497.1367673,https://dl.acm.org/doi/10.1145/1367497.1367673,3,2,2,0,7,Yes,Yes
6,Determinants of wikipedia quality: the roles of global and local contribution inequality,"ACM, Google Scholar, Web of Science",2010,Ofer Arazy; Oded Nov,Conference,"CSCW '10: Conference on Computer Supported Cooperative Work, pp. 233-236",Association for Computing Machinery,"New York City, United States",21,114,"The success of Wikipedia and the relative high quality of its articles seem to contradict conventional wisdom. Recent studies have begun shedding light on the processes contributing to Wikipedia's success, highlighting the role of coordination and contribution inequality. In this study, we expand on these works in two ways. First, we make a distinction between global (Wikipedia-wide) and local (article-specific) inequality and investigate both constructs. Second, we explore both direct and indirect effects of these inequalities, exposing the intricate relationships between global inequality, local inequality, coordination, and article quality. We tested our hypotheses on a sample of a Wikipedia articles using structural equation modeling and found that global inequality exerts significant positive impact on article quality, while the effect of local inequality is indirect and is mediated by coordination",Wikipedia; quality; contributing inequality; coordination,https://dl.acm.org/doi/pdf/10.1145/1718918.1718963,https://dl.acm.org/doi/10.1145/1718918.1718963,1,0,1,0,2,No,No
8,On measuring the quality of Wikipedia articles,"ACM, Google Scholar",2010,Gabriel De la Calzada; Alex Dekhtyar,Conference,"WICOU '10: Workshop on Information Credibility on the Web, pp. 11-18",Association for Computing Machinery,"New York City, United States",33,80,"This paper discusses an approach to modeling and measuring information quality of Wikipedia articles. The approach is based on the idea that the quality of Wikipedia articles with distinctly different profiles needs to be measured using different information quality models. We report on our initial study, which involved two categories of Wikipedia articles: ""stabilized"" (those, whose content has not undergone major changes for a significant period of time) and ""controversial"" (the articles, which have undergone vandalism, revert wars, or whose content is subject to internal discussions between Wikipedia editors). We present simple information quality models and compare their performance on a subset of Wikipedia articles with the information quality evaluations provided by human users. Our experiment shows, that using special-purpose models for information quality captures user sentiment about Wikipedia articles better than using a single model for both categories of articles.",Wikipedia; quality; models,https://dl.acm.org/doi/pdf/10.1145/1772938.1772943,https://dl.acm.org/doi/10.1145/1772938.1772943,2,0,1,0,3,No,No
10,Assessing the quality of Wikipedia articles with lifecycle based metrics,"ACM, Google Scholar",2009,Thomas WÃ¶hner; Ralf Peters,Conference,"WikiSym '09: International Symposium on Wikis and Open Collaboration, no. 16, pp. 1-10",Association for Computing Machinery,"New York City, United States",25,140,"The main feature of the free online-encyclopedia Wikipedia is the wiki-tool, which allows viewers to edit the articles directly in the web browser. As a weakness of this openness for example the possibility of manipulation and vandalism cannot be ruled out, so that the quality of any given Wikipedia article is not guaranteed. Hence the automatic quality assessment has been becoming a high active research field. In this paper we offer new metrics for an efficient quality measurement. The metrics are based on the lifecycles of low and high quality articles, which refer to the changes of the persistent and transient contributions throughout the entire life span.",Wikipedia; quality assessment; Wikipedia lifecycle; transient contribution; persistent contribution,https://dl.acm.org/doi/pdf/10.1145/1641309.1641333,https://dl.acm.org/doi/10.1145/1641309.1641333,2,0,1,1,4,Yes,Yes
11,Statistical measure of quality in Wikipedia,"ACM, Google Scholar",2010,Sara Javanmardi; Cristina Lopes,Conference,"SOMA '10: Workshop on Social Media Analytics, pp. 132-138",Association for Computing Machinery,"New York City, United States",26,58,"Wikipedia is commonly viewed as the main online encyclopedia. Its content quality, however, has often been questioned due to the open nature of its editing model. A high--quality contribution by an expert may be followed by a low-quality contribution made by an amateur or a vandal; therefore the quality of each article may fluctuate over time as it goes through iterations of edits by different users. With the increasing use of Wikipedia, the need for a reliable assessment of the quality of the content is also rising. In this study, we model the evolution of content quality in Wikipedia articles in order to estimate the fraction of time during which articles retain high-quality status. To evaluate the model, we assess the quality of Wikipedia's featured and non-featured articles. We show how the model reproduces consistent results with what is expected. As a case study, we use the model in a CalSWIM mashup the content of which is taken from both highly reliable sources and Wikipedia, which may be less so. Integrating CalSWIM with a trust management system enables it to use not only recency but also quality as its criteria, and thus filter out vandalized or poor-quality content.",Wikipedia; Wiki; Crowdsourcing; Web 2.0,https://dl.acm.org/doi/pdf/10.1145/1964858.1964876,https://dl.acm.org/doi/10.1145/1964858.1964876,2,0,1,0,3,No,No
12,Information quality discussions in wikipedia,Google Scholar,2005,Besiki Stvilia; Michael B. Twidale; Les Gasser; Linda C. Smith,Conference,"ICKM '05: International Conference on Knowledge Management, pp. ?-?",Universiti Putra Malaysia,"Seri Kembangan, Malaysia",37,168,"We examine the Information Quality aspects of Wikipedia. By a study of the discussion pages and other process-oriented pages within the Wikipedia project, it is possible to determine the information quality dimensions that participants in the editing process care about, how they talk about them, what tradeoffs they make between these dimensions and how the quality assessment and improvement process operates. This analysis helps in understanding how high quality is maintained in a project where anyone may participate with no prior vetting. It also carries implications for improving the quality of more conventional datasets.",N/A,https://myweb.fsu.edu/bstvilia/papers/qualWiki.pdf,https://www.researchgate.net/publication/200773232_Information_Quality_Discussions_in_Wikipedia,2,0,1,0,3,No,No
13,Tell me more: an actionable quality model for Wikipedia,"ACM, Google Scholar",2013,Morten Warncke-Wang; Dan Cosley; John Riedl,Conference,"WikiSym '13: International Symposium on Open Collaboration, no. 8, pp. 1-10",Association for Computing Machinery,"New York City, United States",40,126,"In this paper we address the problem of developing actionable quality models for Wikipedia, models whose features directly suggest strategies for improving the quality of a given article. We first survey the literature in order to understand the notion of article quality in the context of Wikipedia and existing approaches to automatically assess article quality. We then develop classification models with varying combinations of more or less actionable features, and find that a model that only contains clearly actionable features delivers solid performance. Lastly we discuss the implications of these results in terms of how they can help improve the quality of articles across Wikipedia.",Wikipedia; Information Quality; Modelling; Classification; Flaw Detection; Machine Learning,https://dl.acm.org/doi/pdf/10.1145/2491055.2491063,https://dl.acm.org/doi/10.1145/2491055.2491063,3,2,1,0,6,Yes,Yes
14,Automatic quality assessment of content created collaboratively by web communities: a case study of wikipedia,"ACM, Google Scholar, Web of Science",2009,Daniel Hasan Dalip; Marcos AndrÃ© GonÃ§alves; Marco Cristo; PÃ¡vel Calado,Conference,"JCDL '09: ACM/IEEE Joint Conference on Digital Libraries, pp. 295-304",Association for Computing Machinery,"New York City, United States",32,150,"The old dream of a universal repository containing all the human knowledge and culture is becoming possible through the Internet and the Web. Moreover, this is happening with the direct collaborative, participation of people. Wikipedia is a great example. It is an enormous repository of information with free access and edition, created by the community in a collaborative manner. However, this large amount of information, made available democratically and virtually without any control, raises questions about its relative quality. In this work we explore a significant number of quality indicators, some of them proposed by us and used here for the first time, and study their capability to assess the quality of Wikipedia articles. Furthermore, we explore machine learning techniques to combine these quality indicators into one single assessment judgment. Through experiments, we show that the most important quality indicators are the easiest ones to extract, namely, textual features related to length, structure and style. We were also able to determine which indicators did not contribute significantly to the quality assessment. These were, coincidentally, the most complex features, such as those based on link analysis. Finally, we compare our combination method with state-of-the-art solution and show significant improvements in terms of effective quality prediction.",Quality Assessment; Wikipedia; Machine Learning; SVM,https://dl.acm.org/doi/pdf/10.1145/1555400.1555449,https://dl.acm.org/doi/10.1145/1555400.1555449,3,1,3,0,7,Yes,Yes
15,Learning to Predict the Quality of Contributions to Wikipedia,Google Scholar,2008,Gregory Druck; Gerome Miklau; Andrew McCallum,N/A,N/A,N/A,N/A,11,67,"Although some have argued that Wikipedia's open edit policy is one of the primary reasons for its success, it also raises concerns about quality --- vandalism, bias, and errors can be problems. Despite these challenges, Wikipedia articles are often (perhaps surprisingly) of high quality, which many attribute to both the dedicated Wikipedia community and ``good Samaritan"" users. As Wikipedia continues to grow, however, it becomes more difficult for these users to keep up with the increasing number of articles and edits. This motivates the development of tools to assist users in creating and maintaining quality. In this paper, we propose metrics that quantify the quality of contributions to Wikipedia through implicit feedback from the community. We then learn discriminative probabilistic models that predict the quality of a new edit using features of the changes made, the author of the edit, and the article being edited. Through estimating parameters for these models, we also gain an understanding of factors that influence quality. We advocate using edit quality predictions and information gleaned from model analysis not to place restrictions on editing, but to instead alert users to potential quality problems, and to facilitate the development of additional incentives for contributors. We evaluate the edit quality prediction models on the Spanish Wikipedia. Experiments demonstrate that the models perform better when given access to content-based features of the edit, rather than only features of contributing user. This suggests that a user-based solution to the Wikipedia quality problem may not be sufficient.",N/A,https://www.academia.edu/158338/Learning_to_Predict_the_Quality_of_Contributions_to_Wikipedia,https://maroo.cs.umass.edu/getpdf.php?id=834,1,0,1,1,3,No,No
16,Measuring Quality of Collaboratively Edited Documents: The Case of Wikipedia,"Google Scholar, Web of Science",2016,Quang-Vinh Dang; Claudia-Lavinia Ignat,Conference,"CIC '16: IEEE 2nd International Conference on Collaboration and Internet Computing, pp. 266-275",Institute of Electrical and Electronic Engineers,"New York City, United States",62,45,"Wikipedia is a great example of large scale collaboration, where people from all over the world together build the largest and maybe the most important human knowledge repository in the history. However, a number of studies showed that the quality of Wikipedia articles is not equally distributed. While many articles are of good quality, many others need to be improved. Assessing the quality of Wikipedia articles is very important for guiding readers towards articles of high quality and suggesting authors and reviewers which articles need to be improved. Due to the huge size of Wikipedia, an effective automatic assessment method to measure Wikipedia articles quality is needed. In this paper, we present an automatic assessment method of Wikipedia articles quality by analyzing their content in terms of their format features and readability scores. Our results show improvements both in terms of accuracy and information gain compared with other existing approaches.",N/A,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7809715,https://ieeexplore.ieee.org/document/7809715,3,3,2,0,8,Yes,Yes
17,Predicting quality flaws in user-generated content: the case of wikipedia,"ACM, Google Scholar, Web of Science",2012,Maik Anderka; Benno Stein; Nedim Lipka,Conference,"SIGIR '12: International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 981-990",Association for Computing Machinery,"New York City, United States",39,109,"The detection and improvement of low-quality information is a key concern in Web applications that are based on user-generated content; a popular example is the online encyclopedia Wikipedia. Existing research on quality assessment of user-generated content deals with the classification as to whether the content is high-quality or low-quality. This paper goes one step further: it targets the prediction of quality flaws, this way providing specific indications in which respects low-quality content needs improvement. The prediction is based on user-defined cleanup tags, which are commonly used in many Web applications to tag content that has some shortcomings. We apply this approach to the English Wikipedia, which is the largest and most popular user-generated knowledge source on the Web. We present an automatic mining approach to identify the existing cleanup tags, which provides us with a training corpus of labeled Wikipedia articles. We argue that common binary or multiclass classification approaches are ineffective for the prediction of quality flaws and hence cast quality flaw prediction as a one-class classification problem. We develop a quality flaw model and employ a dedicated machine learning approach to predict Wikipedia's most important quality flaws. Since in the Wikipedia setting the acquisition of significant test data is intricate, we analyze the effects of a biased sample selection. In this regard we illustrate the classifier effectiveness as a function of the flaw distribution in order to cope with the unknown (real-world) flaw-specific class imbalances. The flaw prediction performance is evaluated with 10,000 Wikipedia articles that have been tagged with the ten most frequent quality flaws: provided test data with little noise, four flaws can be detected with a precision close to 1.",User-generated Content Analysis; Information Quality; Wikipedia; Quality Flaw Prediction; One-class Classification,https://dl.acm.org/doi/pdf/10.1145/2348283.2348413,https://dl.acm.org/doi/10.1145/2348283.2348413,2,1,3,0,6,Yes,Yes
18,Quality and Importance of Wikipedia Articles in Different Languages,"Google Scholar, Web of Science",2016,WÅodzimierz Lewoniewski; Krzysztof WÄcel; Witold Abramowicz,Conference,"ICIST '16: International Conference on Information and Software Technologies, pp. 613-624",Springer,"Cham, Switzerland",23,36,"This article aims to analyse the importance of the Wikipedia articles in different languages (English, French, Russian, Polish) and the impact of the importance on the quality of articles. Based on the analysis of literature and our own experience we collected measures related to articles, specifying various aspects of quality that will be used to build the models of articlesâ importance. For each language version, the influential parameters are selected that may allow automatic assessment of the validity of the article. Links between articles in different languages offer opportunities in terms of comparison and verification of the quality of information provided by various Wikipedia communities. Therefore, the model can be used not only for a relative assessment of the content of the whole article, but also for a relative assessment of the quality of data contained in their structural parts, the so-called infoboxes.",Wikipedia; DBpedia; Information quality; Data quality; WikiRank; Article importance,https://link.springer.com/content/pdf/10.1007/978-3-319-46254-7_50,https://link.springer.com/chapter/10.1007/978-3-319-46254-7_50,1,1,2,1,5,Yes,Yes
19,"A jury of your peers: quality, experience and ownership in Wikipedia","ACM, Google Scholar",2009,Aaron L. Halfaker; Aniket Kittur; Robert Kraut; John Riedl,Conference,"WikiSym '09: International Symposium on Wikis and Open Collaboration, no. 15, pp. 1-10",Association for Computing Machinery,"New York City, United States",23,127,"Wikipedia is a highly successful example of what mass collaboration in an informal peer review system can accomplish. In this paper, we examine the role that the quality of the contributions, the experience of the contributors and the ownership of the content play in the decisions over which contributions become part of Wikipedia and which ones are rejected by the community. We introduce and justify a versatile metric for automatically measuring the quality of a contribution. We find little evidence that experience helps contributors avoid rejection. In fact, as they gain experience, contributors are even more likely to have their work rejected. We also find strong evidence of ownership behaviors in practice despite the fact that ownership of content is discouraged within Wikipedia.",Wikipedia; Peer; Peer Review; WikiWork; Experience; Ownership; Quality,https://dl.acm.org/doi/pdf/10.1145/1641309.1641332,https://dl.acm.org/doi/10.1145/1641309.1641332,2,0,1,0,3,No,No
20,Automatically Assessing Wikipedia Article Quality by Exploiting Article-Editor Networks,Google Scholar,2015,Xinyi Li; Jintao Tang; Ting Wang; Zhunchen Luo; Maarten de Rijke,Conference,"ECIR '15: European Conference on Information Retrieval, pp. 574-580",Springer,"Cham, Switzerland",13,48,"We consider the problem of automatically assessing Wikipedia article quality. We develop several models to rank articles by using the editing relations between articles and editors. First, we create a basic model by modeling the article-editor network. Then we design measures of an editorâs contribution and build weighted models that improve the ranking performance. Finally, we use a combination of featured article information and the weighted models to obtain the best performance. We find that using manual evaluation to assist automatic evaluation is a viable solution for the article quality assessment task on Wikipedia",N/A,https://link.springer.com/content/pdf/10.1007/978-3-319-16354-3_64,https://link.springer.com/chapter/10.1007/978-3-319-16354-3_64,2,0,1,0,3,No,No
22,Interpolating Quality Dynamics in Wikipedia and Demonstrating the Keilana Effect,"ACM, Google Scholar",2017,Aaron L. Halfaker,Conference,"OpenSym '17: International Symposium on Open Collaboration, no. 19, pp. 1-9",Association for Computing Machinery,"New York City, United States",22,37,"For open, volunteer generated content like Wikipedia, quality is a prominent concern. To measure Wikipedia's quality, researchers have historically relied on expert evaluation or assessments of article quality by Wikipedians themselves. While both of these methods have proven effective for answering many questions about Wikipedia's quality and processes, they are both problematic: expert evaluation is expensive and Wikipedian quality assessments are sporadic and unpredictable. Studies that explore Wikipedia's quality level or the processes that result in quality improvements have only examined small snapshots of Wikipedia and often rely on complex propensity models to deal with the unpredictable nature of Wikipedians' own assessments. In this paper, I describe a method for measuring article quality in Wikipedia historically and at a finer granularity than was previously possible. I use this method to demonstrate an important coverage dynamic in Wikipedia (specifically, articles about women scientists) and offer this method, dataset, and open API to the research community studying Wikipedia quality dynamics.",Wikipedia; Quality; Modeling; Predictive; Interpolation; Methods; Dataset,https://dl.acm.org/doi/pdf/10.1145/3125433.3125475,https://dl.acm.org/doi/10.1145/3125433.3125475,1,1,1,0,3,No,No
23,An end-to-end learning solution for assessing the quality of Wikipedia articles,"ACM, Google Scholar",2017,Quang-Vinh Dang; Claudia-Lavinia Ignat,Conference,"OpenSym '17: International Symposium on Open Collaboration, no. 4, pp. 1-10",Association for Computing Machinery,"New York City, United States",61,39,"Wikipedia is considered as the largest knowledge repository in the history of humanity and plays a crucial role in modern daily life. Assigning the correct quality class to Wikipedia articles is an important task in order to provide guidance for both authors and readers of Wikipedia. The manual review cannot cope with the editing speed of Wikipedia. An automatic classification is required to classify the quality of Wikipedia articles. Most existing approaches rely on traditional machine learning with manual feature engineering, which requires a lot of expertise and effort. Furthermore, it is known that there is no general perfect feature set because information leak always occurs in feature extraction phase. Also, for each language of Wikipedia, a new feature set is required. In this paper, we present an approach relying on deep learning for quality classification of Wikipedia articles. Our solution relies on Recurrent Neural Networks (RNN) which is an end-to-end learning technique that eliminates disadvantages of feature engineering. Our approach learns directly from raw data without human intervention and is language-neutral. Experimental results on English, French and Russian Wikipedia datasets show that our approach outperforms state-of-the-art solutions.",Wikipedia; document quality; deep learning; end-to-end learning; Recurrent Neural Network (RNN); Long-Short Term Memory (LSTM),https://dl.acm.org/doi/pdf/10.1145/3125433.3125448,https://dl.acm.org/doi/10.1145/3125433.3125448,3,1,0,1,5,Yes,Yes
24,Quality assessment of Wikipedia articles without feature engineering,"ACM, Google Scholar, Web of Science",2016,Quang-Vinh Dang; Claudia-Lavinia Ignat,Conference,"JCDL '16: ACM/IEEE Joint Conference on Digital Libraries, pp. 27-30",Association for Computing Machinery,"New York City, United States",22,64,"As Wikipedia became the largest human knowledge repository, quality measurement of its articles received a lot of attention during the last decade. Most research efforts focused on classification of Wikipedia articles quality by using a different feature set. However, so far, no âgolden feature setâ was proposed. In this paper, we present a novel approach for classifying Wikipedia articles by analysing their content rather than by considering a feature set. Our approach uses recent techniques in natural language processing and deep learning, and achieved a comparable result with the state-of-the-art.",quality assessment; Wikipedia; feature engineering; document representation; deep learning,https://dl.acm.org/doi/pdf/10.1145/2910896.2910917,https://dl.acm.org/doi/10.1145/2910896.2910917,2,1,0,0,3,No,No
25,Relating Wikipedia article quality to edit behavior and link structure,"Google Scholar, Web of Science",2020,Thorsten Ruprechter; Tiago Santos; Denis Helic,Journal,"Applied Network Science, vol. 5(61)",N/A,N/A,55,14,"Currently, the relation between edit behavior, link structure, and article quality is not well-understood in our community, notwithstanding that this relationship may facilitate editing processes and content quality on Wikipedia. To shed light on this complex relation, we classify article edits and perform an in-depth analysis of editing sequences for 4941 articles. Additionally, we build a network of internal Wikipedia hyperlinks between articles. Using this data, we compute parsimonious metrics to quantify editing and linking behavior. Our analysis unveils that conflicted articles differ substantially from others in almost all metrics, while we also detect slight trends for high-quality articles. With our network analysis we find evidence indicating that controversial and edit war articles frequently span structural holes in the Wikipedia network. Finally, in a prediction experiment we demonstrate the usefulness of edit behavior patterns and network properties in predicting conflict and article quality. With our work, we assist online collaboration communities, especially Wikipedia, in long-term improvement of content quality by offering valuable insights about the interplay of article quality, controversies and edit wars, editing behavior, and network properties via sequence-based edit and network-based article metrics.",Wikipedia; Edit behavior; Link structure; Article quality; Edit wars; Controversy; Conflict; Semantic edit types,https://link.springer.com/content/pdf/10.1007/s41109-020-00305-y.pdf,https://appliednetsci.springeropen.com/articles/10.1007/s41109-020-00305-y,1,1,1,0,3,No,No
26,Assessing the quality of information on wikipedia: A deepâlearning approach,"ACM, Google Scholar, Web of Science",2020,Ping Wang; Xiaodan Li,Journal,"Journal of the Association for Information Science and Technology, vol. 71(1), pp. 16-28",N/A,N/A,65,32,"Currently, web document repositories have been collaboratively created and edited. One of these repositories, Wikipedia, is facing an important problem: assessing the quality of Wikipedia. Existing approaches exploit techniques such as statistical models or machine leaning algorithms to assess Wikipedia article quality. However, existing models do not provide satisfactory results. Furthermore, these models fail to adopt a comprehensive feature framework. In this article, we conduct an extensive survey of previous studies and summarize a comprehensive feature framework, including text statistics, writing style, readability, article structure, network, and editing history. Selected stateâofâtheâart deepâlearning models, including the convolutional neural network (CNN), deep neural network (DNN), long shortâterm memory (LSTMs) network, CNNâLSTMs, bidirectional LSTMs, and stacked LSTMs, are applied to assess the quality of Wikipedia. A detailed comparison of deepâlearning models is conducted with regard to different aspects: classification performance and training performance. We include an importance analysis of different features and feature sets to determine which features or feature sets are most effective in distinguishing Wikipedia article quality. This extensive experiment validates the effectiveness of the proposed model.",N/A,https://www.researchgate.net/publication/332294515_Assessing_the_quality_of_information_on_wikipedia_A_deep-learning_approach,https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24210,3,3,3,0,9,Yes,Yes
29,Using big data and network analysis to understand Wikipedia article quality,"Google Scholar, Web of Science",2018,Jun Liu; Sudha Ram,Journal,"Data & Knowledge Engineering, vol. 115, pp. 80-93",N/A,N/A,77,31,"The research reported in this paper focuses on the question of why Wikipedia articles are different in quality. Since these articles are developed in an open and social environment, our work investigates if the social capital of contributors plays a role in determining the quality of the articles. We focus on three major types of social capital with respect to teams of contributors working on Wikipedia articles: internal bonding, external bridging and functional diversity. Through a social network analysis of these articles based on a dataset extracted from its edit history, our research finds that all three types of social capital have a significant impact on their quality. In addition, we found that internal bonding interacts positively with external bridging resulting in a multiplier effect on article quality. The findings of our research have implications for developing automated techniques for quality assessment of Wikipedia and also provide insights into improving quality of these articles.",N/A,https://reader.elsevier.com/reader/sd/pii/S0169023X18300685?token=B44D54CF3D38230135AED9524BFFD22BFC9DE31B669C688D97EFE423C332436013F0D154796D42F839540C722AC80208&originRegion=eu-west-1&originCreation=20230130105258,https://www.sciencedirect.com/science/article/pii/S0169023X18300685?via%3Dihub,2,0,1,0,3,No,No
30,Measuring article quality in Wikipedia: Lexical clue model,Google Scholar,2011,Yanxiang Xu; Tiejian Luo,Conference,"SWS '11: Symposium on Web Society, pp. 141-146",Institute of Electrical and Electronic Engineers,"New York City, United States",24,43,"Wikipedia is the most entry-abundant on-line encyclopedia. Some studies published by Nature proved that the scientific entries in Wikipedia are of good quality comparable to those in the Encyclopedia Britannica which are mainly maintained by experts. But the manual partition of the articles in Wikipedia from a WikiProject implies that high-quality articles are usually reached grade by grade via being repeatedly revised. So many work address to automatically measuring the article quality in Wikipedia based on some assumption of the relationship between the article quality and contributors' reputations, view behaviors, article status, inter-article link, or so on. In this paper, a lexical clue based measuring method is proposed to assess article quality in Wikipedia. The method is inspired the idea that the good articles have more regular statistic features on lexical usage than the primary ones due to the more revise by more people. We select 8 lexical features derived from the statistic on word usages in articles as the factors that can reflect article quality in Wikipedia. A decision tree is trained based on the lexical clue model. Using the decision tree, our experiments on a well-labeled collection of 200 Wikipedia articles shows that our method has more than 83% precise and recall.",Wikipedia; article quality; lexical clue; decision tree,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6101286,https://ieeexplore.ieee.org/document/6101286,2,1,1,0,4,Yes,Yes
31,A Hybrid Model for Quality Assessment of Wikipedia Articles,Google Scholar,2017,Aili Shen; Jianzhong Qi; Timothy Baldwin,Conference,"ALTA '17: Australasian Language Technology Association Workshop, pp. 43-52",ACL Anthology,Online,45,33,"The task of document quality assessment is a highly complex one, which draws on analysis of aspects including linguistic content, document structure, fact correctness, and community norms. We explore the task in the context of a Wikipedia article assessment task, and propose a hybrid approach combining deep learning with features proposed in the literature. Our method achieves 6.5% higher accuracy than the state of the art in predicting the quality classes of English Wikipedia articles over a novel dataset of around 60k Wikipedia articles. We also discuss limitations with this task setup, and possible directions for establishing more robust document quality assessment evaluations.",N/A,https://aclanthology.org/U17-1005.pdf,https://aclanthology.org/U17-1005/,3,1,2,0,6,Yes,Yes
32,Measuring article quality in Wikipedia using the collaboration network,"ACM, Google Scholar, Web of Science",2015,Baptiste de La Robertie; Yoann Pitarch; Olivier Teste,Conference,"ASONAM '15: IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, pp. 464-471",Association for Computing Machinery,"New York City, United States",15,42,"Collaboratively edited articles such as in Wikipedia suffer from well-identified problems regarding their quality, e.g., information accuracy, reputability of third-party sources, vandalism. Due to the huge number of articles and the intensive edit rate, the manual evaluation of article content quality is inconceivable. In this paper, we tackle the problem of automatically establishing the quality of Wikipedia articles. Evidences are shown to consider the interactions between authors and articles to assess the quality score. Collaborations between authors and reviewers are also considered to reinforce the discriminative process. This work gives a generic formulation of the Mutual Reinforcement principle held between articles quality and authors authority and take explicitly advantage of the co-edits graph generated by individuals. Experiments conducted on a set of representative data from Wikipedia show the effectiveness of our approach.",N/A,https://dl.acm.org/doi/pdf/10.1145/2808797.2808895,https://dl.acm.org/doi/10.1145/2808797.2808895,2,0,1,0,3,No,No
33,QuWi: quality control in Wikipedia,"ACM, Google Scholar, Web of Science",2009,Alberto Cusinato; Vincenzo Della Mea; Francesco Di Salvatore; Stefano Mizzaro,Conference,"WICOW '09: Workshop on Information Credibility on the Web, pp. 27-34",Association for Computing Machinery,"New York City, United States",16,31,"We propose and evaluate QuWi (Quality in Wikipedia), a framework for quality control in Wikipedia. We build upon a previous proposal by Mizzaro [11], who proposed a method for substituting and/or complementing peer review in scholarly publishing. Since articles in Wikipedia are never finished, and their authors change continuously, we define a modified algorithm that takes into account the different domain, with particular attention to the fact that authors contribute identifiable pieces of information that can be further modified by other authors. The algorithm assigns quality scores to articles and contributors. The scores assigned to articles can be used, e.g., to let the reader understand how reliable are the articles he or she is looking at, or to help contributors in identifying low quality articles to be enhanced. The scores assigned to users measure the average quality of their contributions to Wikipedia and can be used, e.g., for conflict resolution policies based on the quality of involved users. Our proposed algorithm is experimentally evaluated by analyzing the obtained quality scores on articles for deletion and featured articles, also on six temporal Wikipedia snapshots. Preliminary results demonstrate that the proposed algorithm seems to appropriately identify high and low quality articles, and that high quality authors produce more long-lived contributions than low quality authors.",Wikipedia; Quality Control; QuWi; Reputation,https://dl.acm.org/doi/pdf/10.1145/1526993.1527001,https://dl.acm.org/doi/10.1145/1526993.1527001,2,0,1,1,4,Yes,Yes
34,Modelling the Quality of Attributes in Wikipedia Infoboxes,"Google Scholar, Web of Science",2015,Krzysztof WÄcel; WÅodzimierz Lewoniewski,Conference,"BIS '15: International Conference on Business Information Systems, pp. 308-320",Springer,"Cham, Switzerland",15,27,"Quality of data in DBpedia depends on underlying information provided in Wikipediaâs infoboxes. Various language editions can provide different information about given subject with respect to set of attributes and values of these attributes. Our research question is which language editions provide correct values for each attribute so that data fusion can be carried out. Initial experiments proved that quality of attributes is correlated with the overall quality of the Wikipedia article providing them. Wikipedia offers functionality to assign a quality class to an article but unfortunately majority of articles have not been graded by community or grades are not reliable. In this paper we analyse the features and models that can be used to evaluate the quality of articles, providing foundation for the relative quality assessment of infoboxâs attributes, with the purpose to improve the quality of DBpedia. ",Data quality; Information quality; DBpedia; Wikipedia; Infobox; Data mining; Wikirank,https://link.springer.com/content/pdf/10.1007/978-3-319-26762-3_27,https://link.springer.com/chapter/10.1007/978-3-319-26762-3_27,2,1,2,1,6,Yes,Yes
35,Towards automatic quality assurance in Wikipedia,"ACM, Google Scholar",2011,Maik Anderka; Benno Stein; Nedim Lipka,Conference,"WWW '11: International Conference Companion on World Wide Web, pp. 5-6",Association for Computing Machinery,"New York City, United States",10,41,"Featured articles in Wikipedia stand for high information quality, and it has been found interesting to researchers to analyze whether and how they can be distinguished from ""ordinary"" articles. Here we point out that article discrimination falls far short of writer support or automatic quality assurance: Featured articles are not identified, but are made. Following this motto we compile a comprehensive list of information quality flaws in Wikipedia, model them according to the latest state of the art, and devise one-class classification technology for their identification.",Wikipedia; Information Quality; Flaw Detection,https://dl.acm.org/doi/pdf/10.1145/1963192.1963196,https://dl.acm.org/doi/10.1145/1963192.1963196,2,1,1,0,4,Yes,Yes
36,What makes a good biography?: multidimensional quality analysis based on wikipedia article feedback data,"ACM, Google Scholar, Web of Science",2014,Lucie Flekova; Oliver Ferschke; Iryna Gurevych,Conference,"WWW '14: International Conference on World Wide Web, pp. 855-866",Association for Computing Machinery,"New York City, United States",44,41,"With more than 22 million articles, the largest collaborative knowledge resource never sleeps, experiencing several article edits every second. Over one fifth of these articles describes individual people, the majority of which are still alive. Such articles are, by their nature, prone to corruption and vandalism. Manual quality assurance by experts can barely cope with this massive amount of data. Can it be effectively replaced by feedback from the crowd? Can we provide meaningful support for quality assurance with automated text processing techniques? Which properties of the articles should then play a key role in the machine learning algorithms and why? In this paper, we study the user-perceived quality of Wikipedia articles based on a novel Wikipedia user feedback dataset. In contrast to previous work on quality assessment which mostly relied on judgements of active Wikipedia authors, we analyze ratings of ordinary Wikipedia users along four quality dimensions (Complete, Well written, Trustworthy and Objective). We first present an empirical analysis of the novel dataset with over 36 million Wikipedia article ratings. We then select a subset of biographical articles and perform classification experiments to predict their quality ratings along each of the dimensions, exploring multiple linguistic, surface and network properties of the rated articles. Additionally, we study the classification performance and differences for the biographies of living and dead people as well as those for men and women. We demonstrate the effectiveness of our approach by the F-scores of 0.94, 0.89, 0.73, and 0.73 for the dimensions Complete, Well written, Trustworthy, and Objective. Based on the results, we believe that the quality assessment of big textual data can be effectively supported by current text classification and language processing tools.",N/A,https://dl.acm.org/doi/pdf/10.1145/2566486.2567972,https://dl.acm.org/doi/10.1145/2566486.2567972,2,1,2,0,5,Yes,Yes
38,NwQM: A Neural Quality Assessment Framework for Wikipedia,"Google Scholar, Web of Science",2020,Bhanu Prakash Reddy Guda; Sasi Bhusan Seelaboyina; Soumya Sarkar; Animesh Mukherjee,Conference,"EMNLP '20: Conference on Empirical Methods in Natural Language Processing, pp. 8396-8406",ACL Anthology,Online,36,37,"Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics. Articles have been categorized by editors into several quality classes, which indicate their reliability as encyclopedic content. This manual designation is an onerous task because it necessitates profound knowledge about encyclopedic language, as well navigating circuitous set of wiki guidelines. In this paper we propose Neural wikipedia QualityMonitor (NwQM), a novel deep learning model which accumulates signals from several key information sources such as article text, meta data and images to obtain improved Wikipedia article representation. We present comparison of our approach against a plethora of available solutions and show 8% improvement over state-of-the-art approaches with detailed ablation studies.",N/A,https://aclanthology.org/2020.emnlp-main.674.pdf,https://aclanthology.org/2020.emnlp-main.674/,2,1,0,0,3,No,No
41,History-Based Article Quality Assessment on Wikipedia,"Google Scholar, Web of Science",2018,Shiyue Zhang; Zheng Hu; Chunhong Zhang; Ke Yu,Conference,"BIGCOMP '18: International Conference on Big Data and Smart Computing, pp. 1-8",Institute of Electrical and Electronic Engineers,"New York City, United States",30,19,"Wikipedia is widely considered as the biggest encyclopedia on Internet. Quality assessment of articles on Wikipedia has been studied for years. Conventional methods addressed this task by feature engineering and statistical machine learning algorithms. However, manually defined features are difficult to represent the long edit history of an article. Recently, researchers proposed an end-to-end neural model which used a Recurrent Neural Network(RNN) to learn the representation automatically. Although RNN showed its power in modeling edit history, the end-to-end method is time and resource consuming. In this paper, we propose a new history-based method to represent an article. We also take advantage of an RNN to handle the long edit history, but we do not abandon feature engineering. We still represent each revision of an article by manually defined features. This combination of deep neural model and feature engineering enables our model to be both simple and effective. Experiments demonstrate our model has better or comparable performance than previous works, and has the potential to work as a real-time service. Plus, we extend our model to do quality prediction.",Wikipedia; Information Quality; LSTM,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8367090,https://ieeexplore.ieee.org/document/8367090,3,1,1,0,5,Yes,Yes
43,FlawFinder: A Modular System for Predicting Quality Flaws in Wikipedia,Google Scholar,2012,Oliver Ferschke; Iryna Gurevych; Marc Rittberger,Conference,"CLEF '12: Conference and Labs of the Evaluation Forum, pp. 1178",CLEF Initiative,"Rome, Italy",28,41,"With over 23 million articles in 285 languages, Wikipedia is the largest free knowledge base on the web. Due to its open nature, everybody is allowed to access and edit the contents of this huge encyclopedia. As a downside of this open access policy, quality assessment of the content becomes a critical issue and is hardly manageable without computational assistance. In this paper, we present FlawFinder, a modular system for automatically predicting quality flaws in unseen Wikipedia articles. It competed in the inaugural edition of the Quality Flaw Prediction Task at the PAN Challenge 2012 and achieved the best precision of all systems and the second place in terms of recall and F1-score.",N/A,https://ceur-ws.org/Vol-1178/CLEF2012wn-PAN-FerschkeEt2012.pdf,https://www.researchgate.net/publication/235982155_FlawFinder_A_Modular_System_for_Predicting_Quality_Flaws_in_Wikipedia,2,1,2,0,5,Yes,Yes
46,Relative Quality and Popularity Evaluation of Multilingual Wikipedia Articles,"Google Scholar, Web of Science",2017,WÅodzimierz Lewoniewski; Krzysztof WÄcel; Witold Abramowicz,Journal,"Informatics, vol. 4(4), pp. 43",N/A,N/A,26,36,"Despite the fact that Wikipedia is often criticized for its poor quality, it continues to be one of the most popular knowledge bases in the world. Articles in this free encyclopedia on various topics can be created and edited in about 300 different language versions independently. Our research has showed that in language sensitive topics, the quality of information can be relatively better in the relevant language versions. However, in most cases, it is difficult for the Wikipedia readers to determine the language affiliation of the described subject. Additionally, each language edition of Wikipedia can have own rules in the manual assessing of the contentâs quality. There are also differences in grading schemes between language versions: some use a 6â8 grade system to assess articles, and some are limited to 2â3. This makes automatic quality comparison of articles between various languages a challenging task, particularly if we take into account a large number of unassessed articles; some of the Wikipedia language editions have over 99% of articles without a quality grade. The paper presents the results of a relative quality and popularity assessment of over 28 million articles in 44 selected language versions. Comparative analysis of the quality and the popularity of articles in popular topics was also conducted. Additionally, the correlation between quality and popularity of Wikipedia articles of selected topics in various languages was investigated. The proposed method allows us to find articles with information of better quality that can be used to automatically enrich other language editions of Wikipedia.",Wikipedia; information quality; WikiRank; DBpedia,https://pdfs.semanticscholar.org/a37c/5e68f712724157a4a6824f58a2425a830168.pdf,https://www.mdpi.com/2227-9709/4/4/43,1,0,1,1,3,No,No
48,A hybrid approach to classifying Wikipedia article quality flaws with feature fusion framework,"Google Scholar, Web of Science",2021,Ping Wang; Muyan Li; Xiaodan Li; Heshen Zhou; Jingrui Hou,Journal,"Expert Systems with Applications, vol. 181(1), pp. 115089",N/A,N/A,38,8,"Article quality has always been a major concern for Wikipedia. To improve article quality, it is critical to first identify defects. Thus, flaw classification has attracted considerable attention. To achieve this, several machine-learning-based approaches are available, including deep learning models based on either manually constructed or autoextracted features. However, adopting only features of either single type may not ensure a comprehensive description of articles. To improve flaw classification, we propose a feature fusion framework combining both handcrafted and autoextracted features. In this research, we first use a rule-based method from a previously proposed framework to extract handcrafted features. Additionally, we obtain autoextracted features using Bidirectional Encoder Representations from Transformers (BERT) and various deep learning models, including bidirectional long short-term memory (Bi LSTM), bidirectional gated recurrent unit (Bi GRU), bidirectional recurrent neural network (Bi RNN), and multihead self-attention models. Finally, the handcrafted features are standardized and concatenated with the autoextracted features. Then, the concatenated features are fed into a feedforward neural network for classification. A detailed comparison of different classifiers is conducted. We compare 12 different classifiers in terms of training performance, classification performance, and model training time. The experiments show that the proposed feature fusion framework can notably improve the effectiveness of quality flaw classification for Wikipedia articles. In particular, a Bi GRU model based on the proposed framework achieves excellent classification accuracy.",Quality flaw; Deep learning; Fusion framework; Text classification,https://reader.elsevier.com/reader/sd/pii/S0957417421005303?token=5388C4415793DA7AF64CBE513CD8B789A104B546D63754CE10326DECA64378FAADD906AE25B606164B3F69EC4FA003F6&originRegion=eu-west-1&originCreation=20230130110039,https://www.sciencedirect.com/science/article/pii/S0957417421005303?via%3Dihub,3,3,1,0,7,Yes,Yes
54,Network analysis of user generated content quality in Wikipedia,"Google Scholar, Web of Science",2013,Myshkin Ingawale; Amitava Dutta; Rahul Roy; Priya Seetharaman,Journal,"Online Information Review, vol. 37(4), pp. 602-619",N/A,N/A,52,31,"Purpose â Social media platforms allow nearâunfettered creation and exchange of user generated content (UGC). Drawing from network science, the purpose of this paper is to examine whether high and low quality UGC differ in their connectivity structures in Wikipedia (which consists of interconnected user generated articles). Design/methodology/approach â Using Featured Articles as a proxy for high quality, a network analysis was undertaken of the revision history of six different language Wikipedias, to offer a networkâcentric explanation for the emergence of quality in UGC.Findings â The network structure of interactions between articles and contributors plays an important role in the emergence of quality. Specifically the analysis reveals that highâquality articles cluster in hubs that span structural holes.Research limitations/implications â The analysis does not capture the strength of interactions between articles and contributors. The implication of this limitation is that quality is viewed as a binary variable. Extensions to this research will relate strength of interactions to different levels of quality in UGC. Practical implications - The findings help harness the âwisdom of the crowdsâ effectively. Organisations should nurture users and articles at the structural hubs from an early stage. This can be done through appropriate design of collaborative knowledge systems and development of organisational policies to empower hubs. Originality/value - The network centric perspective on quality in UGC and the use of a dynamic modelling tool are novel. The paper is of value to researchers in the area of social computing and to practitioners implementing and maintaining such platforms in organisations.",Web sites; Wikis; Social media; Wikipedia; Social computing; Quality; Network analysis; Structural holes; Hubs,https://www.emerald.com/insight/content/doi/10.1108/OIR-03-2011-0182/full/pdf?title=network-analysis-of-user-generated-content-quality-in-wikipedia,https://www.emerald.com/insight/content/doi/10.1108/OIR-03-2011-0182/full/html,2,0,1,1,4,Yes,Yes
57,Knowledge categorization affects popularity and quality of Wikipedia articles,"Google Scholar, Web of Science",2018,JÃ¼rgen Lerner; Alessandro Lomi,Journal,"PLoS ONE, vol. 13(1)",N/A,N/A,56,28,"The existence of a shared classification system is essential to knowledge production, transfer, and sharing. Studies of knowledge classification, however, rarely consider the fact that knowledge categories exist within hierarchical information systems designed to facilitate knowledge search and discovery. This neglect is problematic whenever information about categorical membership is itself used to evaluate the quality of the items that the category contains. The main objective of this paper is to show that the effects of category membership depend on the position that a category occupies in the hierarchical knowledge classification system of Wikipediaâan open knowledge production and sharing platform taking the form of a freely accessible on-line encyclopedia. Using data on all English-language Wikipedia articles, we examine how the position that a category occupies in the classification hierarchy affects the attention that articles in that category attract from Wikipedia editors, and their evaluation of quality of the Wikipedia articles. Specifically, we show that Wikipedia articles assigned to coarse-grained categories (i. e., categories that occupy higher positions in the hierarchical knowledge classification system) garner more attention from Wikipedia editors (i. e., attract a higher volume of text editing activity), but receive lower evaluations (i. e., they are considered to be of lower quality). The negative relation between attention and quality implied by this result is consistent with current theories of social categorization, but it also goes beyond available results by showing that the effects of categorization on evaluation depend on the position that a category occupies in a hierarchical knowledge classification system.",N/A,https://pdfs.semanticscholar.org/fa1d/b666c361fd03226c8e7bd81c5eb515719ac6.pdf,https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0190674,1,0,1,0,2,No,No
58,On improving wikipedia search using article quality,"ACM, Google Scholar",2007,Meiqun Hu; Ee-Peng Lim; Aixin Sun; Hady Wirawan Lauw; Ba-Quy Vuong,Conference,"WIDM '07: ACM International Workshop on Web Information and Data Management, pp. 145-152",Association for Computing Machinery,"New York City, United States",23,30,"Wikipedia is presently the largest free-and-open online encyclopedia collaboratively edited and maintained by volunteers. While Wikipedia offers full-text search to its users, the accuracy of its relevance-based search can be compromised by poor quality articles edited by non-experts and inexperienced contributors. In this paper, we propose a framework that re-ranks Wikipedia search results considering article quality. We develop two quality measurement models, namely Basic and PeerReview, to derive article quality based on co-authoring data gathered from articles' edit history. Compared with Wikipedia's full-text search engine, Google and Wikiseek, our experimental results showed that (i) quality-only ranking produced by PeerReview gives comparable performance to that of Wikipedia and Wikiseek; (ii) PeerReview combined with relevance ranking outperforms Wikipedia's full-text search significantly, delivering search accuracy comparable to Google.",Wikipedia; collaborative authoring; quality-aware search,https://dl.acm.org/doi/pdf/10.1145/1316902.1316926,https://dl.acm.org/doi/10.1145/1316902.1316926,1,0,1,0,2,No,No
59,Computational Trust in Web Content Quality: A Comparative Evalutation on the Wikipedia Project,Google Scholar,2007,Pierpaolo Dondio; Stephen Barrett,Journal,"Informatica, vol. 31(2), pp. 151-160",N/A,N/A,22,62,"The problem of identifying useful and trustworthy information on the World Wide Web is becoming increasingly acute as new tools such as wikis and blogs simplify and democratize publication. It is not hard to predict that in the future the direct reliance on this material will expand and the problem of evaluating the trustworthiness of this kind of content become crucial. The Wikipedia project represents the most successful and discussed example of such online resources. In this paper we present a method to predict Wikipedia articles trustworthiness based on computational trust techniques and a deep domain-specific analysis. Our assumption is that a deeper understanding of what in general defines high-standard and expertise in domains related to Wikipedia â i.e. content quality in a collaborative environment â mapped onto Wikipedia elements would lead to a complete set of mechanisms to sustain trust in Wikipedia context. We present a series of experiment. The first is a study-case over a specific category of articles; the second is an evaluation over 8 000 articles representing 65% of the overall Wikipedia editing activity. We report encouraging results on the automated evaluation of Wikipedia content using our domain-specific expertise method. Finally, in order to appraise the value added by using domain-specific expertise, we compare our results with the ones obtained with a pre-processed cluster analysis, where complex expertise is mostly replaced by training and automatic classification of common features.",computational trust; Wikipedia; content-quality,https://arrow.tudublin.ie/cgi/viewcontent.cgi?article=1031&context=scschcomart&httpsredir=1&referer=,https://arrow.tudublin.ie/scschcomart/25/,1,0,1,0,2,No,No
60,Mutual evaluation of editors and texts for assessing quality of Wikipedia articles,"ACM, Google Scholar",2012,Yu Suzuki; Masatoshi Yoshikawa,Conference,"WikiSym '12: International Symposium on Wikis and Open Collaboration, no. 18, pp. 1-10",Association for Computing Machinery,"New York City, United States",24,28,"In this paper, we propose a method to identify good quality Wikipedia articles by mutually evaluating editors and texts. A major approach for assessing article quality is a text survival ratio based approach. In this approach, when a text survives beyond multiple edits, the text is assessed as good quality. This approach assumes that poor quality texts are deleted by editors with high possibility. However, many vandals delete good quality texts frequently, then the survival ratios of good quality texts are improperly decreased by vandals. As a result, many good quality texts are unfairly assessed as poor quality. In our method, we consider editor quality for calculating text quality, and decrease the impacts on text qualities by the vandals who has low quality. Using this improvement, the accuracy of the text quality should be improved. However, an inherent problem of this idea is that the editor qualities are calculated by the text qualities. To solve this problem, we mutually calculate the editor and text qualities until they converge. We did our experimental evaluation, and we confirmed that the proposed method could accurately assess the text qualities.",Wikipedia; Quality; Peer Review; Edit History; Link Analysis,https://dl.acm.org/doi/pdf/10.1145/2462932.2462956,https://dl.acm.org/doi/10.1145/2462932.2462956,2,0,1,1,4,Yes,Yes
61,Mining the Factors Affecting the Quality of Wikipedia Articles,"ACM, Google Scholar",2010,Kewen Wu; Qinghua Zhu; Yuxiang Zhao; Hua Zheng,Conference,"ISME '10: International Conference of Information Science and Management Engineering, vol. 1, pp. 343-346",Institute of Electrical and Electronic Engineers,"New York City, United States",8,18,"In order to observe the variation of factors affecting the quality of Wikipedia articles during the information quality improvement process, we proposed 28 metrics from four aspects, including lingual, structural, historical and reputational features, and then weighted each metrics indifferent stages by using neural network. We found lingual features weighted more in the lower quality stages, and structural features, along with historical features, became more important while article quality improved. However, reputational features did not act as important as expected. The findings indicate that the information quality is mainly affected by completeness, and well-written is a basic requirement in the initial stage. Reputation of authors or editors is not so important in Wikipedia because of its horizontal structure.",Web2.0; Wikipedia; Information Quality; Neural Network; Quality Assessment,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5572324,https://ieeexplore.ieee.org/document/5572324,2,1,2,0,5,Yes,Yes
62,Measures for Quality Assessment of Articles and Infoboxes in Multilingual Wikipedia,"Google Scholar, Web of Science",2018,WÅodzimierz Lewoniewski,Conference,"BIS '18: International Conference on Business Information Systems, pp. 619-633",Springer,"Cham, Switzerland",84,15,"One of the most popular collaborative knowledge bases on the Internet is Wikipedia. Articles of this free encyclopaedia are created and edited by users from different countries in about 300 languages. Depending on topic and language version, quality of information there may vary. This study presents and classifies measures that can be extracted from Wikipedia articles for the purpose of automatic quality assessment in different languages. Based on a state of the art analysis and own experiments, specific measures for various aspects of quality have been defined. Additional, in this work they were also defined measures for quality assessment of data contained in the structural parts of Wikipedia articles - infoboxes. This study describes also an extraction methods for various sources of measures, that can be used in quality assessment.",Wikipedia; Data quality; Quality measures; DBpedia Wikidata; Quality dimensions; Web 2.0; Encyclopedia,https://link.springer.com/content/pdf/10.1007/978-3-030-04849-5_53,https://link.springer.com/chapter/10.1007/978-3-030-04849-5_53,2,0,2,0,4,Yes,Yes
64,Classifying Wikipedia Article Quality With Revision History Networks,"ACM, Google Scholar, Web of Science",2020,Narun K. Raman; Nathaniel Sauerberg; Jonah Fisher; Sneha Narayan,Conference,"OpenSym '20: International Symposium on Open Collaboration, no. 5, pp. 1-7",Association for Computing Machinery,"New York City, United States",21,8,"We present a novel model for classifying the quality of Wikipedia articles based on structural properties of a network representation of the article's revision history. We create revision history networks (an adaptation of Keegan et. al's article trajectory networks), where nodes correspond to individual editors of an article, and edges join the authors of consecutive revisions. Using descriptive statistics generated from these networks, along with general properties like the number of edits and article size, we predict which of six quality classes (Start, Stub, C-Class, B-Class, Good, Featured) articles belong to, attaining a classification accuracy of 49.35% on a stratified sample of articles. These results suggest that structures of collaboration underlying the creation of articles, and not just the content of the article, should be considered for accurate quality classification.",Wikipedia; network analysis; quantitative methods; article quality; classification; collaboration,https://dl.acm.org/doi/pdf/10.1145/3412569.3412581,https://dl.acm.org/doi/10.1145/3412569.3412581,2,2,1,0,5,Yes,Yes
65,WikipediaViz: Conveying article quality for casual Wikipedia readers,"Google Scholar, Web of Science",2010,Fanny Chevalier; StÃ©phane Huot; Jean-Daniel Fekete,Conference,"PacificVis '10: Pacific Visualization Symposium, pp. 49-56",Institute of Electrical and Electronic Engineers,"New York City, United States",26,47,"As Wikipedia has become one of the most used knowledge bases worldwide, the problem of the trustworthiness of the information it disseminates becomes central. With WikipediaViz, we introduce five visual indicators integrated to the Wikipedia layout that can keep casual Wikipedia readers aware of important metainformation about the articles they read. The design of WikipediaViz was inspired by two participatory design sessions with expert Wikipedia writers and sociologists who explained the clues they used to quickly assess the trustworthiness of articles. According to these results, we propose five metrics for Maturity and Quality assessment of Wikipedia articles and their accompanying visualizations to provide the readers with important clues about the editing process at a glance. We also report and discuss about the results of the user studies we conducted. Two preliminary pilot studies show that all our subjects trust Wikipedia articles almost blindly. With the third study, we show that WikipediaViz significantly reduces the time required to assess the quality of articles while maintaining a good accuracy.",Wikipedia; network analysis; quantitative methods; article quality; classification; collaboration,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5429611,https://ieeexplore.ieee.org/document/5429611/,1,0,1,1,3,No,No
66,Multilingual Ranking of Wikipedia Articles with Quality and Popularity Assessment in Different Topics,"Google Scholar, Web of Science",2019,WÅodzimierz Lewoniewski; Krzysztof WÄcel; Witold Abramowicz,Journal,"Computers, vol. 8(3), pp. 60",N/A,N/A,92,21,"On Wikipedia, articles about various topics can be created and edited independently in each language version. Therefore, the quality of information about the same topic depends on the language. Any interested user can improve an article and that improvement may depend on the popularity of the article. The goal of this study is to show what topics are best represented in different language versions of Wikipedia using results of quality assessment for over 39 million articles in 55 languages. In this paper, we also analyze how popular selected topics are among readers and authors in various languages. We used two approaches to assign articles to various topics. First, we selected 27 main multilingual categories and analyzed all their connections with sub-categories based on information extracted from over 10 million categories in 55 language versions. To classify the articles to one of the 27 main categories, we took into account over 400 million links from articles to over 10 million categories and over 26 million links between categories. In the second approach, we used data from DBpedia and Wikidata. We also showed how the results of the study can be used to build local and global rankings of the Wikipedia content.",Wikipedia; Information quality; Popularity; Topics identification; Wikidata; DBpedia; WikiRank,https://pdfs.semanticscholar.org/d5b3/f3e9403eda8b45184951b269ee7997452c6b.pdf,https://www.mdpi.com/2073-431X/8/3/60,2,0,1,1,4,Yes,Yes
67,Automatically assessing the quality of Wikipedia contents,"ACM, Google Scholar, Web of Science",2019,Elias Bassani; Marco Viviani,Conference,"SAC '19: ACM/SIGAPP Symposium on Applied Computing, pp. 804-807",Association for Computing Machinery,"New York City, United States",18,10,"With the development of Web 2.0 technologies, people have gone from being mere content users to content generators. In this context, the evaluation of the quality of (potential) information available online has become a crucial issue. Nowadays, one of the biggest online resources that users rely on as a knowledge base is Wikipedia. The collaborative aspect at the basis of Wikipedia can let to the possible creation of low-quality articles or even misinformation if the process of monitoring the generation and the revision of articles is not performed in a precise and timely way. For this reason, in this paper, the problem of automatically evaluating the quality of Wikipedia contents is considered, by proposing a supervised approach based on Machine Learning to perform the classification of articles on qualitative bases. With respect to prior literature, a wider set of features connected to Wikipedia articles has been taken into account, as well as previously unconsidered aspects connected to the generation of a labeled dataset to train the model, and the use of Gradient Boosting, which produced encouraging results.",Information Quality; Social Media; Machine Learning; Wikipedia,https://dl.acm.org/doi/pdf/10.1145/3297280.3297357,https://dl.acm.org/doi/10.1145/3297280.3297357,3,3,1,0,7,Yes,Yes
69,Wisdom of crowds: the effect of participant composition and contribution behavior on Wikipedia article quality,"Google Scholar, Web of Science",2020,Yan Lin; Chenxi Wang,Journal,"Journal of Knowledge Management, vol. 24(2), pp. 324-345",N/A,N/A,57,10,"Purpose - This paper aims to explore the effect of participant composition and contribution behavior of the different types of participants on the quality of knowledge generation in online communities. Design/methodology/approach - This study samples all the featured articles in Chinese Wikipedia and performs a Cox regression to reveal how participant composition and contribution behavior affect the quality of articles in different contexts. Findings - The results show that an increase in the number of participants increases the possibility of either enhancing or reducing the article quality. In most cases, the greater the proportion of core members (people who frequently participate in editing), the higher the possibility of enhancing the article quality. Occasional participantsâ editorial behavior hinders quality promotion, this negative effect weakens when such editorial behavior becomes more frequent. Practical implications - The findings help to better leverage the role of online communities in practice and to achieve knowledge collaboration in a more efficient manner. For example, an appropriate centralized organizational form should be established in online communities to improve the efficiency of crowd contributions. And it is worth developing mechanism to encourage participants to frequently participate in editing the article. Originality/value - This study contributes to the research on the organizational forms of online communities by showing the effect of participant composition and behavior in the new form of organizing on knowledge generation. This study also contributes to the research on wisdom of crowds by revealing who in a group of participants, in what context, and by what means influence knowledge generation.",Online community; Wikipedia; Wisdom of crowds; Cox regression; Knowledge collaboration,https://www.emerald.com/insight/content/doi/10.1108/JKM-08-2019-0416/full/pdf?title=wisdom-of-crowds-the-effect-of-participant-composition-and-contribution-behavior-on-wikipedia-article-quality,https://www.emerald.com/insight/content/doi/10.1108/JKM-08-2019-0416/full/html,1,0,1,1,3,No,No
70,Quality Evaluation of Wikipedia Articles through Edit History and Editor Groups,"ACM, Google Scholar, Web of Science",2010,Se Wang; Mizuho Iwaihara,Conference,"APWeb '11: Asia-Pacific Web Conference, pp. 188-199",Springer,"Berlin, Heidelberg",20,18,"Wikipedia is well known as a free encyclopedia, which is a type of collaborative repository system that allows the viewer to create and edit articles directly in the web browser. The weakness of the Wikipedia system is the possibility of manipulation and vandalism cannot be ruled out, so that the quality of any given Wikipedia article is not guaranteed. It is an important work to establish a quality evaluation method to help users decide how much they should trust an article in Wikipedia. In this paper we investigate the edit history of Wikipedia articles and propose a model of network structure of editors. We propose an algorithm to calculate the network structural indicator restoreratio. We use the proposed indicator combined with existing metrics to predict the quality of Wikipedia articles through support vector machine technology. The experimental results show that the proposed indicator has better performance in quality evaluation than several existing metrics.",Wikipedia; quality evaluation; web mining; edit network; web trust,https://link.springer.com/content/pdf/10.1007/978-3-642-20291-9_20.pdf,https://link.springer.com/chapter/10.1007/978-3-642-20291-9_20,2,1,1,0,4,Yes,Yes
71,Application of SEO Metrics to Determine the Quality of Wikipedia Articles and Their Sources,"Google Scholar, Web of Science",2018,WÅodzimierz Lewoniewski; Ralf-Christian HÃ¤rting; Krzysztof WÄcel; Christopher Reichstein; Witold Abramowicz,Conference,"ICIST '18: International Conference on Information and Software Technologies, pp. 139-152",Springer,"Cham, Switzerland",29,15,"The leading online encyclopedia Wikipedia is struggling with inconsistent article quality caused by the collaborative editing model. While one can find many helpful articles with consistent information on Wikipedia, there are also a lot of questionable articles with unclear or unfinished information yet. The quality of each article may vary over time as different users repeatedly re-edit content. One of the most important elements of the Wikipedia articles are references which allow to verify content and to show its source to user. Based on the fact that most of these references are web pages, it is possible to get more information about their quality by using citation analysis tools. For science and practice the empirical proof of the quality of the articles in Wikipedia could have a further signal effect, as the citation of Wikipedia articles, especially in scientific practice, is not yet recognised. This paper presents general results of Wikipedia analysis using metrics from the Toolbox SISTRIX, which is one of the leading providers of indicators for Search Engine Optimization (SEO). In addition to the preliminary analysis of the Wikipedia articles as separate web pages, we extracted data from more than 30 million references in different language versions of Wikipedia and analyzed over 180 thousand most popular hosts. In addition, we compared the same sources from different geographical perspectives using country-specific visibility indices.",Data quality; Wikipedia; References; SEO; SISTRIX Sources; Visibility Index; Search engine,https://link.springer.com/content/pdf/10.1007/978-3-319-99972-2_11,https://link.springer.com/chapter/10.1007/978-3-319-99972-2_11,2,0,1,1,4,Yes,Yes
72,Diversity of editors and teams versus quality of cooperative work: experiments on wikipedia,"ACM, Google Scholar, Web of Science",2017,Marcin Sydow; Katarzyna Baraniak; PaweÅ Teisseyre,Journal,"Journal of Intelligent Information Systems, vol. 48, pp. 601-632",N/A,N/A,28,16,"We study whether and how the diversity of editors and teams affects the quality of work in a virtual cooperative work environment on the Wikipedia example. We propose a measure of interests diversity of an editor and some measures of team diversity in terms of membersâ interests and experience. Statistical and machine learning methods are used to investigate the dependency between diversity and work quality. The presented experimental results confirm our hypothesis that interest diversity of a single editors and team diversity are positively related to the quality of their work. Interestingly, some of our experiments also indicate that diversity may be more important than such attributes as productivity of an editor or size or experience of the team. Our experimental results demonstrate that it is possible to predict work quality based on diversity which is an additional statistical signal that diversity is correlated with work quality.",Diversity of interest; Team diversity; Wikipedia; Article quality; Open collaboration; Machine learning,https://link.springer.com/content/pdf/10.1007/s10844-016-0428-1,https://link.springer.com/article/10.1007/s10844-016-0428-1,1,1,1,1,4,Yes,Yes
74,A deep learning-based quality assessment model of collaboratively edited documents: A case study of Wikipedia,"ACM, Google Scholar, Web of Science",2019,Ping Wang; Xiaodan Li; Renli Wu,Journal,"Journal of Information Science, vol. 47(2), pp. 176 - 191",N/A,N/A,65,8,"Wikipedia is becoming increasingly critical in helping people obtain information and knowledge. Its leading advantage is that users can not only access information but also modify it. However, this presents a challenging issue: how can we measure the quality of a Wikipedia article? The existing approaches assess Wikipedia quality by statistical models or traditional machine learning algorithms. However, their performance is not satisfactory. Moreover, most existing models fail to extract complete information from articles, which degrades the modelâs performance. In this article, we first survey related works and summarise a comprehensive feature framework. Then, state-of-the-art deep learning models are introduced and applied to assess Wikipedia quality. Finally, a comparison among deep learning models and traditional machine learning models is conducted to validate the effectiveness of the proposed model. The models are compared extensively in terms of their training and classification performance. Moreover, the importance of each feature and the importance of different feature sets are analysed separately.",Deep learning; feature framework; information quality assessment; Wikipedia,https://journals.sagepub.com/doi/pdf/10.1177/0165551519877646,https://journals.sagepub.com/doi/10.1177/0165551519877646,3,3,3,0,9,Yes,Yes
76,Automatically Labeling Low Quality Content on Wikipedia By Leveraging Patterns in Editing Behaviors,"ACM, Google Scholar",2021,Sumit Asthana; Sabrina Tobar Thommel; Aaron L. Halfaker; Nikola Banovic,Journal,"Proceedings of the ACM on Human-Computer Interaction, vol. 5(CSCW2), no. 359, pp. 1 - 23",N/A,N/A,55,1,"Wikipedia articles aim to be definitive sources of encyclopedic content. Yet, only 0.6% of Wikipedia articles have high quality according to its quality scale due to insufficient number of Wikipedia editors and enormous number of articles. Supervised Machine Learning (ML) quality improvement approaches that can automatically identify and fix content issues rely on manual labels of individual Wikipedia sentence quality. However, current labeling approaches are tedious and produce noisy labels. Here, we propose an automated labeling approach that identifies the semantic category (e.g., adding citations, clarifications) of historic Wikipedia edits and uses the modified sentences prior to the edit as examples that require that semantic improvement. Highest-rated article sentences are examples that no longer need semantic improvements. We show that training existing sentence quality classification algorithms on our labels improves their performance compared to training them on existing labels. Our work shows that editing behaviors of Wikipedia editors provide better labels than labels generated by crowdworkers who lack the context to make judgments that the editors would agree with.",Wikipedia; Content Labeling; Machine Learning,https://dl.acm.org/doi/pdf/10.1145/3479503,https://dl.acm.org/doi/10.1145/3479503,1,1,1,0,3,No,No
78,Article quality classification on Wikipedia: introducing document embeddings and content features,"ACM, Google Scholar, Web of Science",2019,Manuel Schmidt; Eva Zangerle,Conference,"OpenSym '19: International Symposium on Open Collaboration, no. 13, pp. 1-8",Association for Computing Machinery,"New York City, United States",24,9,"The quality of articles on the Wikipedia platform is vital for its success. Currently, the assessment of quality is performed manually by the Wikipedia community, where editors classify articles into pre-defined quality classes. However, this approach is hardly scalable and hence, approaches for the automatic classification have been investigated. In this paper, we extend this previous line of research on article quality classification by extending the set of features with novel content and edit features (e.g., document em-beddings of articles). We propose a classification approach utilizing gradient boosted trees based on this novel, extended set of features extracted from Wikipedia articles. Based on an established dataset containing Wikipedia articles and quality classes, we show that our approach is able to substantially outperform previous approaches (also including recent deep learning methods). Furthermore, we shed light on the contribution of individual features and show that the proposed features indeed capture the quality of an article well.",Wikipedia; Collaborative Information Systems; Information Quality; Classification; Gradient Boosted Trees,https://dl.acm.org/doi/pdf/10.1145/3306446.3340831,https://dl.acm.org/doi/10.1145/3306446.3340831,2,2,2,0,6,Yes,Yes
79,Assessing quality score of Wikipedia article using mutual evaluation of editors and texts,"ACM, Google Scholar, Web of Science",2013,Yu Suzuki; Masatoshi Yoshikawa,Conference,"CIKM '13: ACM International Conference on Information & Knowledge Management, pp. 1722-1732",Association for Computing Machinery,"New York City, United States",7,20,"In this paper, we propose a method for assessing quality scores of Wikipedia articles by mutually evaluating editors and texts. Survival ratio based approach is a major approach to assessing article quality. In this approach, when a text survives beyond multiple edits, the text is assessed as good quality, because poor quality texts have a high probability of being deleted by editors. However, many vandals, low quality editors, delete good quality texts frequently, which improperly decreases the survival ratios of good quality texts. As a result, many good quality texts are unfairly assessed as poor quality. In our method, we consider editor quality score for calculating text quality score, and decrease the impact on text quality by vandals. Using this improvement, the accuracy of the text quality score should be improved. However, an inherent problem with this idea is that the editor quality scores are calculated by the text quality scores. To solve this problem, we mutually calculate the editor and text quality scores until they converge. In this paper, we prove that the text quality score converges. We did our experimental evaluation, and confirmed that our proposed method could accurately assess the text quality scores.",Wikipedia; Quality; Peer Review; Vandalism; Edit History,https://dl.acm.org/doi/pdf/10.1145/2505515.2505610,https://dl.acm.org/doi/10.1145/2505515.2505610,2,0,1,1,4,Yes,Yes
82,Quality Assessment of Wikipedia Articles Using h-index,Google Scholar,2015,Yu Suzuki,Journal,"Journal of Information Processing, vol. 23(1), pp. 22-30",N/A,N/A,30,25,"In this paper, we propose a method for assessing quality values of Wikipedia articles from edit history using h-index. One of the major methods for assessing Wikipedia article quality is a peer-review based method. In this method, we assume that if an editorâs texts are left by the other editors, the texts are approved by the editors, then the editor is decided as a good editor. However, if an editor edits multiple articles, and the editor is approved at a small number of articles, the quality value of the editor deeply depends on the quality of the texts. In this paper, we apply h-index, which is a simple but resistant to excessive values, to the peer-review based Wikipedia article assessment method. Although h-index can identify whether an editor is a good quality editor or not, h-index cannot identify whether the editor is a vandal or an inactive editor. To solve this problem, we propose p-ratio for identifying which editors are vandals or inactive editors. From our experiments, we confirmed that by integrating h-index with p-ratio, the accuracy of article quality assessment in our method outperforms the existing peer-review based method.",Wikipedia; h-index; peer-review; quality,https://www.jstage.jst.go.jp/article/ipsjjip/23/1/23_22/_pdf,https://www.jstage.jst.go.jp/article/ipsjjip/23/1/23_22/_article,2,0,1,1,4,Yes,Yes
83,WikiLyzer: Interactive Information Quality Assessment in Wikipedia,"ACM, Google Scholar, Web of Science",2017,Cecilia di Sciascio; David Strohmaier; Marcelo Errecalde; Eduardo Veas,Conference,"IUI '17: International Conference on Intelligent User Interfaces, pp. 377-388",Association for Computing Machinery,"New York City, United States",33,14,"Digital libraries and services enable users to access large amounts of data on demand. Yet, quality assessment of information encountered on the Internet remains an elusive open issue. For example, Wikipedia, one of the most visited platforms on the Web, hosts thousands of user-generated articles and undergoes 12 million edits/contributions per month. User-generated content is undoubtedly one of the keys to its success, but also a hindrance to good quality: contributions can be of poor quality because anyone, even anonymous users, can participate. Though Wikipedia has defined guidelines as to what makes the perfect article, authors find it difficult to assert whether their contributions comply with them and reviewers cannot cope with the ever growing amount of articles pending review. Great efforts have been invested in algorithmic methods for automatic classification of Wikipedia articles (as featured or non-featured) and for quality flaw detection. However, little has been done to support quality assessment of user-generated content through interactive tools that combine automatic methods and human intelligence. We developed WikiLyzer, a Web toolkit comprising three interactive applications designed to assist (i) knowledge discovery experts in creating and testing metrics for quality measurement, (ii) Wikipedia users searching for good articles, and (iii) Wikipedia authors that need to identify weaknesses to improve a particular article. A design study sheds a light on how experts could create complex quality metrics with our tool, while a user study reports on its usefulness to identify high-quality content.",Text Analytics; Text Quality; User Generated Content; Wikipedia; Visual Analytics,https://dl.acm.org/doi/pdf/10.1145/3025171.3025201,https://dl.acm.org/doi/10.1145/3025171.3025201,1,0,1,0,2,No,No
84,"âWP2Cochraneâ, a tool linking Wikipedia to the Cochrane Library: Results of a bibliometric analysis evaluating article quality and importance","Google Scholar, Web of Science",2019,Arash Joorabchi; Calibhe Doherty; Jennifer Dawson,Journal,"Health Informatics Journal, vol. 26(3) pp. 1881 - 1897",N/A,N/A,54,7,"Medical information on English Wikipedia was accessed over 2 billion times in 2018. Our goal was to develop an automated system to assist Wikipedia volunteers to improve articles with high-quality sources from journals such as The Cochrane Library. We created an automated indexing system by linking available reviews from the Cochrane library with disease-related Wikipedia articles and evaluating the relationship between the quality and importance of these articles with the number of relevant and cited Cochrane reviews. We first conducted a bibliometric analysis, identifying disease-related Wikipedia articles and relevant/cited Cochrane reviews. Citations were thematically coded, and descriptive statistics were calculated. Finally, separate multinomial logistic regression analyses were conducted for article quality and importance. The indexing system identified 4381 disease-related Wikipedia articles, 1193 (27%) of which cited a Cochrane review. Higher quality Wikipedia articles were more likely to cite a Cochrane review (p = 0.002), while lower quality articles were less likely to cite a Cochrane review (p < 0.0005). A greater number of Cochrane reviews are available for more âimportantâ Wikipedia articles (p < 0.005), and these articles were more likely to cite a Cochrane review (p < 0.005). This approach to an indexing system can be leveraged by Wikipedia contributors and editors seeking to update disease-related Wikipedia articles with relevant Cochrane reviews (thus improving their quality), and online information seekers in need of additional information to supplement their Wikipedia search.",Bibliometrics [MeSH]; Encyclopedias as Topic [MeSH]; Internet; knowledge translation; open knowledge; peer review; Wikipedia,https://journals.sagepub.com/doi/pdf/10.1177/1460458219892711,https://journals.sagepub.com/doi/10.1177/1460458219892711,1,0,1,0,2,No,No
85,An Edit-centric Approach for Wikipedia Article Quality Assessment,Google Scholar,2019,Edison Marrese-Taylor; Pablo Loyola; Yutaka Matsuo,Conference,"WNUT '19: Workshop on Noisy User-generated Text, pp. 381-386",ACL Anthology,Online,25,8,"We propose an edit-centric approach to assess Wikipedia article quality as a complementary alternative to current full document-based techniques. Our model consists of a main classifier equipped with an auxiliary generative module which, for a given edit, jointly provides an estimation of its quality and generates a description in natural language. We performed an empirical study to assess the feasibility of the proposed model and its cost-effectiveness in terms of data and quality requirements.",N/A,https://aclanthology.org/D19-5550.pdf,https://aclanthology.org/D19-5550/,3,1,0,1,5,Yes,Yes
87,StRE: Self Attentive Edit Quality Prediction in Wikipedia,"Google Scholar, Web of Science",2019,Soumya Sarkar; Bhanu Prakash Reddy Guda; Sandipan Sikdar; Animesh Mukherjee,Conference,"ACL '19: Annual Meeting of the Association for Computational Linguistics, pp. 3962-3972",ACL Anthology,Online,39,10,"Wikipedia can easily be justified as a behemoth, considering the sheer volume of content that is added or removed every minute to its several projects. This creates an immense scope, in the field of natural language processing toward developing automated tools for content moderation and review. In this paper we propose Self Attentive Revision Encoder (StRE) which leverages orthographic similarity of lexical units toward predicting the quality of new edits. In contrast to existing propositions which primarily employ features like page reputation, editor activity or rule based heuristics, we utilize the textual content of the edits which, we believe contains superior signatures of their quality. More specifically, we deploy deep encoders to generate representations of the edits from its text content, which we then leverage to infer quality. We further contribute a novel dataset containing â¼ 21M revisions across 32K Wikipedia pages and demonstrate that StRE outperforms existing methods by a significant margin â at least 17% and at most 103%. Our pre-trained model achieves such result after retraining on a set as small as 20% of the edits in a wikipage. This, to the best of our knowledge, is also the first attempt towards employing deep language models to the enormous domain of automated content moderation and review in Wikipedia.",N/A,https://aclanthology.org/P19-1387.pdf,https://aclanthology.org/P19-1387/,2,1,0,0,3,No,No
89,Quality assessment of wikipedia articles: a deep learning approach,"ACM, Google Scholar",2016,Quang-Vinh Dang; Claudia-Lavinia Ignat,Other,"SIGWEB '16: ACM Sigweb Newsletter, Issue Autumn",N/A,N/A,26,12,"Wikipedia is indeed a very important knowledge sharing platform. However, since its start in 2001, the quality of Wikipedia is questioned because its content is created potentially by everyone who can access the Internet. Currently, the quality of Wikipedia articles is assessed by human judgement. The method is not scalable up to huge size and fast changing speed of Wikipedia today. An automatic quality classifier for Wikipedia articles is required to support user to choose high quality articles for reading and to notify authors for improving their products. While other existing approaches are based on manually predefined specific feature set, we present our approach of using deep learning to automatically represent Wikipedia articles for quality classification.",N/A,https://dl.acm.org/doi/pdf/10.1145/2996442.2996447,https://dl.acm.org/doi/10.1145/2996442.2996447,2,1,0,0,3,No,No
90,Evaluating the trustworthiness of Wikipedia articles through quality and credibility,"ACM, Google Scholar",2009,Sai T. Moturu; Huan Liu,Conference,"WikiSym '09: International Symposium on Wikis and Open Collaboration, no. 28, pp. 1-2",Association for Computing Machinery,"New York City, United States",5,23,"Wikipedia has become a very popular destination for Web surfers seeking knowledge about a wide variety of subjects. While it contains many helpful articles with accurate information, it also consists of unreliable articles with inaccurate or incomplete information. A casual observer might not be able to differentiate between the good and the bad. In this work, we identify the necessity and challenges for trust assessment in Wikipedia, and propose a framework that can help address these challenges by identifying relevant features and providing empirical means to meet the requirements for such an evaluation. We select relevant variables and perform experiments to evaluate our approach. The results demonstrate promising performance that is better than comparable approaches and could possibly be replicated with other social media applications.",Wikipedia; social media; trust; trustworthiness; quality,https://dl.acm.org/doi/pdf/10.1145/1641309.1641349,https://dl.acm.org/doi/10.1145/1641309.1641349,2,0,1,0,3,No,No
92,A matter of words: NLP for quality evaluation of Wikipedia medical articles,"Google Scholar, Web of Science",2016,Vittoria Cozza; Marinella Petrocchi; Angelo Spognardi,Conference,"IWCE '16: International Conference on Web Engineering, pp. 448-456",Springer,"Cham, Switzerland",18,15,"Automatic quality evaluation of Web information is a task with many fields of applications and of great relevance, especially in critical domains, like the medical one. We move from the intuition that the quality of content of medical Web documents is affected by features related with the specific domain. First, the usage of a specific vocabulary (Domain Informativeness); then, the adoption of specific codes (like those used in the infoboxes of Wikipedia articles) and the type of document (e.g., historical and technical ones). In this paper, we propose to leverage specific domain features to improve the results of the evaluation of Wikipedia medical articles, relying on Natural Language Processing (NLP) and dictionaries-based techniques. The results of our experiments confirm that, by considering domain-oriented features, it is possible to improve existing solutions, mainly with those articles that other approaches have less correctly classified.",N/A,https://link.springer.com/content/pdf/10.1007/978-3-319-38791-8_31,https://link.springer.com/chapter/10.1007/978-3-319-38791-8_31,3,1,1,0,5,Yes,Yes
95,Mining team characteristics to predict Wikipedia article quality,"ACM, Google Scholar, Web of Science",2016,Grace Gimon Betancourt; Armando Segnine; Carlos Trabuco; Amira Rezgui; Nicolas Jullien,Conference,"OpenSym '16: International Symposium on Open Collaboration, no. 15, pp. 1-9",Association for Computing Machinery,"New York City, United States",41,10,"In this study, we were interested in studying which characteristics of virtual teams are good predictors for the quality of their production. The experiment involved obtaining the Spanish Wikipedia database dump and applying different data mining techniques suitable for large data sets to label the whole set of articles according to their quality (comparing them with the Featured/Good Articles, or FA/GA). Then we created the attributes that describe the characteristics of the team who produced the articles and using decision tree methods, we obtained the most relevant characteristics of the teams that produced FA/GA. The team's maximum efficiency and the total length of contribution are the most important predictors. This article contributes to the literature on virtual team organization.",Wikipedia; Epistemic community; Article Quality; Teaming,https://dl.acm.org/doi/pdf/10.1145/2957792.2971802,https://dl.acm.org/doi/10.1145/2957792.2971802,1,1,1,1,4,Yes,Yes
96,Estimating the Quality of Articles in Russian Wikipedia Using the Logical-Linguistic Model of Fact Extraction,"Google Scholar, Web of Science",2017,Nina Khairova; WÅodzimierz Lewoniewski; Krzysztof WÄcel,Conference,"BIS '17: International Conference on Business Information Systems, pp. 28-40",Springer,"Cham, Switzerland",22,13,We present the method of estimating the quality of articles in Russian Wikipedia that is based on counting the number of facts in the article. For calculating the number of facts we use our logical-linguistic model of fact extraction. Basic mathematical means of the model are logical-algebraic equations of the finite predicates algebra. The model allows extracting of simple and complex types of facts in Russian sentences. We experimentally compare the effect of the density of these types of facts on the quality of articles in Russian Wikipedia. Better articles tend to have a higher density of facts.,Russian Wikipedia; Article quality; Fact extraction; Logical equations,https://link.springer.com/content/pdf/10.1007/978-3-319-59336-4_3,https://link.springer.com/chapter/10.1007/978-3-319-59336-4_3,2,0,1,1,4,Yes,Yes
97,"Quality Change: Norm or Exception? Measurement, Analysis and Detection of Quality Change in Wikipedia","ACM, Google Scholar",2021,Paramita Das; Bhanu Prakash Reddy Guda; Sasi Bhusan Seelaboyina; Soumya Sarkar; Animesh Mukherjee,Journal,"Proceedings of the ACM on Human-Computer Interaction, vol. 6(CSCW1), no. 112, pp. 1 - 36",N/A,N/A,84,0,"Wikipedia has been turned into an immensely popular crowd-sourced encyclopedia for information dissemination on numerous versatile topics in the form of subscription free content. It allows anyone to contribute so that the articles remain comprehensive and updated. For enrichment of content without compromising standards, the Wikipedia community enumerates a detailed set of guidelines, which should be followed. Based on these, articles are categorized into several quality classes by the Wikipedia editors with increasing adherence to guidelines. This quality assessment task by editors is laborious as well as demands platform expertise. As a first objective, in this paper, we study evolution of a Wikipedia article with respect to such quality scales. Our results show novel non-intuitive patterns emerging from this exploration. As a second objective we attempt to develop an automated data driven approach for the detection of the early signals influencing the quality change of articles. We posit this as a change point detection problem whereby we represent an article as a time series of consecutive revisions and encode every revision by a set of intuitive features. Finally, various change point detection algorithms are used to efficiently and accurately detect the future change points. We also perform various ablation studies to understand which group of features are most effective in identifying the change points. To the best of our knowledge, this is the first work that rigorously explores English Wikipedia article quality life cycle from the perspective of quality indicators and provides a novel unsupervised page level approach to detect quality switch, which can help in automatic content monitoring in Wikipedia thus contributing significantly to the CSCW community.",Wikipedia; Quality classes; Change point detection; Unsupervised approach,https://dl.acm.org/doi/pdf/10.1145/3512959,https://dl.acm.org/doi/10.1145/3512959,2,0,2,0,4,Yes,Yes
100,Quality flaw prediction in Spanish Wikipedia: A case of study with verifiability flaws,"Google Scholar, Web of Science",2018,Edgardo Ferretti; Leticia C. Cagnina; Viviana Paiz; SebastiÃ¡n Delle Donne; Rodrigo Zacagnini; Marcelo Errecalde,Journal,"Information Processing & Management, vol. 54(6), pp. 1169-1181",N/A,N/A,36,13,"In this work, we present the first quality flaw prediction study for articles containing the two most frequent verifiability flaws in Spanish Wikipedia: articles which do not cite any references or sources at all (denominated Unreferenced) and articles that need additional citations for verification (so-called Refimprove). Based on the underlying characteristics of each flaw, different state-of-the-art approaches were evaluated. For articles not citing any references, a well-established rule-based approach was evaluated and interesting findings show that some of them suffer from Refimprove flaw instead. Likewise, for articles that need additional citations for verification, the well-known PU learning and one-class classification approaches were evaluated. Besides, new methods were compared and a new feature was also proposed to model this latter flaw. The results showed that new methods such as under-bagged decision trees with sum or majority voting rules, biased-SVM, and centroid-based balanced SVM, perform best in comparison with the ones previously published.",Information quality; Quality flaw prediction; Semi-supervised learning; Supervised learning; Wikipedia,https://reader.elsevier.com/reader/sd/pii/S0306457317309329?token=932F3ED2D2166622793C94EF06681099A807E81123D2C8A2103641E6DFA3906BCFCA8A59AF7D631C3F1183491945618F&originRegion=eu-west-1&originCreation=20230130113926,https://www.sciencedirect.com/science/article/pii/S0306457317309329?via%3Dihub,2,3,2,1,8,Yes,Yes
101,Structural Analysis of Wikigraph to Investigate Quality Grades of Wikipedia Articles,"ACM, Google Scholar, Web of Science",2021,Anamika Chhabra; Shubham Srivastava; S. R. S. Iyengar; Poonam Saini,Conference,"WWW '21: The Web Conference, pp. 584-590",Association for Computing Machinery,"New York City, United States",34,1,"The quality of Wikipedia articles is manually evaluated which is time inefficient as well as susceptible to human bias. An automated assessment of these articles may help in minimizing the overall time and manual errors. In this paper, we present a novel approach based on the structural analysis of Wikigraph to automate the estimation of the quality of Wikipedia articles. We examine the network built using the complete set of English Wikipedia articles and identify the variation of network signatures of the articles with respect to their quality. Our study shows that these signatures are useful for estimating the quality grades of un-assessed articles with an accuracy surpassing the existing approaches in this direction. The results of the study may help in reducing the need for human involvement for quality assessment tasks.",Wikipedia; Quality estimation; Wikigraph; Network analysis,https://dl.acm.org/doi/pdf/10.1145/3442442.3452345,https://dl.acm.org/doi/10.1145/3442442.3452345,2,1,1,0,4,Yes,Yes
106,Relative Quality Assessment of Wikipedia Articles in Different Languages Using Synthetic Measure,"Google Scholar, Web of Science",2017,WÅodzimierz Lewoniewski; Krzysztof WÄcel,Conference,"BIS '17: International Conference on Business Information Systems, pp. 282-292",Springer,"Cham, Switzerland",16,11,"Online encyclopedia Wikipedia is one of the most popular sources of knowledge. It is often criticized for poor information quality. Articles can be created and edited even by anonymous users independently in almost 300 languages. Therefore, a difference in the information quality in various language versions on the same topic is observed. The Wikipedia community has created a system for assessing the quality of articles, which can be helpful in deciding which language version is more complete and correct. There are several issues: each Wikipedia language can use own grading scheme and there is usually a large number of unevaluated articles. In this paper, we propose to use a synthetic measure for automatic quality evaluation of the articles in different languages based on important features",Wikipedia; article quality; synthetic measure; wikirank,https://www.researchgate.net/publication/320446880_Relative_Quality_Assessment_of_Wikipedia_Articles_in_Different_Languages_Using_Synthetic_Measure,https://link.springer.com/chapter/10.1007/978-3-319-69023-0_24,2,0,1,1,4,Yes,Yes
109,Enrichment of Information in Multilingual Wikipedia Based on Quality Analysis,"Google Scholar, Web of Science",2017,WÅodzimierz Lewoniewski,Conference,"BIS '17: International Conference on Business Information Systems, pp. 216-227",Springer,"Cham, Switzerland",31,11,"Despite the fact that Wikipedia is one of the most popular sources of information in the world, it is often criticized for the poor quality of content. In this online encyclopaedia articles on the same topic can be created and edited independently in different languages. Some of this language versions can provide valuable information on a specific topics. Wikipedia articles may include infobox, which used to collect and present a subset of important information about its subject. This study presents method for quality assessment of Wikipedia articles and information contained in their infoboxes. Choosing the best language versions of a particular article will allow for enrichment of information in less developed version editions of particular articles.",Wikipedia; article quality; infobox; DBpedia,http://lewoniewski.info/files/bis2017_wiki_enrichment.pdf,https://link.springer.com/chapter/10.1007/978-3-319-69023-0_19,2,0,1,1,4,Yes,Yes
111,GreenWiki: a tool to support users' assessment of the quality of Wikipedia articles,"ACM, Google Scholar",2011,Daniel Hasan Dalip; Raquel Lara Santos; Diogo RennÃ³ Rocha de Oliveira; ValÃ©ria Freitas Amaral; Marcos AndrÃ© GonÃ§alves; Raquel Oliveira Prates; Raquel C. M. Minardi; Jussara Marques de Almeida,Conference,"JCDL '11: ACM/IEEE Joint Conference on Digital Libraries, pp. 469-470",Association for Computing Machinery,"New York City, United States",4,6,"In this work, we present GreenWiki, which is a wiki with a panel of quality indicators to assist the reader of a Wikipedia article in assessing its quality.",Quality Assessment; Wikipedia; Quality Metrics,https://dl.acm.org/doi/pdf/10.1145/1998076.1998190,https://dl.acm.org/doi/10.1145/1998076.1998190,1,0,1,0,2,No,No
114,An Empirical Study to Predict the Quality of Wikipedia Articles,Google Scholar,2019,Imran Khan; Shahid Hussain; Hina Gul; Muhammad Shahid; Muhammad Jamal,Conference,"WorldCIST '19: World Conference on Information Systems and Technologies, pp. 485-492",Springer,"Cham, Switzerland",17,3,"Wikipedia is considered a common way to deliver content in a more effective way as compared to other types of an encyclopedia. However, the quality threat remains an issue regarding the Wikipedia articles. The basic aim of propose research to perform an empirical study to predict the quality of Wikipedia articles. In the proposed methodology, we consider few metrics such as article length (total number of word in an article), number of edits, article age (in the day) and article ranking and perform few statistical tests analyze the quality of Wikipedia articles. Moreover, we observe a significant correlation of proposed metrics with the rating of articles in order to identify their quality. ",Wikipedia; Correlation; Linear regression; Article length; Number of edits; Article age,https://link.springer.com/content/pdf/10.1007/978-3-030-16187-3_47,https://link.springer.com/chapter/10.1007/978-3-030-16187-3_47,2,0,1,0,3,No,No
115,On the Use of PU Learning for Quality Flaw Prediction in Wikipedia,Google Scholar,2012,Edgardo Ferretti; Donato Hernandez Fusilier; Rafael GuzmÃ¡n-Cabrera; Manuel Montes y GÃ³mez; Marcelo Errecalde; Paolo Rosso,Conference,"CLEF '12: Conference and Labs of the Evaluation Forum, pp. 1178",CLEF Initiative,"Rome, Italy",17,9,"In this article we describe a new approach to assess Quality Flaw Prediction in Wikipedia. The partially supervised method studied, called PU Learning, has been successfully applied in classifications tasks with traditional corpora like Reuters-21578 or 20-Newsgroups. To the best of our knowledge, this is the first time that it is applied in this domain. Throughout this paper, we describe how the original PU Learning approach was evaluated for assessing quality flaws and the modifications introduced to get a quality flaws predictor which obtained the best F1 scores in the task âQuality Flaw Prediction in Wikipediaâ of the PAN challenge",N/A,https://ceur-ws.org/Vol-1178/CLEF2012wn-PAN-FerrettiEt2012.pdf,https://www.researchgate.net/publication/236565329_On_the_Use_of_PU_Learning_for_Quality_Flaw_Prediction_in_Wikipedia,2,1,3,0,6,Yes,Yes
117,QualityRank: assessing quality of wikipedia articles by mutually evaluating editors and texts,"ACM, Google Scholar",2012,Yu Suzuki; Masatoshi Yoshikawa,Conference,"HT '12: ACM Conference on Hypertext & Social Media, pp. 307-308",Association for Computing Machinery,"New York City, United States",3,10,"In this paper, we propose a method to identify high-quality Wikipedia articles by mutually evaluating editors and texts. A major approach for assessing articles using edit history is a text survival ratio based approach. However, the problem is that many high-quality articles are identified as low quality, because many vandals delete high-quality texts, then the survival ratios of high-quality texts are decreased by vandals. Our approach's strongest point is its resistance to vandalism. Using our method, if we calculate text quality values using editor quality values, vandals do not affect any quality values of the other editors, then the accuracy of text quality values should improve. However, the problem is that editor quality values are calculated by text quality values, and text quality values are calculated by editor quality values. To solve this problem, we mutually calculate editor and text quality values until they converge. Using this method, we can calculate a quality value of a text that takes into consideration that of its editors.",Quality; Peer Review; Edit History; Link Analysis,https://dl.acm.org/doi/pdf/10.1145/2309996.2310047,https://dl.acm.org/doi/10.1145/2309996.2310047,2,0,1,0,3,No,No
118,A Framework for Assessing the Quality of Wikipedia Articles: A Meta-synthesis of the Literature,Google Scholar,2022,Fatemeh Fahimnia; Mansoureh Damerchiloo; Mohammad Khandan; Mahshid Eltemasi ,Journal,"International Journal of Information Science and Management, vol. 20(1), pp. 91-118",N/A,N/A,39,0,"This study aimed to design and validate an information quality assessment framework based on the systematic review of the literature. A meta-synthesis method was applied to identify features and dimensions for designing the framework for assessing the quality of Wikipedia. Following this, the validity of the obtained framework was evaluated by the Kappa Test of Agreement. The statistical population consists of all scientific documents related to the quality of information and a sample of 39 documents selected based on CASP. MAXQDA 11 was used to analyze data. Nine dimensions were identified and classified across 3 levels. The features and components necessary to evaluate each dimension were identified and explained. The strong level approved the framework of expert agreement (0.654). The proposed framework can be used to assess the Wikipedia quality independently of specialists, improve the quality of articles through identified effective features, and ultimately, build tools and practical guidelines to assess the quality of information.",Information quality; assessment; Wikipedia; Meta-synthesis,https://ijism.ricest.ac.ir/article_698359_9629f4a6d2a3231b83c48768209dfba2.pdf,https://www.magiran.com/paper/2379640,3,0,2,0,5,Yes,Yes
119,Assessing Information Quality of Wikipedia Articles through Googleâs E-A-T Model,"Google Scholar, Web of Science",2022,Chinthani Sugandhika; Supunmali Ahangama,Journal,"IEEE Access, vol. 10, pp. 52196-52209",N/A,N/A,73,1,"Along with the emergence of Web 2.0, User Generated Content (UGC) is becoming increasingly important for knowledge sharing. Wikipedia being the worldâs largest-ever community-based collaborative encyclopedia, is also one of the biggest UGC databases in the world. Wikipedia is dealing with a significant problem of Information Quality (IQ) because of its open-source and collaborative nature. When carrying out attacks such as link spamming, malicious users take advantage of Wikipediaâs popularity on the World Wide Web (WWW). As a result, Wikipedia is generally not recommended for academic-related work. There are, however, some articles that are both rich in information and quality. Existing approaches for assessing Wikipediaâs IQ involve statistical models and machine learning algorithms; however, the existing models do not produce satisfactory results. In this study, a novel theoretical model based on Googleâs E-A-T framework is introduced to assess Wikipediaâs IQ. The model comprises three IQ constructs Expertise, Authority and Trustworthiness. Based on the empirical findings and study results, a set of IQ dimensions that influence the above three IQ constructs, as well as 45 IQ attributes to measure the IQ dimensions, were identified. The IQ attributes were automatically and inexpensively extracted from the content and meta-data statistics of Wikipedia articles using a Selenium 3.14 web automation script. A sample of 2000 articles comprising 1000 Featured Articles (FA) and 1000 non-FA articles from six WikiProjects was used for the data analysis. The proposed model was compared with three previously published models in terms of classification and clustering accuracy. It received classification and clustering accuracies of 95% and 93% respectively, which is a drastic improvement over the existing models. Furthermore, an average inter-rater agreement of 84% was observed. Thus, the proposed modelâs effectiveness is fairly validated by this extensive experiment. This study contributes to the related knowledge area by introducing a novel framework to assess Wikipedia articlesâ IQ.",Authority; Expertise; Googleâs E-A-T; Hybrid approach; Information quality; Web 2.0; Wikipedia; Trustworthiness,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09770051.pdf,https://ieeexplore.ieee.org/document/9770051,3,2,1,0,6,Yes,Yes
122,Measuring Quality of Wikipedia Articles by Feature Fusionâbased Stack Learning,Google Scholar,2021,Jingrui Hou; Jiangnan Li; Ping Wang,Conference,"ASIST '21: Association for Information Science and Technology, vol. 58(1), pp. 206-217",Association for Information Science & Technology,"Silver Spring, Maryland",62,1,"Online open-source knowledge repository such as Wikipedia has become an increasingly important source for users to access knowledge. However, due to its large volume, it is challenging to evaluate Wikipedia article quality manually. To fill this gap, we propose a novel approach named âfeature fusion-based stack learningâ to assess the quality of Wikipedia articles. Pre-trained language models including BERT (Bidirectional Encoder Representations from Transformers) and ELMo (Embeddings from Language Models) are applied to extract semantic information in Wikipedia content. The feature fusion framework consisting of semantic and statistical features is built and fed into an out-of-sample (OOS) stacking model, which includes both machine learning and deep learning models. We compare the performance of proposed model with some existing models with different metrics extensively, and conduct ablation studies to prove the effectiveness of our framework and OOS stacking. Generally, the experiment shows that our method is much better than state-of-the-art models.",Wikipedia article quality assessment; language representation model; deep ensemble learning,N/A,https://asistdl.onlinelibrary.wiley.com/doi/10.1002/pra2.449,3,3,2,0,8,Yes,Yes
125,"How do metrics of link analysis correlate to quality, relevance and popularity in wikipedia?","ACM, Google Scholar",2013,RaÃ­za Hanada; Marco Cristo; Maria da GraÃ§a Campos Pimentel,Conference,"WebMedia '13: Brazilian Symposium on Multimedia and the Web, pp. 105-112",Association for Computing Machinery,"New York City, United States",15,9,"Many links between Web pages can be viewed as indicative of the quality and importance of the pages they pointed to. Accordingly, several studies have proposed metrics based on links to infer web page content quality. However, as far as we know, the only work that has examined the correlation between such metrics and content quality consisted of a limited study that left many open questions. In spite of these metrics having been shown successful in the task of ranking pages which were provided as answers to queries submitted to search engines, it is not possible to determine the specific contribution of factors such as quality, popularity, and importance to the results. This difficulty is partially due to the fact that such information is hard to obtain for Web pages in general. Unlike ordinary Web pages, the quality, importance and popularity of Wikipedia articles are evaluated by human experts or might be easily estimated. Thus, it is feasible to verify the relation between link analysis metrics and such factors in Wikipedia articles, our goal in this work. To accomplish that, we implemented several link analysis algorithms and compared their resulting rankings with the ones created by human evaluators regarding factors such as quality, popularity and importance. We found that the metrics are more correlated to quality and popularity than to importance, and the correlation is moderate.",Link Analysis; Wikipedia; Quality of Content,https://dl.acm.org/doi/pdf/10.1145/2526188.2526198,https://dl.acm.org/doi/10.1145/2526188.2526198,1,0,1,1,3,No,No
127,"On the Relation of Edit Behavior, Link Structure, and Article Quality on Wikipedia","Google Scholar, Web of Science",2019,Thorsten Ruprechter; Tiago Santos; Denis Helic,Conference,"COMPLEX NETWORKS '19: International Workshop on Complex Networks & Their Applications, pp. 242-254",Springer,"Cham, Switzerland",36,3,"When editing articles on Wikipedia, arguments between editors frequently occur. These conflicts occasionally lead to destructive behavior and diminish article quality. Currently, the relation between editing behavior, link structure, and article quality is not well-understood in our community, notwithstanding that this relation may facilitate editing processes and article quality on Wikipedia. To shed light on this complex relation, we classify edits for 13,045 articles and perform an in-depth analysis of a 4,800 article subsample. Additionally, we build a network of wikilinks (internal Wikipedia hyperlinks) between articles. Using this data, we compute parsimonious metrics to quantify editing and linking behavior. Our analysis unveils that controversial articles differ considerably from others for almost all metrics, while slight trends are also detectable for higher-quality articles. With our work, we assist online collaboration communities, especially Wikipedia, in long-term improvement of article quality by identifying deviant behavior via simple sequence-based edit and network-based article metrics. ",Wikipedia; Edit behavior; Link structure; Article quality; Edit wars; Semantic edit types; Multi-label classification; Network analysis,https://link.springer.com/content/pdf/10.1007/978-3-030-36683-4_20.pdf,https://link.springer.com/chapter/10.1007/978-3-030-36683-4_20,1,0,1,0,2,No,No
128,A Psycho-Lexical Approach to the Assessment of Information Quality on Wikipedia,"Google Scholar, Web of Science",2015,Qi Su; Pengyuan Liu,Conference,"WI-IAT '15: IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology, vol. 3, pp. 184-187",Institute of Electrical and Electronic Engineers,"New York City, United States",18,6,"The great popularity of Wikipedia makes it one of the dominant knowledge source around the World. However, since one of the core principles of Wikipedia is being open for anyone to maintain it, Wikipedia cannot fully ensure the reliability of its articles, and thus sometimes suffered criticism for containing low-quality information. It is therefore essential to assess the quality of Wikipedia articles automatically. In this paper we describe how we approach that problem by using a psycho-lexical resource, i.e., the Language Inquiry and Word Count (LIWC) dictionary. By training a classifier on different LIWC categories, we discuss the implications of each category for Wikipedia quality assessment.",Wikipedia; LIWC; Information Quality; User-generated Content,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7397452,https://ieeexplore.ieee.org/document/7397452,2,1,1,0,4,Yes,Yes
130,Thai Wikipedia article quality filtering algorithm,Google Scholar,2017,Nuanwan Soonthornphisaj; Peerapoom Paengporn,Conference,"IMECS '17: International MultiConference of Engineers and Computer Scientists, vol. 1",International Association of Engineers,"Hong Kong, China",14,4,"Wikipedia is a content creation system that uses open collaboration as a strategy to drive the variety of topic coverage. There are approximately 100,000 Thai articles in Wikipedia. We found that the quality of content is the big issue since there are only 240 articles that has been labeled as featured articles whereas the rest of Thai articles are unlabeled. The website is ranked number 12 in term of user access in Thailand. That infers the use of their content in many academic documents and it would affect Thai educational quality in the long term. This paper present a good quality article filtering framework using decision tree algorithm. We propose new feature set obtained from a variety of references found in Wikipedia articles. The feature sets are applied in the machine learning algorithm in order to get the classifier with the knowledge concept of high and low quality articles. The performance of filtering algorithm on unlabeled articles is evaluated by real users to validate the performance of the system. ",Thai Wikipedia article; decision tree; feature set,https://www.iaeng.org/publication/IMECS2017/IMECS2017_pp299-305.pdf,https://www.iaeng.org/publication/IMECS2017/IMECS2017_pp299-305.pdf,2,1,1,1,5,Yes,Yes
131,Assessing the Quality of Wikipedia Pages Using Edit Longevity and Contributor Centrality,Google Scholar,2012,Xiangju Qin; PÃ¡draig Cunningham,Other,arXiv: 1206.2517,N/A,N/A,20,8,"In this paper we address the challenge of assessing the quality of Wikipedia pages using scores derived from edit contribution and con- tributor authoritativeness measures. The hypothesis is that pages with significant contributions from authoritative contributors are likely to be high-quality pages. Contributions are quantified using edit longevity measures and contributor authoritativeness is scored using centrality metrics in either the Wikipedia talk or co-author networks. The results suggest that it is useful to take into account the contributor authori- tativeness when assessing the information quality of Wikipedia content. The percentile visualization of the quality scores provides some insights about the anomalous articles, and can be used to help Wikipedia editors to identify Start and Stub articles that are of relatively good quality.",N/A,https://arxiv.org/pdf/1206.2517.pdf,https://arxiv.org/abs/1206.2517,1,0,1,0,2,No,No
132,Towards Information Quality Assurance in Spanish: Wikipedia,"Google Scholar, Web of Science",2017,Edgardo Ferretti; MatÃ­as Soria; SebastiÃ¡n PÃ©rez Casseignau; Lian Pohn; Guido Urquiza; Sergio Alejandro GÃ³mez; Marcelo Errecalde,Journal,"Journal of Computer Science and Technology (JCS&T, Argentina), vol. 17(1), pp. 29-36",N/A,N/A,23,6,"Featured Articles (FA) are considered to be the best articles that Wikipedia has to offer and in the last years, researchers have found interesting to analyze whether and how they can be distinguished from âordinaryâ articles. Likewise, identifying what issues have to be enhanced or fixed in ordinary articles in order to improve their quality is a recent key research trend. Most of the approaches developed to face these information quality problems have been proposed for the English Wikipedia. However, few efforts have been accomplished in Spanish Wikipedia, despite being Spanish, one of the most spoken languages in the world by native speakers. In this respect, we present a breakdown of Spanish Wikipediaâs quality flaw structure. Besides, we carry out studies with three different corpora to automatically assess information quality in Spanish Wikipedia, where FA identification is evaluated as a binary classification task. Our evaluation on a unified setting allows to compare with the English version, the performance achieved by our approach on the Spanish version. The best results obtained show that FA identification in Spanish, can be performed with an F1 score of 0.88 using a document model consisting of only twenty six features and Support Vector Machine as classification algorithm.",Featured Article Identification; Information Quality; Quality Flaws Prediction; Wikipedia,https://host170.sedici.unlp.edu.ar/server/api/core/bitstreams/6908402f-f564-4313-aa26-a121a593a2a6/content,https://www.semanticscholar.org/paper/8cba1878de84959de7a5401c9181819ee9bdf205,2,1,2,1,6,Yes,Yes
133,Effects of Implicit Positive Ratings for Quality Assessment of Wikipedia Articles,Google Scholar,2013,Yu Suzuki,Journal,"Journal of Information Processing, vol. 21(2), pp. 342-348",N/A,N/A,21,10,"In this paper, we propose a method to identify high-quality Wikipedia articles by using implicit positive ratings. One of the major approaches for assessing Wikipedia articles is a text survival ratio based approach. In this approach, when a text survives beyond multiple edits, the text is assessed as high quality. However, the problem is that many low quality articles are misjudged as high quality, because every editor does not always read the whole article. If there is a low quality text at the bottom of a long article, and the text has not seen by the other editors, then the text survives beyond many edits, and the text is assessed as high quality. To solve this problem, we use a section and a paragraph as a unit instead of a whole page. In our method, if an editor edits an article, the system considers that the editor gives positive ratings to the section or the paragraph that the editor edits. From experimental evaluation, we confirmed that the proposed method could improve the accuracy of quality values for articles.",wikipedia; reputation; quality; edit history,https://www.jstage.jst.go.jp/article/ipsjjip/21/2/21_342/_pdf,https://www.jstage.jst.go.jp/article/ipsjjip/21/2/21_342/_article,2,0,1,1,4,Yes,Yes
136,Assessing the Quality of Wikipedia Articles,"ACM, Google Scholar",2021,Quang-Vinh Dang,Conference,"ICMLSC '21: International Conference on Machine Learning and Soft Computing, pp. 1-4",Association for Computing Machinery,"New York City, United States",35,0,"Wikipedia is a very important information reference source for the Internet users. Due to the fact that the content of Wikipedia is the collaborative result from a massive number of participants all over the world, the quality of Wikipedia might be questionable. Over the last decade, many research works are dedicated to solve the issue of Wikipedia quality. In this paper, we present our latest research in determining the quality of Wikipedia articles. The evaluation on the real-world dataset shows that our method outperforms other baseline methods proposed recently.",Wikipedia; machine learning; quality assessment,https://dl.acm.org/doi/pdf/10.1145/3453800.3453801,https://dl.acm.org/doi/10.1145/3453800.3453801,2,1,1,0,4,Yes,Yes
139,Using Morphological and Semantic Features for the Quality Assessment of Russian Wikipedia,"Google Scholar, Web of Science",2017,WÅodzimierz Lewoniewski; Nina Khairova; Krzysztof WÄcel; Nataliia Stratiienko; Witold Abramowicz,Conference,"ICIST '17: International Conference on Information and Software Technologies, pp. 550-560",Springer,"Cham, Switzerland",18,4,"Nowadays, the assessment of the quality and credibility of Wikipedia articles becomes increasingly important. We propose to use morphological and semantic features to estimate the quality of Wikipedia articles in Russian language. We distinguished over 150 linguistic features and divided them into four groups. In these groups, we considered the features of encyclopedic style, readability and subjectivism of the articleâs text. Based on Random Forest as a classification algorithm, we show the most importance linguistic features that affect the quality of Russian Wikipedia articles. We compare the classification results of our four linguistic features groups separately. We have achieved the F-measure of 89,75%.",Quality assessment of texts; Morphological and semantics; features; Russian Wikipedia articles; Random forests classification; Encyclopedic; Readability; Subjectivism,https://link.springer.com/content/pdf/10.1007/978-3-319-67642-5_46.pdf,https://link.springer.com/chapter/10.1007/978-3-319-67642-5_46,2,1,0,1,4,Yes,Yes
142,Quality Assessment of Peer-Produced Content in Knowledge Repositories Using Big Data and Social Networks: The Case of Implicit Collaboration in Wikipedia,"ACM, Google Scholar, Web of Science",2019,Srikar Velichety,Journal,"ACM SIGMIS Database: The DATABASE for Advances in Information Systems, vol. 50(4), pp. 28-51",N/A,N/A,84,3,"This research provides a method for quality assessment of peer-produced content in knowledge repositories using a complementary view of collaboration. Using the definition of collaboration as the action of working with someone to produce something, we identify the aspects of collaboration that the present research on online communities does not consider. To this end, we introduce and define the concept of implicit collaboration and then identify two dimensions and four possible areas of collaboration. In each area, we identify the relevant social network that captures collaboration. Using customized measures on each of the networks that capture various aspects of collaboration, we quantify the utility of implicit collaboration in assessing article quality. Experiments conducted on the complete population of graded English language Wikipedia articles show that all the identified measures improve the predictive accuracy of the existing models by 11.89 percent while improving the class-wise precision by 9-18 percent and the class-wise recall by 5-26 percent. We also find that our method complements the existing quality assessment approaches well. Our research has implications for developing automated quality assessment methods for peer-produced content using big data and social networks.",Implicit Collaboration; Wikipedia; Social Networks; Edits; Discussions,https://dl.acm.org/doi/pdf/10.1145/3371041.3371045,https://dl.acm.org/doi/10.1145/3371041.3371045,2,2,1,0,5,Yes,Yes
145,Equal opportunities in the access to quality online health information? A multi-lingual study on Wikipedia,"ACM, Google Scholar, Web of Science",2021,Luis Couto; Carla Teixeira Lopes,Conference,"OpenSym '21: International Symposium on Open Collaboration, no. 13, pp. 1-13",Association for Computing Machinery,"New York City, United States",45,0,"Wikipedia is a free, multilingual, and collaborative online encyclopedia. Nowadays, it is one of the largest sources of online knowledge, often appearing at the top of the results of the major search engines, being one of the most sought-after resources by the public searching for health information. The collaborative nature of Wikipedia raises security concerns since this information is used for decision-making, especially in the health area. Despite being available in hundreds of idioms, there are asymmetries between idioms, namely regarding their quality. In this work, we compare the quality of health information on Wikipedia between idioms with 100 million native speakers or more, and also in Greek, Italian, Korean, Turkish, Persian, Catalan and Hebrew, for historical tradition. Quality metrics are applied to health and medical articles in English, maintained by WikiProject Medicine, and their versions in the above idioms. With this, we contribute to a clarification of the role of Wikipedia in the access to health information. We demonstrate differences in both the quantity and quality of information available between idioms. English is the idiom with the highest quality in general. Urdu, Greek, Indonesian, and Hindi achieved lower values of quality.",Information Quality; Wikipedia; Health information; Multilingual information access,https://dl.acm.org/doi/pdf/10.1145/3479986.3480000,https://dl.acm.org/doi/10.1145/3479986.3480000,2,0,1,1,4,Yes,Yes
147,Predicting Information Quality Flaws in Wikipedia by Using Classical and Deep Learning Approaches,Google Scholar,2019,GerÃ³nimo BazÃ¡n Pereyra; Carolina Cuello; Gianfranco Capodici; Vanessa JofrÃ©; Edgardo Ferretti; Rodolfo Bonnin; Marcelo Errecalde,Conference,"CACIC '19: Argentine Congress of Computer Science, pp. 3-18",Springer,"Cham, Switzerland",35,3,"Quality flaws prediction in Wikipedia is an ongoing research trend. In particular, in this work we tackle the problem of automatically predicting five out of the ten most frequent quality flaws; namely: No footnotes, Notability, Primary Sources, Refimprove and Wikify. Different classical and deep learning state-of-the-art approaches were studied. From among the evaluated approaches, some of them always reach or improve the existing benchmarks on the test corpus from the 1st International Competition on Quality Flaw Prediction in Wikipedia; a well-known uniform evaluation corpus from this research field. Particularly, the results showed that under-bagged decision trees with different aggregation rules perform best improving the existing benchmarks for four out the five flaws.",Wikipedia; Information Quality; Quality Flaws Prediction; Deep Learning,https://link.springer.com/content/pdf/10.1007/978-3-030-48325-8_1,https://link.springer.com/chapter/10.1007/978-3-030-48325-8_1,2,2,3,0,7,Yes,Yes
148,Quality assessment of Arabic web content: The case of the Arabic Wikipedia,"Google Scholar, Web of Science",2014,Adnan Yahya; Ali Salhi,Conference,"IIT '14: International Conference on Innovations in Information Technology, pp. 36-41",Institute of Electrical and Electronic Engineers,"New York City, United States",16,3,"With the huge size and large diversity of Arabic web content, machine assessment of document quality acquires added importance. Users are in dire need for quality rating of the material returned in response to their queries. The Wikipedia, with its large metadata, has been a topic of extensive research on document quality assessment. Criteria used include text properties and style parameters, contributor and edit characteristics and multimedia components. In this paper we report on our ongoing work to adapt existing document assessment approaches to Arabic content with concentration on the Arabic Wikipedia and present some of the results. We also try to augment that with features specific to Arabic as well as parameters like author expertise and social media presence. One of our goals is an aggregate measure integrating many of the features into a single document quality index. We plan to use Wikipedia article quality assessment results to train general content assessment methods that can be applied to general content that lacks major Wikipedia features.",Document Quality Assessment; Arabic Wikipedia; Web Content Quality; Quality Assessment Parameters,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6987558,https://ieeexplore.ieee.org/document/6987558,2,0,2,1,5,Yes,Yes
149,Determining Quality of Articles in Polish Wikipedia Based on Linguistic Features,"Google Scholar, Web of Science",2018,WÅodzimierz Lewoniewski; Krzysztof WÄcel; Witold Abramowicz,Conference,"ICIST '18: International Conference on Information and Software Technologies, pp. 546-558",Springer,"Cham, Switzerland",31,3,"Wikipedia is the most popular and the largest user-generated source of knowledge on the Web. Quality of the information in this encyclopedia is often questioned. Therefore, Wikipedians have developed an award system for high quality articles, which follows the specific style guidelines. Nevertheless, more than 1.2 million articles in Polish Wikipedia are unassessed. This paper considers over 100 linguistic features to determine the quality of Wikipedia articles in Polish language. We evaluate our models on 500 000 articles of Polish Wikipedia. Additionally, we discuss the importance of linguistic features for quality prediction.",Wikipedia; NLP; Data Quality; Quality Assessment Random Forest ; Polish; Linguistic Features; Linguistics Data Mining,https://link.springer.com/content/pdf/10.1007/978-3-319-99972-2_45,https://link.springer.com/chapter/10.1007/978-3-319-99972-2_45,2,1,1,1,5,Yes,Yes
150,Quality Classification of ASEAN Wikipedia Articles using Statistical Features,"Google Scholar, Web of Science",2018,Kanchana Saengthongpattana; Thepchai Supnithi; Nuanwan Soonthornphisaj,Conference,"iSAI-NLP '18: International Joint Symposium on Artificial Intelligence and Natural Language Processing, pp. 1-6",Institute of Electrical and Electronic Engineers,"New York City, United States",12,3,"The quality of Wikipedia articles is still the main concerned in all languages. Wikipedia relies mostly on human editors and administrators to provide the quality of content. But the magnitude of Wikipedia content makes locating all instances of article very time consuming. Therefore, we need the automatic quality detection that can help users to evaluate the quality of articles. In this paper, we propose the feature set to applied for the ASEAN language Wikipedia articles. We investigate the statistical features such as # of link, # of infobox, length of article, # of headings, # of files, # of contributors, # of viewer, # of written articles found in other languages, and # of templates applied in the article. The experiments are perform using NaÃ¯ve Bayes and Decision tree algorithm. We found that the accuracy of Decision tree (96.34%) outperform NaÃ¯ve Bayes (86.47%). Moreover, we found that the statistical features play an important role in quality classification of Vietnamese, Indonesian, Malaysian, Thai, and Tagalog/Philippines Wikipedia articles.",Quality of articles; Southeast Asian languages Wikipedia; NaÃ¯ve Bayes; Decision tree; Statistical feature,https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=8692954&ref=,https://ieeexplore.ieee.org/document/8692954/,2,1,1,1,5,Yes,Yes
155,Cumulative Experience and Recent Behavior and their Relation to Content Quality on Wikipedia,"Google Scholar, Web of Science",2017,Michail Tsikerdekis,Journal,"Interacting with Computers, vol. 29(5), pp. 737-754",N/A,N/A,64,1,"Cumulative experience is often seen as a major factor for influencing content quality in collaborative projects such as Wikipedia. However, past studies often utilize cumulative experience based on the quantity of work rather than quality and context. Moreover, the perspective on cumulative experience assumes a final destination for user behavior, whereas much of the literature indicates that user behavior changes over time. This paper aims to address these two factors by providing better descriptions and context to determine their effect on content quality. The study rematerialized these factors based on the quality of work and built a comprehensive model of how article quality materializes through groups of users. Regression models in this study indicate that recent behavior is a more powerful predictor of content quality change than cumulative experience. The implications of the findings impact the design of task-routing systems as well as designers and group managersâ work in collaborative projects.",collaborative and social computing; computer supported cooperative work; blogs; wikis and similar,https://academic.oup.com/iwc/article/29/5/737/3885842,https://academic.oup.com/iwc/article/29/5/737/3885842,1,0,1,0,2,No,No
157,On Quality Assesement in Wikipedia Articles Based on Markov Random Fields,"Google Scholar, Web of Science",2017,Rajmund Kleminski; Tomasz Kajdanowicz; Roman Bartusiak; Przemyslaw Kazienko,Conference,"ACIIDS '17: Asian Conference on Intelligent Information and Database Systems, pp. 782-791",Springer,"Cham, Switzerland",13,2,"This article investigates the possibility of accurate quality prediction of resources generated by communities based on the crowd-generated content. We use data from Wikipedia, the prime example of community-run site, as our object of study. We define the quality as a distribution of user-assigned grades across a predefined range of possible scores and present a measure of distribution similarity to quantify the accuracy of a prediction. The proposed method of quality prediction is based on Markov Random Field and its Loopy Belief Propagation implementation. Based on our results, we highlight key problems in the approach as presented, as well as trade-offs caused by relying solely on network structure and characteristics, excluding metadata. The overall results of content quality prediction are promising in homophilic networks.",Wikipedia; Quality prediction; Iterative classification,https://link.springer.com/content/pdf/10.1007/978-3-319-54472-4_73,https://link.springer.com/chapter/10.1007/978-3-319-54472-4_73,1,0,1,0,2,No,No
159,Assessing Quality Values of Wikipedia Articles Using Implicit Positive and Negative Ratings,Google Scholar,2012,Yu Suzuki,Conference,"WAIM '12: International Conference on Web-Age Information Management, pp. 127-138",Springer,"Berlin, Heidelberg",10,3,"In this paper, we propose a method to identify high-quality Wikipedia articles by mutually evaluating editors and text using implicit positive and negative ratings. One of major approaches for assessing Wikipedia articles is a text survival ratio based approach. However, the problem of this approach is that many low quality articles are misjudged as high quality, because of two issues. This is because, every editor does not always read the whole articles. Therefore, if there is a low quality text at the bottom of a long article, and the text have not seen by the other editors, then the text survives beyond many edits, and the survival ratio of the text is high. To solve this problem, we use a section or a paragraph as a unit of remaining instead of a whole page. This means that if an editor edits an article, the system treats that the editor gives positive ratings to the section or the paragraph that the editor edits. This is because, we believe that if editors edit articles, the editors may not read the whole page, but the editors should read the whole sections or paragraphs, and delete low-quality texts. From experimental evaluation, we confirmed that the proposed method could improve the accuracy of quality values for articles. ",Wikipedia; Edit History; Quality; Reputation,https://link.springer.com/content/pdf/10.1007/978-3-642-32281-5_13,https://link.springer.com/chapter/10.1007/978-3-642-32281-5_13,2,0,1,1,4,Yes,Yes
160,On the Assessment of Information Quality in Spanish Wikipedia,Google Scholar,2016,Guido Urquiza; MatÃ­as Soria; SebastiÃ¡n PÃ©rez Casseignau; Edgardo Ferretti; Sergio Alejandro GÃ³mez; Marcelo Errecalde,Conference,"CACIC '19: Argentine Congress of Computer Science, pp. 702-711",National University of La Plata,"La Plata, Argentina",21,2,"Featured Articles (FA) are considered to be the best articles that Wikipedia has to offer and in the last years, researchers have found interesting to analyze whether and how they can be distinguished from âordinaryâ articles. Likewise, identifying what issues have to be enhanced or fixed in ordinary articles in order to improve their quality is a recent key research trend. Most of the approaches developed in these research trends have been proposed for the English Wikipedia. However, few efforts have been accomplished in Spanish Wikipedia, despite being Spanish, one of the most spoken languages in the world by native speakers. In this respect, we present a first breakdown of Spanish Wikipediaâs quality flaw structure. Besides, we carry out a study to automatically assess information quality in Spanish Wikipedia, where FA identification is evaluated as a binary classification task. The results obtained show that FA identification can be performed with an F1 score of 0.81, using a document model consisting of only twenty six features and AdaBoosted C4.5 decision trees as classification algorithm.",Wikipedia; Information Quality; Featured Article Identification; Quality Flaws Prediction,http://sedici.unlp.edu.ar/bitstream/handle/10915/56750/Documento_completo.pdf-PDFA.pdf?isAllowed=y&sequence=1,http://sedici.unlp.edu.ar/handle/10915/56750,2,1,2,1,6,Yes,Yes
161,Assessing the quality of health-related Wikipedia articles with generic and specific metrics,"ACM, Google Scholar, Web of Science",2021,Luis Couto; Carla Teixeira Lopes,Conference,"WWW '21: The Web Conference, pp. 640-647",Association for Computing Machinery,"New York City, United States",31,1,"Wikipedia is an online, free, multi-language, and collaborative encyclopedia, currently one of the most significant information sources on the web. The open nature of Wikipedia contributions raises concerns about the quality of its information. Previous studies have addressed this issue using manual evaluations and proposing generic measures for quality assessment. In this work, we focus on the quality of health-related content. For this purpose, we use general and health-specific features from Wikipedia articles to propose health-specific metrics. We evaluate these metrics using a set of Wikipedia articles previously assessed by WikiProject Medicine. We conclude that it is possible to combine generic and specific metrics to determine health-related contentâs information quality. These metrics are computed automatically and can be used by curators to identify quality issues. Along with the explored features, these metrics can also be used in approaches that automatically classify the quality of Wikipedia health-related articles.",Information Quality; Wikipedia; Health-related Content,https://dl.acm.org/doi/pdf/10.1145/3442442.3452355,https://dl.acm.org/doi/10.1145/3442442.3452355,2,0,2,0,4,Yes,Yes
162,Ontology-Based Classifiers for Wikipedia Article Quality Classification,Google Scholar,2017,Kanchana Saengthongpattana; Thepchai Supnithi; Nuanwan Soonthornphisaj,Conference,"iSAI-NLP '17: International Joint Symposium on Artificial Intelligence and Natural Language Processing, pp. 68-81",Springer,"Cham, Switzerland",28,2,Quality of Wikipedia article is the main issues that need to be solved. This research proposes the ontology-based classification framework that considers the quality of article in term of its comprehensive content which is one of the properties for featured and good articles in Thai Wikipedia. We create concepts or main ideas of articles in three domains using ontology as a knowledge representation. Knowledge based are created using OAM tool that do data mapping and classify the quality of articles via set of rules. We have investigated the ontology approach which combined Naive Bayes classifier and found that the precision of our proposed method outperform traditional Naive Bayes for two times.,Ontology; Thai Wikipedia; Concept feature; NaÃ¯ve Bayes Classifier,https://link.springer.com/content/pdf/10.1007/978-3-319-94703-7_7,https://link.springer.com/chapter/10.1007/978-3-319-94703-7_7,2,1,0,1,4,Yes,Yes
169,Assessing the Quality of Thai Wikipedia Articles Using Concept and Statistical Features,"Google Scholar, Web of Science",2014,Kanchana Saengthongpattana; Nuanwan Soonthornphisaj,Conference,"WorldCIST '14: World Conference on Information Systems and Technologies, pp. 513-523 ",Springer,"Cham, Switzerland",12,3,"The quality evaluation of Thai Wikipedia articles relies on user consideration. There are increasing numbers of articles every day therefore the automatic evaluation method is needed for user. Components of Wikipedia articles such as headers, pictures, references, and links are useful to indicate the quality of articles. However readers need complete content to cover all of concepts in that article. The concept features are investigated in this work. The aim of this research is to classify Thai Wikipedia articles into two classes namely high-quality and low-quality class. Three article domains (Biography, Animal, and Place) are testes with decision tree and Naive Bayes. We found that Naive Bayes gets high TP Rate compared to decision tree in every domain. Moreover, we found that the concept feature plays an important role in quality classification of Thai Wikipedia articles. ",Quality of Thai Wikipedia articles; NaÃ¯ve Bayes; Decision tree; Concept feature; Statistical feature,https://link.springer.com/content/pdf/10.1007/978-3-319-05951-8_49,https://link.springer.com/chapter/10.1007/978-3-319-05951-8_49,2,1,1,1,5,Yes,Yes
172,Structure-Based Features for Predicting the Quality of Articles in Wikipedia,Google Scholar,2017,Baptiste de La Robertie; Yoann Pitarch; Olivier Teste,Book,Prediction and Inference from Social Networks and Social Media,Springer,"Cham, Switzerland",18,1,"Success of Wikipedia is decidedly due to the free availability of high quality articles across many different expertise areas. If most of these resolute collaborations between authoritative users might constitute referenceable sources, Wikipedia is not sheltered from well-identified problems regarding articles quality, e.g., reputability of third-party sources and vandalism. Because of the huge number of articles and the intensive edit rate, it is not reasonable to even consider the manual evaluation of the content quality of each article. In this paper, we tackle the problem of modeling and predicting the quality of articles in collaborative platforms. We propose a quality model integrating both temporal and structural features captured from the implicit peer review process enabled by Wikipedia. A generic HITS-like framework is developed and able to capture both the quality of the content and the authority of the associated authors. Notably, a mutual reinforcement principle held between articles quality and authorâs authority is exploited in order to take advantage of the collaborative graph generated by the users. Experiments conducted on a set of representative data from Wikipedia show the effectiveness of the computed indicators both in an unsupervised and supervised scenario.",N/A,https://link.springer.com/content/pdf/10.1007/978-3-319-51049-1_6,https://link.springer.com/chapter/10.1007/978-3-319-51049-1_6,2,1,1,0,4,Yes,Yes
173,Evaluating Article Quality and Editor Reputation in Wikipedia,Google Scholar,2013,Yuqing Lu; Lei Zhang; Juan-Zi Li,Conference,"CSWS '13: China Semantic Web Symposium and Web Science Conference, pp. 215-227",Springer,"Berlin, Heidelberg",23,1,"We study a novel problem of quality and reputation evaluation for Wikipedia articles. We propose a difficult and interesting question: How to generate reasonable article quality score and editor reputation in a framework at the same time? In this paper, We propose a dual wing factor graph(DWFG) model, which utilizes the mutual reinforcement between articles and editors to generate article quality and editor reputation. To learn the proposed factor graph model, we further design an efficient algorithm. We conduct experiments to validate the effectiveness of the proposed model. By leveraging the belief propagation between articles and editors, our approach obtains significant improvement over several alternative methods(SVM, LR, PR, CRF). ",factor graph; quality evaluation; editor reputation,https://link.springer.com/content/pdf/10.1007/978-3-642-54025-7_19,https://link.springer.com/chapter/10.1007/978-3-642-54025-7_19,2,2,1,0,5,Yes,Yes
181,Understanding the 'Quality Motion' of Wikipedia Articles Through Semantic Convergence Analysis,Google Scholar,2015,Huijing Deng; Bernadetta Tarigan; Mihai Grigore; Juliana Sutanto,Conference,"HCIB '15: International Conference on HCI in Business, pp. 64-75",Springer,"Cham, Switzerland",9,1,"To better inform the users of the articles quality, Wikipedia assigns quality labels to the articles. While most of the existing studies of the Wikipedia phenomenon took the quality ratings provided by Wikipedia as the outcome variable of their research, a few yet growing number of studies ask expert raters to rate the quality of selected Wikipedia articles because of their doubts in Wikipediaâs ratings. This study aims to check if Wikipediaâs ratings really reflect its stated criteria. According to Wikipedia criteria, having abundant and stable content is the key to articleâs quality promotion; we therefore examine the content change in terms of quantity change and content stability by showing the semantic convergence. We found out that the quantity of content change is significant in the promoted articles, which complies with Wikipediaâs stated criteria. ",Wikipedia; Article quality; Quality motion; Content change; Semantic convergence,https://link.springer.com/content/pdf/10.1007/978-3-319-20895-4_7,https://link.springer.com/chapter/10.1007/978-3-319-20895-4_7,1,0,1,0,2,No,No
197,Predicting Low-Quality Wikipedia Articles Using Userâs Judgements,"Google Scholar, Web of Science",2015,Ning Zhang; Lingyun Ruan; Luo Si,Book,"Roles, Trust, and Reputation in Social Media Knowledge Markets",Springer,"Cham, Switzerland",10,0,"Wikipedia has become the most popular on-line encyclopedia. Millions of users rely on it to obtain desired knowledge and thus it becomes important and practical to model the quality of Wikipedia articles and to have inferior contents which bother readers or even mislead readers to be predicted. While identifying low-quality articles with manual efforts is a possible solution, it costs too much manpower and is too time-consuming. In this paper, we utilize article ratings from Wikipedia users for the first time to assess article quality. We define âlow-qualityâ based on those ratings and design automatic methods to identify potential low-quality articles. More specifically, we formulate the problem as a set of binary classification problems and label articles according to whether they are âlow-qualityâ. We compare two baseline algorithms and Logistic Regression algorithm, and the results indicate that it is promising to design effective and efficient automatic solutions for the task. We believe that our work is important for ensuring the quality of Wikipedia, as well as other knowledge markets. ",N/A,https://link.springer.com/content/pdf/10.1007/978-3-319-05467-4_6,https://link.springer.com/chapter/10.1007/978-3-319-05467-4_6,2,1,2,0,5,Yes,Yes
199,An investigation of the relationship between the amount of extra-textual data and the quality of Wikipedia articles,"ACM, Google Scholar",2013,Marcelo Yuji Himoro; RaÃ­za Hanada; Marco Cristo; Maria da GraÃ§a Campos Pimentel,Conference,"WebMedia '13: Brazilian Symposium on Multimedia and the Web, pp. 333-336",Association for Computing Machinery,"New York City, United States",23,0,"Wikipedia, a web-based collaboratively maintained free encyclopedia, is emerging as one of the most important websites on the internet. However, its openness raises many concerns about the quality of the articles and how to assess it automatically. In the Portuguese-speaking Wikipedia, articles can be rated by bots and by the community. In this paper, we investigate the correlation between these ratings and the count of media items (namely images and sounds) through a series of experiments. Our results show that article ratings and the count of media items are correlated.",Extra-textual data; Correlations; Wikipedia; Content Quality,https://dl.acm.org/doi/pdf/10.1145/2526188.2526218,https://dl.acm.org/doi/10.1145/2526188.2526218,2,0,1,1,4,Yes,Yes
202,Monitoring network structure and content quality of signal processing articles on wikipedia,"Google Scholar, Web of Science",2013,Tao-Chi Lee; Jayakrishnan Unnikrishnan,Conference,"ICASSP '13: International Conference on Acoustics, Speech and Signal Processing, pp. 8766-8770",Institute of Electrical and Electronic Engineers,"New York City, United States",15,0,"Wikipedia has become a widely-used resource on signal processing. However, the freelance-editing model of Wikipedia makes it challenging to maintain a high content quality. We develop techniques to monitor the network structure and content quality of Signal Processing (SP) articles on Wikipedia. Using metrics to quantify the importance and quality of articles, we generate a list of SP articles on Wikipedia arranged in the order of their need for improvement. The tools we use include the HITS and PageRank algorithms for network structure, crowdsourcing for quantifying article importance and known heuristics for article quality.",Signal processing; Wikipedia; network analysis; crowdsourcing; information quality metrics,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6639378,https://ieeexplore.ieee.org/document/6639378,1,0,1,0,2,No,No
262,Cultural diversity of quality of information on Wikipedias,ACM,2017,Dariusz Jemielniak; Maciej Wilamowski,Journal,"Journal of the Association for Information Science and Technology, vol. 68(10), pp. 2460-2470",N/A,N/A,49,33,"This article explores the relationship between linguistic culture and the preferred standards of presenting information based on article representation in major Wikipedias. Using primary research analysis of the number of images, references, internal links, external links, words, and characters, as well as their proportions in Good and Featured articles on the eight largest Wikipedias, we discover a high diversity of approaches and format preferences, correlating with culture. We demonstrate that highâquality standards in information presentation are not globally shared and that in many aspects, the language culture's influence determines what is perceived to be proper, desirable, and exemplary for encyclopedic entries. As a result, we demonstrate that standards for encyclopedic knowledge are not globally agreedâupon and âobjectiveâ but local and very subjective.",N/A,https://crow.kozminski.edu.pl/papers/cultures%20of%20wikipedias.pdf,https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.23901,1,0,1,1,3,No,No
338,On the Feasibility of External Factual Support as Wikipedia's Quality Metric,Web of Science,2017,Carlos G. VelÃ¡zquez; Leticia C. Cagnina; Marcelo Errecalde,Journal,"Processamiento del Lenguaje Natural, vol. 58, pp. 93-100",N/A,N/A,20,6,"Developing metrics to estimate the information quality of Wikipedia articles is an interesting and important research area. In this article, we propose and analyse the feasibility, of a new quality metric based on the âexternal factual supportâ of an article. The rationale behind this metric is identified, a formal definition of the metric is presented and some implementation aspects are introduced. Preliminary results show the feasibility of our proposal and its potential to discriminate high quality versus low quality Wikipediaâs articles.",Quality metrics; Wikipedia; featured articles; external support,http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/5417/3181,http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/5417,1,0,1,0,2,No,No
359,Quality of Wikipedia Articles: Analyzing Features and Building a Ground Truth for Supervised Classification,Web of Science,2019,Elias Bassani; Marco Viviani,Conference,"KDIR '19: International Conference on Knowledge Discovery and Information Retrieval, pp. 338-346",Vienna University of Technology,"Vienna, Austria",35,6,"Wikipedia is nowadays one of the biggest online resources on which users rely as a source of information. The amount of collaboratively generated content that is sent to the online encyclopedia every day can let to the possible creation of low-quality articles (and, consequently, misinformation) if not properly monitored and revised. For this reason, in this paper, the problem of automatically assessing the quality of Wikipedia articles is considered. In particular, the focus is (i) on the analysis of groups of hand-crafted features that can be employed by supervised machine learning techniques to classify Wikipedia articles on qualitative bases, and (ii) on the analysis of some issues behind the construction of a suitable ground truth. Evaluations are performed, on the analyzed features and on a specifically built labeled dataset, by implementing different supervised classifiers based on distinct machine learning algorithms, which produced promising results.",Data Quality; Wikipedia; Supervised Classification; Feature Analysis; Ground Truth Building,https://pdfs.semanticscholar.org/f118/37c304132974ec60f0fa98a2e8ecf161a313.pdf,https://www.scitepress.org/Link.aspx?doi=10.5220/0008149303380346,3,3,3,0,9,Yes,Yes
1001,Predicting the quality of user contributions via LSTMs,Backward Citation Tracking,2016,Rakshit Agrawal; Luca deAlfaro,Conference,"OpenSym '16: International Symposium on Open Collaboration, no. 19, pp. 1-10",Association for Computing Machinery,"New York City, United States",20,8,"In many collaborative systems it is useful to automatically estimate the quality of new contributions; the estimates can be used for instance to flag contributions for review. To predict the quality of a contribution by a user, it is useful to take into account both the characteristics of the revision itself, and the past history of contributions by that user. In several approaches, the user's history is first summarized into a number of features, such as number of contributions, user reputation, time from previous revision, and so forth. These features are then passed along with features of the current revision to a machine-learning classifier, which outputs a prediction for the user contribution. The summarization step is used because the usual machine learning models, such as neural nets, SVMs, etc. rely on a fixed number of input features. We show in this paper that this manual selection of summarization features can be avoided by adopting machine-learning approaches that are able to cope with temporal sequences of input. In particular, we show that Long-Short Term Memory (LSTM) neural nets are able to process directly the variable-length history of a user's activity in the system, and produce an output that is highly predictive of the quality of the next contribution by the user. Our approach does not eliminate the process of feature selection, which is present in all machine learning. Rather, it eliminates the need for deciding which features from a user's past are most useful for predicting the future: we can simply pass to the machine-learning apparatus all the past, and let it come up with an estimate for the quality of the next contribution. We present models combining LSTM and NN for predicting revision quality and show that the prediction accuracy attained is far superior to the one obtained using the NN alone. More interestingly, we also show that the prediction attained is superior to the one obtained using user reputation as a feature summarizing the quality of a user's past work. This can be explained by noting that the primary function of user reputation is to provide an incentive towards performing useful contributions, rather than to be a feature optimized for prediction of future contribution quality. We also show that the LSTM output changes in a natural way in response to user behavior, increasing when the user performs a sequence of good quality contributions, and decreasing when the user performs a sequence of low-quality work. The LSTM output for a user could thus be usefully shown to other users, alongside the user's reputation and other information.",Reputation; Machine Learning; Wikipedia; LSTM; Neural Networks,https://dl.acm.org/doi/pdf/10.1145/2957792.2957811,https://dl.acm.org/doi/10.1145/2957792.2957811,1,2,1,0,4,Yes,Yes
1002,A general multiview framework for assessing the quality of collaboratively created content on web 2.0,Backward Citation Tracking,2016,Daniel Hasan Dalip; Marcos AndrÃ© GonÃ§alves; Marco Cristo; PÃ¡vel Calado,Journal,"Journal of the Association for Information Science and Technology, vol. 68(2), pp. 286-308",N/A,N/A,54,38,"User-generated content is one of the most interesting phenomena of current published media, as users are now able not only to consume, but also to produce content in a much faster and easier manner. However, such freedom also carries concerns about content quality. In this work, we propose an automatic framework to assess the quality of collaboratively generated content. Quality is addressed as a multidimensional concept, modeled as a combination of independent assessments, each regarding different quality dimensions. Accordingly, we adopt a machine-learning (ML)-based multiview approach to assess content quality. We perform a thorough analysis of our framework on two different domains: Questions and Answer Forums and Collaborative Encyclopedias. This allowed us to better understand when and how the proposed multiview approach is able to provide accurate quality assessments. Our main contributions are: (a) a general ML multiview framework that takes advantage of different views of quality indicators; (b) the improvement (up to 30%) in quality assessment over the best state-of-the-art baseline methods; (c) a thorough feature and view analysis regarding impact, informativeness, and correlation, based on two distinct domains.",N/A,https://sci-hub.hkvisa.net/10.1002/asi.23650,https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.23650,3,1,2,0,6,Yes,Yes
1003,Automatic assessment of document quality in web collaborative digital libraries,Backward Citation Tracking,2011,Daniel Hasan Dalip; Marcos AndrÃ© GonÃ§alves; Marco Cristo; PÃ¡vel Calado,Journal,"Journal of Data and Information Quality, vol. 2(3), no. 14, pp. 1-30",N/A,N/A,65,71,"The old dream of a universal repository containing all of human knowledge and culture is becoming possible through the Internet and the Web. Moreover, this is happening with the direct collaborative participation of people. Wikipedia is a great example. It is an enormous repository of information with free access and open edition, created by the community in a collaborative manner. However, this large amount of information, made available democratically and virtually without any control, raises questions about its quality. In this work, we explore a significant number of quality indicators and study their capability to assess the quality of articles from three Web collaborative digital libraries. Furthermore, we explore machine learning techniques to combine these quality indicators into one single assessment. Through experiments, we show that the most important quality indicators are those which are also the easiest to extract, namely, the textual features related to the structure of the article. Moreover, to the best of our knowledge, this work is the first that shows an empirical comparison between Web collaborative digital libraries regarding the task of assessing article quality.",Quality assessment; quality features; wiki; machine learning; SVM,https://dl.acm.org/doi/pdf/10.1145/2063504.2063507,https://dl.acm.org/doi/10.1145/2063504.2063507,2,1,3,0,6,Yes,Yes
1004,Quality assessment of collaborative content with minimal information,Backward Citation Tracking,2014,Daniel Hasan Dalip; Harlley Lima; Marcos AndrÃ© GonÃ§alves; Marco Cristo; PÃ¡vel Calado,Conference,"JCDL '14: ACM/IEEE Joint Conference on Digital Libraries, pp. 201-210",Association for Computing Machinery,"New York City, United States",26,28,"Content generated by users is one of the most interesting phenomena of published media. However, the possibility of unrestricted edition is a source of doubts about its quality. This issue has motivated many studies on how to automatically assess content quality in collaborative web sites. Generally, these studies use machine learning techniques to combine large number of quality indicators into a single value representing the overall quality of the document. This need for a high number of indicators, however, has detrimental implications both on the efficiency and on the effectiveness of the quality assessment algorithms. In this work, we exploit and extend a feature selection method based on the SPEA2 multi-objective genetic algorithm. Results show that we can reduce the feature set to a fraction of 15% through 25% of the original, while obtaining error rates comparable to the state of the art.",Quality Assessment; Wikipedia; Machine Learning; Feature Selection; Genetic Algorithm,https://dl.acm.org/doi/pdf/10.5555/2740769.2740804,https://dl.acm.org/doi/10.5555/2740769.2740804,2,2,3,0,7,Yes,Yes
1006,Review-based ranking of Wikipedia articles,Backward Citation Tracking,2009,Yasser Ganjisaffar; Sara Javanmardi; Cristina Lopes,Conference,CASON '09: International Conference on Computational Aspects of Social Networks,Institute of Electrical and Electronic Engineers,"New York City, United States",33,10,"Wikipedia, the largest encyclopedia on the Web, is often seen as the most successful example of crowdsourcing. The encyclopedic knowledge it accumulated over the years is so large that one often uses search engines, to find information in it. In contrast to regular Web pages, Wikipedia is fairly structured, and articles are usually accompanied with history pages, categories and talk pages. The meta-data available in these pages can be analyzed to gain a better understanding of the content and quality of the articles. We discuss how the rich meta-data available in wiki pages can be used to provide better search results in Wikipedia. Built on the studies on ""Wisdom of Crowds"" and the effectiveness of the knowledge collected by a large number of people, we investigate the effect of incorporating the extent of review of an article in the quality of rankings of the search results. The extent of review is measured by the number of distinct editors contributed to the articles and is extracted by processing Wikipedia's history pages. We compare different ranking algorithms that explore combinations of text-relevancy, PageRank, and extent of review. The results show that the review-based ranking algorithm which combines the extent of review and text-relevancy outperforms the rest; it is more accurate and less computationally expensive compared to PageRank-based rankings.",Wikipedia; Search; Ranking,https://ieeexplore.ieee.org/document/5176107/,https://ieeexplore.ieee.org/document/5176107/,1,1,1,0,3,No,No
1010,A classifier to determine which Wikipedia biographies will be accepted,Backward Citation Tracking,2015,Nir Ofek; Lior Rokach,Journal,"Journal of the Association for Information Science and Technology, vol. 66(1), pp. 213-218",N/A,N/A,13,8,"Wikipedia, like other encyclopedias, includes biographies of notable people. However, because it is jointly written by many contributors, it is subject to constant manipulation by contributors attempting to add biographies of non-notable people. Over time, Wikipedia has developed inclusion criteria for notable people (e.g., receiving a significant award) based on which newly contributed biographies are evaluated. In this paper we present and analyze a set of simple indicators that can be used to predict which article will eventually be accepted. These indicators do not refer to the content itself, but to meta-content features (such as the number of categories that the biography is associated with) and to author-based features (such as if it is a first-time author). By training a classifier on these features, we successfully reached a high predictive performance (area under the receiver operating characteristic [ROC] curve [AUC] of 0.97) even though we overlooked the actual biography text.",N/A,https://sci-hub.hkvisa.net/10.1002/asi.23199,https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.23199,2,1,1,0,4,Yes,Yes
1012,A framework for information quality assessment,Backward Citation Tracking,2007,Besiki Stvilia; Les Gasser; Michael B. Twidale; Linda C. Smith,Journal,"Journal of the Association for Information Science and Technology, vol. 58(12), pp. 1720-1733",N/A,N/A,43,606,"One cannot manage information quality (IQ) without first being able to measure it meaningfully and establishing a causal connection between the source of IQ change, the IQ problem types, the types of activities affected, and their implications. In this article we propose a general IQ assessment framework. In contrast to context-specific IQ assessment models, which usually focus on a few variables determined by local needs, our framework consists of comprehensive typologies of IQ problems, related activities, and a taxonomy of IQ dimensions organized in a systematic way based on sound theories and practices. The framework can be used as a knowledge resource and as a guide for developing IQ measurement models for many different settings. The framework was validated and refined by developing specific IQ measurement models for two large-scale collections of two large classes of information objects: Simple Dublin Core records and online encyclopedia articles.",N/A,https://myweb.fsu.edu/bstvilia/papers/stvilia_IQFramework_p.pdf,https://onlinelibrary.wiley.com/doi/10.1002/asi.20652,2,0,2,0,4,Yes,Yes
1013,Assessing information quality of a community-based encyclopedia,Backward Citation Tracking,2005,Besiki Stvilia; Michael B. Twidale; Linda C. Smith; Les Gasser,Conference,"ICIQ '05: International Conference on Information Quality, pp. 442-454",Massachusetts Institute of Technology ,"Cambridge, Massachusetts",26,401,"Effective information quality analysis needs powerful yet easy ways to obtain metrics. The English version of Wikipedia provides an extremely interesting yet challenging case for the study of Information Quality dynamics at both macro and micro levels. We propose seven IQ metrics which can be evaluated automatically and test the set on a representative sample of Wikipedia content. The methodology of the metrics construction and the results of tests, along with a number of statistical characterizations of Wikipedia articles, their content construction, process metadata and social context are reported.",N/A,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.6243&rep=rep1&type=pdf,https://www.semanticscholar.org/paper/Assessing-Information-Quality-of-a-Community-Based-Stvilia-Twidale/dd888dddccc2075a44f99ec2380fda652040afaf,3,1,2,0,6,Yes,Yes
1016,Analysing wiki quality using probabilistic model checking,Backward Citation Tracking,2015,Giuseppe De Ruvo; Antonella Santone,Conference,WET ICE '15: IEEE International Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises,Institute of Electrical and Electronic Engineers,"New York City, United States",23,8,"Wikis delineate a new work tool in enterprises and they are spreading everywhere. Indeed, they are often used as internal documentation for various in-house systems and applications as well as powerful tools for collaboration and knowledge sharing. As occurs with software, the fundamental growth of a wiki may lead to its degradation. The quality of wikis, especially in enterprise contexts, should not play a trivial role. Software quality is a very discussed topic, but there are not many studies regarding the quality of wikis. We propose a probabilistic model to represent wikis and to investigate their quality. Due to the similarity with the World Wide Web it is natural to consider the popular Google PageRank (with minor modifications) to calculate probabilities between pages. Each wiki category, a set of wiki pages, is modelled using the PRISM language in order to verify specific properties in PCTL*. Experiments conducted on a adequate number of (enterprise) wikis assess the validity of our methodology.",Wiki; Probabilistic Temporal Logic; PRISM; Quality,https://ieeexplore.ieee.org/document/7194365,https://ieeexplore.ieee.org/document/71943655,2,0,1,0,3,No,No
1019,Identifying featured articles in wikipedia: writing style matters,Backward Citation Tracking,2010,Nedim Lipka; Benno Stein,Conference,"WWW '10: International Conference on the World Wide Web, pp. 1147-1148",Association for Computing Machinery,"New York City, United States",10,120,"Wikipedia provides an information quality assessment model with criteria for human peer reviewers to identify featured articles. For this classification task ""Is an article featured or not?"" we present a machine learning approach that exploits an article's character trigram distribution. Our approach differs from existing research in that it aims to writing style rather than evaluating meta features like the edit history. The approach is robust, straightforward to implement, and outperforms existing solutions. We underpin these claims by an experiment design where, among others, the domain transferability is analyzed. The achieved performances in terms of the F-measure for featured articles are 0.964 within a single Wikipedia domain and 0.880 in a domain transfer situation.",Wikipedia; Information Quality; Domain Transfer,https://dl.acm.org/doi/pdf/10.1145/1772690.1772847,https://dl.acm.org/doi/10.1145/1772690.1772847,2,1,1,0,4,Yes,Yes
1020,Exploring the Feasibility of Automatically Rating Online Article Quality,Backward Citation Tracking,2007,Laura Rassbach; Trevor Blackford; Brian Mingus,Conference,Wikimania '07: Wikimania Conference,Wikimedia Foundation,"San Francisco, California",27,38,"We demonstrate the feasibility of building an automatic system to assign quality ratings to articles in Wikipedia, the online encyclopedia. Our preliminary system uses a Maximum Entropy classification model trained on articles handtagged for quality by humans. This simple system demonstrates extremely good results, with significant avenues of improvement still to explore.",Quality; Wikipedia; Maximum Entropy,https://upload.wikimedia.org/wikipedia/wikimania2007/d/d3/RassbachPincockMingus07.pdf,https://scholar.google.pt/citations?view_op=view_citation&hl=pt-PT&user=T_sFnwoAAAAJ&citation_for_view=T_sFnwoAAAAJ:u-x6o8ySG0sC,2,1,1,0,4,Yes,Yes
1021,Classifying Wikipedia articles using network motif counts and ratios,Backward Citation Tracking,2012,Guangyu Wu; Martin Harrigan; PÃ¡draig Cunningham,Conference,"WikiSym '12: International Symposium on Wikis and Open Collaboration, no. 12, pp. 1-12",Association for Computing Machinery,"New York City, United States",25,30,"Because the production of Wikipedia articles is a collaborative process, the edit network around a article can tell us something about the quality of that article. Articles that have received little attention will have sparse networks; at the other end of the spectrum, articles that are Wikipedia battle grounds will have very crowded networks. In this paper we evaluate the idea of characterizing edit networks as a vector of motif counts that can be used in clustering and classification. Our objective is not immediately to develop a powerful classifier but to assess what is the signal in network motifs. We show that this motif count vector representation is effective for classifying articles on the Wikipedia quality scale. We further show that ratios of motif counts can effectively overcome normalization problems when comparing networks of radically different sizes.",Wikipedia Quality; Edit Networks,https://dl.acm.org/doi/pdf/10.1145/2462932.2462948,https://dl.acm.org/doi/10.1145/2462932.2462948,2,2,1,0,5,Yes,Yes
1024,Does it matter who contributes: a study on featured articles in the german wikipedia,Backward Citation Tracking,2007,Klaus Stein; Claudia Hess,Conference,"HT '07: Conference on Hypertext and Hypermedia, pp. 171-174",Association for Computing Machinery,"New York City, United States",9,107,"The considerable high quality of Wikipedia articles is often accredited to the large number of users who contribute to Wikipedia's encyclopedia articles, who watch articles and correct errors immediately. In this paper, we are in particular interested in a certain type of Wikipedia articles, namely, the featured articles - articles marked by a community's vote as being of outstanding quality. The German Wikipedia has the nice property that it has two types of featured articles: excellent and worth reading. We explore on the German Wikipedia whether only the mere number of contributors makes the difference or whether the high quality of featured articles results from having experienced authors contributing with a reputation for high quality contributions. Our results indicate that it does matter who contributes.",wiki; Wikipedia; measures of quality and reputation; statistical analysis of Wikipedia; collaborative working,https://dl.acm.org/doi/pdf/10.1145/1286240.1286290,https://dl.acm.org/doi/10.1145/1286240.1286290,2,0,1,1,4,Yes,Yes
1026,Measuring the quality of web content using factual information,Backward Citation Tracking,2012,Elisabeth Lex; Michael Voelske; Marcelo Errecalde; Edgardo Ferretti; Leticia C. Cagnina; Christopher Horn; Benno Stein; Michael Granitzer,Conference,"WebQuality '12: Joint WICOW/AIRWeb Workshop on Web Quality, pp. 7-10",Association for Computing Machinery,"New York City, United States",10,60,"Nowadays, many decisions are based on information found in the Web. For the most part, the disseminating sources are not certified, and hence an assessment of the quality and credibility of Web content became more important than ever. With factual density we present a simple statistical quality measure that is based on facts extracted from Web content using Open Information Extraction. In a first case study, we use this measure to identify featured/good articles in Wikipedia. We compare the factual density measure with word count, a measure that has successfully been applied to this task in the past. Our evaluation corroborates the good performance of word count in Wikipedia since featured/good articles are often longer than non-featured. However, for articles of similar lengths the word count measure fails while factual density can separate between them with an F-measure of 90.4%. We also investigate the use of relational features for categorizing Wikipedia articles into featured/good versus non-featured ones. If articles have similar lengths, we achieve an F-measure of 86.7% and 84% otherwise.",N/A,https://dl.acm.org/doi/pdf/10.1145/2184305.2184308,https://dl.acm.org/doi/10.1145/2184305.2184308,2,1,1,0,4,Yes,Yes
1027,Detection of text quality flaws as a one-class classification problem,Backward Citation Tracking,2011,Maik Anderka; Benno Stein; Nedim Lipka,Conference,"CIKM '11: ACM International Conference on Information and Knowledge Management, pp. 2313-2316",Association for Computing Machinery,"New York City, United States",14,30,"For Web applications that are based on user generated content the detection of text quality flaws is a key concern. Our research contributes to automatic quality flaw detection. In particular, we propose to cast the detection of text quality flaws as a one-class classification problem: we are given only positive examples (= texts containing a particular quality flaw) and decide whether or not an unseen text suffers from this flaw. We argue that common binary or multiclass classification approaches are ineffective in here, and we underpin our approach by a real-world application: we employ a dedicated one-class learning approach to determine whether a given Wikipedia article suffers from certain quality flaws. Since in the Wikipedia setting the acquisition of sensible test data is quite intricate, we analyze the effects of a biased sample selection. In addition, we illustrate the classifier effectiveness as a function of the flaw distribution in order to cope with the unknown (real-world) flaw-specific class imbalances. Altogether, provided test data with little noise, four from ten important quality flaws in Wikipedia can be detected with a precision close to 1.",N/A,https://dl.acm.org/doi/pdf/10.1145/2063576.2063954,https://dl.acm.org/doi/10.1145/2063576.2063954,1,1,0,0,2,No,No
1030,Characterizing Wikipedia pages using edit network motif profiles,Backward Citation Tracking,2011,Guangyu Wu; Martin Harrigan; PÃ¡draig Cunningham,Conference,"SMUC '11: International Workshop on Search and Mining User-generated Contents, pp. 45-52",Association for Computing Machinery,"New York City, United States",17,53,"Good Wikipedia articles are authoritative sources due to the collaboration of a number of knowledgeable contributors. This is the many eyes idea. The edit network associated with a Wikipedia article can tell us something about its quality or authoritativeness. In this paper we explore the hypothesis that the characteristics of this edit network are predictive of the quality of the corresponding article's content. We characterize the edit network using a profile of network motifs and we show that this network motif profile is predictive of the Wikipedia quality classes assigned to articles by Wikipedia editors. We further show that the network motif profile can identify outlier articles particularly in the 'Featured Article' class, the highest Wikipedia quality class.",Wikipedia; Authoritativeness; Network motifs,https://dl.acm.org/doi/pdf/10.1145/2065023.2065036,https://dl.acm.org/doi/10.1145/2065023.2065036,1,1,1,0,3,No,No
1038,"Issues of cross-contextual information quality evaluationâThe case of Arabic, English, and Korean Wikipedias",Backward Citation Tracking,2009,Besiki Stvilia; Abdullah Al-Faraj; Yong Jeong Yi,Journal,"Library & Information Science Research, vol. 31(4), pp. 232-239",N/A,N/A,41,71,"An initial exploration into the issue of information quality evaluation across different cultural and community contexts based on data collected from the Arabic, English, and Korean Wikipedias showed that different Wikipedia communities may have different understandings of and models for quality. It also showed the feasibility of using some article edit-based metrics for automated quality measurement across different Wikipedia contexts. A model for measuring context similarity was developed and used to evaluate the relationship between similarities in sociocultural factors and the understanding of information quality by the three Wikipedia communities.",N/A,https://reader.elsevier.com/reader/sd/pii/S0740818809000954?token=75CFEBF42D926D5C7E7E8F58A2C33ED68EB12B4CA004D61F444B290C84F4399CEF13AA00775FB9520774622BD77CE26C&originRegion=eu-west-1&originCreation=20230311133412,https://www.sciencedirect.com/science/article/pii/S0740818809000954,1,0,1,1,3,No,No
1040,Identifying featured articles in Spanish Wikipedia,Backward Citation Tracking,2014,Lian Pohn; Edgardo Ferretti; Marcelo Errecalde,N/A,N/A,N/A,N/A,19,4,"Information Quality assessment in Wikipedia has become an ever-growing research line in the last years. However, few efforts have been accomplished in Spanish Wikipedia, despite being Spanish, one of the most spoken languages in the world by native speakers. In this respect, we present the first study to automatically assess information quality in Spanish Wikipedia, where Featured Articles identification is evaluated as a binary classification task. Two popular classification approaches like Naive Bayes and Support Vector Machine (SVM) are evaluated with different document  representations and vocabulary sizes. The obtained results show that FA identification can be performed with an F1 score of 0.81, when SVM is used as classification algorithm and documents are represented with a binary codification of the bag-of-words model with reduced vocabulary.",Wikipedia; Information Quality; Featured Article; Support Vector Machine,http://sedici.unlp.edu.ar/bitstream/handle/10915/42288/Documento_completo.pdf?sequence=1,http://sedici.unlp.edu.ar/bitstream/handle/10915/42288/Documento_completo.pdf?sequence=1,2,1,1,0,4,Yes,Yes
1041,"Modelling Wikipediaâs Information Quality using Informativeness, Reliability and Authority",Backward Citation Tracking,2021,Chinthani Sugandhika; Supunmali Ahangama; Sapumal Ahangama,Conference,"ICAC '21: International Conference on Advancements in Computing, pp. 169-174",Institute of Electrical and Electronic Engineers,"New York City, United States",44,1,"Wikipedia is the largest collaborative encyclopedia published on the internet. Due to its âopen source' model, Wikipedia faces many issues regarding its Information Quality (IQ). Due to this reason, Wikipedia is generally not recommended for academic and research activities. However, hybrid approach which utilizes both content and metadata statistics of Wikipedia articles provide good insights in measuring the underlying IQ. Therefore, aligning with this hybrid approach, this study presents a simple yet precise model to assess the IQ of Wikipedia. The model comprises three IQ dimensions (1) Informativeness, (2) Reliability and (3) Authority, and 23 IQ features. The proposed model was tested with 1000 articles extracted from five WikiProjects Medicine, Politics, Sports, History, and Science. A Selenium-based web scraping technique was used to extract the data from articles automatically. The model received a classification accuracy of 79% and a clustering accuracy of 84%. Thus, this extensive experiment validates the effectiveness of the proposed model. Accordingly, the methodology, analysis and results, implications of the findings to theoretical discourse and practical applications, limitations, and futuristic directions are discussed in this paper.",Information quality; Wikipedia; informativeness; reliability; authority; collaborative content; hybrid approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9671092,https://ieeexplore.ieee.org/document/9671092,2,2,2,0,6,Yes,Yes
1044,Measuring Qualities of Articles Contributed by Online Communities,Backward Citation Tracking,2006,Ee-Peng Lim; Ba-Quy Vuong; Hady Wirawan Lauw; Aixin Sun,Conference,"WI '16: IEEE WIC ACM International Conference on Web Intelligence, pp. 81-87",Institute of Electrical and Electronic Engineers,"New York City, United States",10,71,"Using open source Web editing software (e.g., wiki), online community users can now easily edit, review and publish articles collaboratively. While much useful knowledge can be derived from these articles, content users and critics are often concerned about their qualities. In this paper, we develop two models, namely basic model and peer review model, for measuring the qualities of these articles and the authorities of their contributors. We represent collaboratively edited articles and their contributors in a bipartite graph. While the basic model measures an article's quality using both the authorities of contributors and the amount of contribution from each contributor, the peer review model extends the former by considering the review aspect of article content. We present results of experiments conducted on some Wikipedia pages and their contributors. Our result show that the two models can effectively determine the articles' qualities and contributors' authorities using the collaborative nature of online communities",N/A,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4061345,https://ieeexplore.ieee.org/document/4061345,2,0,1,0,3,No,No
1046,Interlingual Aspects Of Wikipedia's Quality,Backward Citation Tracking,2010,Rainer HammwÃ¶hner,N/A,N/A,N/A,N/A,30,21,"This paper presents interim results of an ongoing project on quality issues concerning Wikipedia. One focus of research is the relation of language and quality measurement. The other one is the use of interlingual relations for quality assessment and improvement. The study is based on mono- and multilingual samples of featured and non-featured Wikipedia articles in English, French, German, and Italian that are evaluated automatically.",Information Quality; Wikipedia; Knowledge Organization; Indexing,https://epub.uni-regensburg.de/15572/1/iciq_2007.pdf,https://epub.uni-regensburg.de/15572/,2,0,1,1,4,Yes,Yes
1051,Web Article Quality Assessment in Multi-dimensional Space,Backward Citation Tracking,2011,Jingyu Han; Xiong Fu; Kejia Chen; Chuandong Wang,Conference,WAIM '11: International Conference on Web-Age Information Management,Springer,"Berlin, Heidelberg",11,4,"Nowadays user-generated content (UGC) such as Wikipedia, is emerging on the web at an explosive rate, but its data quality varies dramatically. How to effectively rate the articleâs quality is the focus of research and industry communities. Considering that each quality class demonstrates its specific characteristics on different quality dimensions, we propose to learn the web quality corpus by taking different quality dimensions into consideration. Each article is regarded as an aggregation of sections and each sectionâs quality is modelled using Dynamic Bayesian Network(DBN) with reference to accuracy, completeness and consistency. Each quality class is represented by three dimension corpora, namely accuracy corpus, completeness corpus and consistency corpus. Finally we propose two schemes to compute quality ranking. Experiments show our approach performs well.",N/A,https://link.springer.com/chapter/10.1007/978-3-642-23535-1_20,https://link.springer.com/chapter/10.1007/978-3-642-23535-1_20,2,0,1,0,3,No,No
1055,ORES: Lowering Barriers with Participatory Machine Learning in Wikipedia,Backward Citation Tracking,2020,Aaron L. Halfaker; R. Stuart Geiger,Journal,"Proceedings of the ACM on Human-Computer Interaction, vol. 4(CSCW2), no. 148, pp. 1-37",N/A,N/A,113,70,"Algorithmic systems---from rule-based bots to machine learning classifiers---have a long history of supporting the essential work of content moderation and other curation work in peer production projects. From counter-vandalism to task routing, basic machine prediction has allowed open knowledge projects like Wikipedia to scale to the largest encyclopedia in the world, while maintaining quality and consistency. However, conversations about how quality control should work and what role algorithms should play have generally been led by the expert engineers who have the skills and resources to develop and modify these complex algorithmic systems. In this paper, we describe ORES: an algorithmic scoring service that supports real-time scoring of wiki edits using multiple independent classifiers trained on different datasets. ORES decouples several activities that have typically all been performed by engineers: choosing or curating training data, building models to serve predictions, auditing predictions, and developing interfaces or automated agents that act on those predictions. This meta-algorithmic system was designed to open up socio-technical conversations about algorithms in Wikipedia to a broader set of participants. In this paper, we discuss the theoretical mechanisms of social change ORES enables and detail case studies in participatory machine learning around ORES from the 5 years since its deployment.",Wikipedia; Reflection; Machine learning; Transparency; Fairness; Algorithms; Governance,https://dl.acm.org/doi/pdf/10.1145/3415219,https://dl.acm.org/doi/10.1145/3415219,3,1,1,1,6,Yes,Yes
1061,A joint model for multimodal document quality assessment,Backward Citation Tracking,2019,Aili Shen; Bahar Salehi; Timothy Baldwin; Jianzhong Qi,Conference,"JCDL '19: Joint Conference on Digital Libraries, pp. 107-110",Association for Computing Machinery,"New York City, United States",24,18,"The quality of a document is affected by various factors, including grammaticality, readability, stylistics, and expertise depth, making the task of document quality assessment a complex one. In this paper, we explore this task in the context of assessing the quality of Wikipedia articles. Observing that the visual rendering of a document can capture implicit quality indicators that are not present in the document text --- such as images, font choices, and visual layout --- we propose a joint model that combines the text content with a visual rendering of the document for document quality assessment. Experimental results over a Wikipedia dataset reveal that textual and visual features are complementary, achieving state-of-the-art results. Further experiments on a Peer Review dataset verify the general applicability of our proposed model.",Document quality assessment; Multimodal; Visual embeddings; Textual embeddings; Wikipedia articles,https://dl.acm.org/doi/pdf/10.1109/JCDL.2019.00024,https://dl.acm.org/doi/10.1109/JCDL.2019.00024,2,1,0,0,3,No,No
2001,Who Did What: Editor Role Identification in Wikipedia,Forward Citation Tracking,2016,Diyi Yang; Aaron L. Halfaker; Robert Kraut; Eduard Hovy,Conference,"ICWSM '16: International AAAI Conference on Web and Social Media, vol. 10(1), pp. 446-455",Association for the Advancement of Artificial Intelligence,"Palo Alto, California",34,67,"Understanding the social roles played by contributors to online communities can facilitate the process of task routing. In this work, we develop new techniques to find roles in Wikipedia based on editors' low-level edit types and investigate how work contributed by people from different roles affect the article quality. To do this, we first built machine-learning models to automatically identify the edit categories associated with edits. We then applied a graphical model analogous to Latent Dirichlet Allocation to uncover the latent roles in editors' edit histories. Applying this technique revealed eight different roles editors play. Finally, we validated how our identified roles collaborate to improve the quality of articles. The results demonstrate that editors carrying on different roles contribute differently in terms of edit categories and articles in different quality stages need different types of editors. Implications for editor role identification and the validation of role contribution are discussed.",N/A,https://ojs.aaai.org/index.php/ICWSM/article/view/14732/14581,https://ojs.aaai.org/index.php/ICWSM/article/view/14732,1,0,1,0,2,No,No
2002,Quality Assessment of Peer-Produced Content in Knowledge Repositories using Development and Coordination Activities,Forward Citation Tracking,2019,Srikar Velichety; Sudha Ram; Jesse Bockstedt,Journal,"Journal of Management Information Systems, vol. 36(2), pp. 478-512",N/A,N/A,83,18,"We develop a method to assess the quality of peer-produced content in knowledge repositories using their development and coordination histories. We also develop a process to identify relevant features for quality assessment models and algorithms for processing datasets in large-scale knowledge repositories. Models using these features, on English language Wikipedia articles, outperform existing methods for quality assessment. We achieve an overall accuracy of 81 percent which is a 7 percent improvement over existing models. In addition, our features improve the precision and recall of each class up to 9 percent and 17 percent respectively. Finally, our models are robust to ten-fold cross validation and techniques used for classification. Overall, our research provides a comprehensive design science framework for both identifying and efficiently extracting features related to development and coordination activities and assessing quality using these features. We also provide details of potential implementation of a quality assessment system for knowledge repositories.",knowledge repositories; Wikipedia; peer-produced content; design science big data analytics; predictive analytics,https://www.tandfonline.com/doi/pdf/10.1080/07421222.2019.1598692,https://www.tandfonline.com/doi/full/10.1080/07421222.2019.1598692,2,2,1,0,5,Yes,Yes
2005,"Qualifying Articles of Persian Wikipedia Encyclopedia Through J48 Algorithm, ANFIS and Subtractive Clustering",Forward Citation Tracking,2016,Seyedtaha Seyedsadr; Mohammadali Afsharkazemi; Hashem Nikoomaram,Journal,"Automation, Control and Intelligent Systems, vol. 3(6), pp. 141-153",N/A,N/A,51,0,"Since Wikipedia encyclopedia is one of the most popular web sites on the internet, providing accurate information is of abundant importance. In this research, the effective variables on quality of Persian articles are identified and a system is, then, designed for judging articles in three quality levels: high quality, cleanup needed, and deletion. First, the variables relating to the articles included in the list of featured articles, good articles, cleanup needed, and deletion articles are collected. Then, two methods are used for the analysis of data: First, a decision tree explains the relationships among the collected variables as rules that are implemented by adaptive neuro fuzzy interference system. Second, the data are implemented by subtractive clustering algorithm and the error of both methods is, finally, measured and compared. The results indicate that the average daily hits, total views, page length, total number of edits, total number of authors, and number of templates used are directly related to quality of Persian articles while the number of recent number of authors is inversely related to quality of articles.",Wikipedia Encyclopedia; Quality of Articles; J48 Decision Tree; ANFIS; Subtractive Clustering Algorithm,https://www.sciencepublishinggroup.com/journal/paperinfo?journalid=134&doi=10.11648/j.acis.20150306.18,https://www.sciencepublishinggroup.com/journal/paperinfo?journalid=134&doi=10.11648/j.acis.20150306.18,3,1,1,1,6,Yes,Yes
2008,Developing the Quality Model for Collaborative Open Data,Forward Citation Tracking,2020,Mouzhi Ge; WÅodzimierz Lewoniewski,Journal,"Procedia Computer Science, vol. 176, pp. 1883-1892",N/A,N/A,35,5,"Nowadays, the development of data sharing technologies allows to involve more people to collaboratively contribute knowledge on the Web. The shared knowledge is usually represented as Collaborative Open Data (COD), for example, Wikipedia is one of the well-known sources for COD. The Wikipedia articles can be written in different languages, updated in real time, and originated from a vast variety of editors. However, COD also bring different data quality problems such as data inconsistency and low data objectiveness due to the crowd-based and dynamic nature. These data quality problems such as biased information may lead to sentimental changes or social impacts. This paper therefore proposes a new measurement model to assess the quality of COD. In order to evaluate the proposed model, A preliminary experiment is conducted with a large scale of Wikipedia articles to validate the applicability and efficiency of this proposed quality model in the real-world scenario.",Data Quality; Quality Assessment; Collaborative Open Data; Wikipedia; Quality Model,https://reader.elsevier.com/reader/sd/pii/S187705092032130X?token=F5690D611562DCE69A0A72CF1ECD6B803156C8F0B804D4E2D6837A5568BDE7B745597FFE9C8495AA74D8B679C3A4BEE2&originRegion=eu-west-1&originCreation=20230312205946,https://www.sciencedirect.com/science/article/pii/S187705092032130X,2,0,2,1,5,Yes,Yes
2009,A multimodal approach to assessing document quality,Forward Citation Tracking,2020,Aili Shen; Bahar Salehi; Jainzhong Qi; Timothy Baldwin,Journal,"Journal of Artificial Intelligence Research, vol. 68, pp. 607-632",N/A,N/A,73,2,"The perceived quality of a document is affected by various factors, including grammat- icality, readability, stylistics, and expertise depth, making the task of document quality assessment a complex one. In this paper, we explore this task in the context of assessing the quality of Wikipedia articles and academic papers. Observing that the visual rendering of a document can capture implicit quality indicators that are not present in the document text â such as images, font choices, and visual layout â we propose a joint model that combines the text content with a visual rendering of the document for document qual- ity assessment. Our joint model achieves state-of-the-art results over five datasets in two domains (Wikipedia and academic papers), which demonstrates the complementarity of textual and visual features, and the general applicability of our model. To examine what kinds of features our model has learned, we further train our model in a multi-task learning setting, where document quality assessment is the primary task and feature learning is an auxiliary task. Experimental results show that visual embeddings are better at learning structural features while textual embeddings are better at learning readability scores, which further verifies the complementarity of visual and textual features.",N/A,https://www.jair.org/index.php/jair/article/view/11647/26595,https://www.jair.org/index.php/jair/article/view/11647,3,1,2,0,6,Yes,Yes
2010,Improved Automatic Maturity Assessment of Wikipedia Medical Articles,Forward Citation Tracking,2014,Emanuel Marzini; Angelo Spognardi; Ilaria Matteucci; Paolo Mori; Marinella Petrocchi; Riccardo Conti ,Conference,"OTM '14: Confederated International Conferences ""On the Move to Meaningful Internet Systems"", pp. 612-662",Springer,"Berlin, Heidelberg",23,2,"The Internet is naturally a simple and immediate mean to retrieve information. However, not everything one can find is equally accurate and reliable. In this paper, we continue our line of research towards effective techniques for assessing the quality of online content. Focusing on the Wikipedia Medicinal Portal, in a previous work we implemented an automatic technique to assess the quality of each article and we compared our results to the classification of the articles given by the portal itself, obtaining quite different outcomes. Here, we present a lightweight instantiation of our methodology that reduces both redundant features and those not mentioned by the WikiProject guidelines. What we obtain is a fine-grained assessment and a better discrimination of the articlesâ quality, w.r.t. previous work. Our proposal could help to automatically evaluate the maturity of Wikipedia medical articles in an efficient way.",N/A,https://link.springer.com/chapter/10.1007/978-3-662-45563-0_37,https://link.springer.com/chapter/10.1007/978-3-662-45563-0_37,2,0,2,0,4,Yes,Yes
2011,Models for Arabic Document Quality Assessment,Forward Citation Tracking,2020,Adnan Yahya; Afnan Ahmad; Alaa Assaf; Rawan Khater; Ali Salhi,Conference,"BIS '20: International Conference on Business Information Systems, pp. 297-310",Springer,"Cham, Switzerland",16,0,"Digital content has been increasing rapidly. This content can be generated, accessed and used by anyone and thus the need for quality assessment of web content before usage becomes an important issue. Devising methods to assess the quality of Arabic digital content is the focus of this paper. Our work was partially based on Wikipedia articles annotated into featured and good according to quality guidelines of Wikipedia. Our analysis was directed at finding features that can serve as best quality indicators. Using the defined features, we trained a high accuracy quality assessment model using machine-learning algorithms. Our work went beyond the Wikipedia documents to build a general model that can assess the quality of Arabic documents that lack Wikipedia metadata with acceptable accuracy. The model was trained and built using features from documents we collected from Arabic online news sites and blogs, and annotated in collaboration with university students.",Document quality assessment; Arabic Wikipedia; Arabic information retrieval,https://link.springer.com/chapter/10.1007/978-3-030-61146-0_24,https://link.springer.com/chapter/10.1007/978-3-030-61146-0_24,2,1,2,1,6,Yes,Yes
2013,Automating assessment of collaborative writing quality in multiple stages: the case of wiki,Forward Citation Tracking,2016,Xiao Hu; Tzi-Dong Jeremy Ng; Lu Tian; Chi-Un Lei,Conference,"LAK '16: International Conference on Learning Analytics & Knowledge, pp. 518-519",Association for Computing Machinery,"New York City, United States",7,6,"This study attempts to investigate to what extent indicators of academic writing and cognitive thinking can help measure the writing quality of group collaborative writings on Wikis. Particularly, comparisons were made on Wiki content in different stages of the projects. Preliminary results from a multiple linear regression analysis reveal that linguistic indicators such as engagement markers and self-mention were significant predictors in earlier stages to the projects, whereas verbs indicating cognitive thinking in the evaluation level were significant in later project stages.",Wiki; automated assessment; metadiscourse,https://dl.acm.org/doi/pdf/10.1145/2883851.2883963,https://dl.acm.org/doi/abs/10.1145/2883851.2883963,1,0,1,0,2,No,No
2014,On MultiView-Based Meta-learning for Automatic Quality Assessment of Wiki Articles,Forward Citation Tracking,2012,Daniel Hasan Dalip; Marcos AndrÃ© GonÃ§alves; Marco Cristo; PÃ¡vel Calado,Conference,"TPDL '12: International Conference on Theory and Practice of Digital Libraries, pp. 234-246",Springer,"Berlin, Heidelberg",19,9,"The Internet has seen a surge of new types of repositories with free access and collaborative open edition. However, this large amount of information, made available democratically and virtually without any control, raises questions about its quality. In this work, we investigate the use of meta-learning techniques to combine sets of semantically related quality indicators (aka, views) in order to automatically assess the quality of wiki articles. The idea is inspired on the combination of multiple (quality) experts. We perform a thorough analysis of the proposed multiview-based meta-learning approach in 3 collections. In our experiments, meta-learning was able to improve the performance of a state-of-the-art method in all tested datasets, with gains of up to 27% in quality assessment.",N/A,https://link.springer.com/chapter/10.1007/978-3-642-33290-6_26,https://link.springer.com/chapter/10.1007/978-3-642-33290-6_26,2,1,1,0,4,Yes,Yes
2015,Probabilistic Quality Assessment Based on Articleâs Revision History,Forward Citation Tracking,2011,Jingyu Han; Chuandong Wang; Dawei Jiang ,Conference,"DEXA '11: International Conference on Database and Expert Systems Applications, pp. 574-588",Springer,"Berlin, Heidelberg",16,11,"The collaborative efforts of users in social media services such as Wikipedia have led to an explosion in user-generated content and how to automatically tag the quality of the content is an eminent concern now. Actually each article is usually undergoing a series of revision phases and the articles of different quality classes exhibit specific revision cycle patterns. We propose to Assess Quality based on Revision History (AQRH) for a specific domain as follows. First, we borrow Hidden Markov Model (HMM) to turn each articleâs revision history into a revision state sequence. Then, for each quality class its revision cycle patterns are extracted and are clustered into quality corpora. Finally, articleâs quality is thereby gauged by comparing the articleâs state sequence with the patterns of pre-classified documents in probabilistic sense. We conduct experiments on a set of Wikipedia articles and the results demonstrate that our method can accurately and objectively capture web articleâs quality.",N/A,https://link.springer.com/chapter/10.1007/978-3-642-23091-2_50,https://link.springer.com/chapter/10.1007/978-3-642-23091-2_50,2,0,1,0,3,No,No
2016,Probabilistic Quality Assessment of Articles Based on Learning Editing Patterns,Forward Citation Tracking,2011,Jingyu Han; Chuandong Wang; Xiong Fu; Kejia Chen,Conference,"CSSS '11: International Conference on Computer Science and Service System, pp. 564-570",Institute of Electrical and Electronic Engineers,"New York City, United States",15,1,"As a new model of distributed, collaborative information source, such as Wikipedia, is emerging, its content is constantly being generated, updated and maintained by various users and its data quality varies from time to time. Thus the quality assessment of the content is a pressing concern now. We observe that each article usually goes through a series of editing phases such as building structure, contributing text, discussing text, etc., gradually getting into the final quality state and that the articles of different quality classes exhibit specific edit cycle patterns. We propose a new approach to Assess Quality based on article's Editing History (AQEH) for a specific domain as follows. First, each article's editing history is transformed into a state sequence borrowing Hidden Markov Model(HMM). Second, edit cycle patterns are first extracted for each quality class and then each quality class is further refined into quality corpora by clustering. Now, each quality class is clearly represented by a series of quality corpora and each quality corpus is described by a group of frequently co-occurring edit cycle patterns. Finally, article quality can be determined in probabilistic sense by comparing the article with the quality corpora. Experimental results demonstrate that our method can capture and predict web article's quality accurately and objectively.",data quality; HMM; quality assessment; web article; Wikipedia,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5973947,https://ieeexplore.ieee.org/abstract/document/5973947,2,0,1,0,3,No,No
2017,Quality assessment of web-based information on type 2 diabetes,Forward Citation Tracking,2022,Didem ÃlÃ§er; TuÄba TaÅkaya Temizel,Journal,"Online Information Review, vol. 46(4), pp. 715-732",N/A,N/A,51,1,"Purpose - This paper proposes a framework that automatically assesses content coverage and information quality of health websites for end-users. Design/methodology/approach - The study investigates the impact of textual and content-based features in predicting the quality of health-related texts. Content-based features were acquired using an evidence-based practice guideline in diabetes. A set of textual features inspired by professional health literacy guidelines and the features commonly used for assessing information quality in other domains were also used. In this study, 60 websites about type 2 diabetes were methodically selected for inclusion. Two general practitioners used DISCERN to assess each website in terms of its content coverage and quality. Findings - The proposed framework outputs were compared with the experts' evaluation scores. The best accuracy was obtained as 88 and 92% with textual features and content-based features for coverage assessment respectively. When both types of features were used, the proposed framework achieved 90% accuracy. For information quality assessment, the content-based features resulted in a higher accuracy of 92% against 88% obtained using the textual features. Research limitations/implications - The experiments were conducted for websites about type 2 diabetes. As the whole process is costly and requires extensive expert human labelling, the study was carried out in a single domain. However, the methodology is generalizable to other health domains for which evidence-based practice guidelines are available. Practical implications - Finding high-quality online health information is becoming increasingly difficult due to the high volume of information generated by non-experts in the area. The search engines fail to rank objective health websites higher within the search results. The proposed framework can aid search engine and information platform developers to implement better retrieval techniques, in turn, facilitating end-users' access to high-quality health information. Social implications - Erroneous, biased or partial health information is a serious problem for end-users who need access to objective information on their health problems. Such information may cause patients to stop their treatments provided by professionals. It might also have adverse financial implications by causing unnecessary expenditures on ineffective treatments. The ability to access high-quality health information has a positive effect on the health of both individuals and the whole society. Originality/value - The paper demonstrates that automatic assessment of health websites is a domain-specific problem, which cannot be addressed with the general information quality assessment methodologies in the literature. Content coverage of health websites has also been studied in the health domain for the first time in the literature.",DISCERN; Website quality; Diabetes; Information quality; Information coverage,https://www.emerald.com/insight/content/doi/10.1108/OIR-02-2021-0089/full/pdf?title=quality-assessment-of-web-based-information-on-type-2-diabetes,https://www.emerald.com/insight/content/doi/10.1108/OIR-02-2021-0089/full/html,1,0,2,0,3,No,No
2019,Is cross-linguistic advert flaw detection in Wikipedia feasible? A multilingual-BERT-based transfer learning approach,Forward Citation Tracking,2022,Muyan Li; Heshen Zhou; Jingrui Hou; Ping Wang; Erpei Gao,Journal,"Knowledge-Based Systems, vol. 252(109330), pp. ?-?",N/A,N/A,52,1,"Wikipedia is one of the most prominent online platforms from which people acquire knowledge; thus, its article quality should be of great concern. Currently, many scholars focus on the quality assessment and quality flaws detection in Wikipedia articles. However, most of them considered only one language version, typically English. One major obstacle to conducting such research in non-English or multilanguage scenarios is insufficient labeled data. To address this, we introduce transfer learning based on a pretraining multilanguage model to verify whether it is feasible to conduct cross-language flaw detection. Specifically, we chose the Advert flaw (containing content written like an advertisement) as our research objective; French, Spanish, and Chinese as the target language scenarios; and English articles as the source scenario. Multilingual BERT combined with a sequential model was used to extract semantic features and build classifiers. Moreover, we compared three strategies (direct transfer, fine-tuning transfer and nontransfer) to determine the best strategy for cross-language Advert flaw detection at different training sample scales. The experimental results demonstrated that the proposed model trained with the English dataset can identify the Advert flaw in other languages; fine-tuning transfer yields the best performance as the corpus increases.",Wikipedia quality flaw; Cross-lingual transfer learning; Pretraining language model; Text classification,https://reader.elsevier.com/reader/sd/pii/S0950705122006670?token=7FF38D9CB53BA2E0A063F0902B91AD1742F2306DDF6B9F7E637B74E911AA746B4586E80AF63C167E9B175056748F7C07&originRegion=eu-west-1&originCreation=20230312231815,https://www.sciencedirect.com/science/article/pii/S0950705122006670,1,1,0,1,3,No,No
2022,Good Authors = Good Articles? - How Wikis Work,Forward Citation Tracking,2015,Thomas WÃ¶hner; Sebastian KÃ¶hler; Ralf Peters,Conference,"WI '15: International Conference on Wirtschaftsinformatik, 59",Association for Information Systems,"Atlanta, Georgia",30,2,"Wikis are websites to develop content collaboratively. The question arises to what extent the reputation of participants influences the quality of wiki sites. We analyze the impact of author reputation using the example of Wikipedia. We extend previous research by considering a set of different reputation metrics and a new model for aggregating reputation values. Since anonymous authors tend to have a lower reputation, we also quantify the level of participation of anonymous authors as an indicator for the reputation of the crowd. Our analysis finds out that reputation matters, but strongly depends on the used reputation metric and therefore on the corresponding author characteristics. The study shows that the experience of authors in the development of high-quality articles is highly relevant whereas the number of edits and the quality of contributions are of lower importance. Finally, our investigation proves the open editing model and the self-healing mechanism of wikis.",Wiki; Quality; Reputation; Wisdom of the Crowd,https://aisel.aisnet.org/wi2015/59/,https://aisel.aisnet.org/wi2015/59/?utm_source=aisel.aisnet.org%2Fwi2015%2F59&utm_medium=PDF&utm_campaign=PDFCoverPages,1,0,1,1,3,No,No
2024,Finding high quality documents through link and click graphs,Forward Citation Tracking,2018,Linfeng Yu; Mizuho Iwaihara,Conference,"IIAI-AAI '18: International Congress on Advanced Applied Informatics, pp. 49-54",Institute of Electrical and Electronic Engineers,"New York City, United States",15,2,"Link graphs of web pages have been utilized to evaluate importance of each page. Existing link analysis algorithms, including HITS and PageRank, exploit static link connectivity between pages. On the other hand, service providers often record HTTP requests that contain the resource and referrer of each request, from which we can construct a click graph that has edge weights representing the times of clicks on each link, or link traffic. Click graphs reflect users' choices of interesting links, thus the graphs are useful for evaluating importance of pages. However, clicks are often skewed onto highly popular links, so that click graphs only could not properly evaluate less clicked pages. In this paper, we propose an algorithm called click count-weighted HITS algorithm, which integrates HITS algorithm with click graphs, for finding high quality documents. Our evaluations on finding featured articles of English Wikipedia show that our click count-weighted HITS algorithm shows better performance on a large Wikipedia corpus than algorithms that utilize link graphs or click graphs only.",link analysis; document ranking; click graph; HITS algorithm; quality; Wikipedia ,https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=8693372&ref=,https://ieeexplore.ieee.org/abstract/document/8693372,1,0,1,0,2,No,No
2028,Quality assessment of collaboratively-created web content with no manual intervention based on soft multi-view generation,Forward Citation Tracking,2019,Luiz Felipe GonÃ§alves MagalhÃ£es; Marcos AndrÃ© GonÃ§alves; SÃ©rgio Daniel Canuto; Daniel Hasan Dalip; Marco Cristo; PÃ¡vel Calado,Journal,"Expert Systems with Applications, vol. 132, pp. 226-238",N/A,N/A,39,1,"Automated quality assessment of collaboratively created Web content is important to guarantee scalability and lack of bias. The state-of-the-art solution for this problem relies on multi-view learning, where quality is considered a multifaceted concept that can be learned from human assessments. To this effect, features describing quality have been devised and grouped into views based on criteria such as text structure, readability, style, user edit history, etc. The tasks of determining the views and properly combining them require the assistance of an expert, which is hard to do in scenarios where they are overlapping or hard to interpret by humans. In this work we propose an automatic view generator, specially designed for the problem of automated content quality assessment with no manual intervention. Automatic view generation is achieved by finding clusters of highly correlated features. This process is performed iteratively, by automatically creating new clusters, evaluating them, and keeping those that perform the best. Experiments on three popular Wiki datasets show that our automated views are able to reduce the classification error of the original features by up to 20%. This happens by automatically generating views that are very similar to those manually built, while keeping only a small set of features to reduce noise and overfitting.",Multi-view; Machine learning; Information retrieval; Automatic text quality assessment,https://reader.elsevier.com/reader/sd/pii/S0957417419302830?token=2ACE3B215F31DCDCEB9EDD11457C8631344364F25A149D43F62D8220BCCF2C2DB0672AB7B7FA22D12E3A06518E84A90C&originRegion=eu-west-1&originCreation=20230315003537,https://www.sciencedirect.com/science/article/pii/S0957417419302830,2,1,3,0,6,Yes,Yes
2029,Discourse Connective - A Marker for Identifying Featured Articles in Biological Wikipedia,Forward Citation Tracking,2016,Sindhuja Gopalan; Paolo Rosso; Sobha Lalitha Devi,Journal,"Research in Computing Science, vol. 117(1), pp. 109-119",N/A,N/A,17,0,"Wikipedia is a free-content Internet encyclopedia that can be edited by anyone who accesses it. As a result, Wikipedia contains both featured and non-featured articles. Featured articles are high-quality articles and nonfeatured articles are poor quality articles. Since there is an exponential growth of Wikipedia articles, the need to identify the featured Wikipedia articles has become indispensable so as to provide quality information to the users. As very few attempts have been carried out in the biology domain of English Wikipedia articles, we present our study to automatically measure the information quality in biological Wikipedia articles. Since the coherence shows representational information quality of a text, we have used the discourse connective count measure for our study. We compare this novel measure with two other popular approaches word count measure and explicit document model method that have been successfully applied to the task of quality measurement in Wikipedia articles. We organized the Wikipedia articles into balanced and unbalanced set. The balanced set contains featured and non-featured articles of equal length and the unbalanced set contains randomly selected featured and non-featured articles. The best result for the balanced set is obtained with F-measure of 83.2%, while using Support Vector Machine classifier with 4-gram representation and Term Frequency-Inverse Document Frequency weighting scheme. Meanwhile, the best result for unbalanced corpus is obtained using the discourse connective count measure with an F - measure of 98.06%",Wikipedia articles quality; Document classification; Featured article; Non-featured article; Word count measure; Discourse connective count measure. ,https://www.rcs.cic.ipn.mx/rcs/2016_117/Discourse%20Connective%20-%20A%20Marker%20for%20Identifying%20Featured%20Articles%20in%20Biological%20Wikipedia.pdf,https://www.researchgate.net/journal/Research-in-Computing-Science-1870-4069,1,1,1,0,3,No,No
2030,Detection of Article Qualities in the Chinese Wikipedia Based on C4.5 Decision Tree,Forward Citation Tracking,2013,Kui Xiao; Bing Li; Peng He; Xi-hui Yang ,Conference,"KSEM '13: International Conference on Knowledge Science, Engineering and Management, pp. 444-452",Springer,"Berlin, Heidelberg",11,1,"The number of articles in Wikipedia is growing rapidly. It is important for Wikipedia to provide users with high quality and reliable articles. However, the quality assessment metric provided by Wikipedia are inefficient, and other mainstream quality detection methods only focus on the qualities of the English Wikipedia articles, and usually analyze the text contents of articles, which is also a time-consuming process. In this paper, we propose a method for detecting the article qualities of the Chinese Wikipedia based on C4.5 decision tree. The problem of quality detection is transformed to classification problem of high-quality and low-quality articles. By using the fields from the tables in the Chinese Wikipedia database, we built the decision trees to distinguish high-quality articles from low-quality ones.",Wikipedia; Article quality; Data ming; Decision tree; Application of supervised learning,https://link.springer.com/chapter/10.1007/978-3-642-39787-5_36,https://link.springer.com/chapter/10.1007/978-3-642-39787-5_36,2,1,1,1,5,Yes,Yes