id,databases,title,abstract,authors,year,publication_type,published_in,num_references,num_citations,url,doi,pdf
1,"ACM, Google Scholar",Measuring article quality in wikipedia: models and evaluation,"Wikipedia has grown to be the world largest and busiest free encyclopedia, in which articles are collaboratively written and maintained by volunteers online. Despite its success as a means of knowledge sharing and collaboration, the public has never stopped criticizing the quality of Wikipedia articles edited by non-experts and inexperienced contributors. In this paper, we investigate the problem of assessing the quality of articles in collaborative authoring of Wikipedia. We propose three article quality measurement models that make use of the interaction data between articles and their contributors derived from the article edit history. Our B<scp>asic</scp> model is designed based on the mutual dependency between article quality and their author authority. The P<scp>eer</scp>R<scp>eview</scp> model introduces the review behavior into measuring article quality. Finally, our P<scp>rob</scp>R<scp>eview</scp> models extend P<scp>eer</scp>R<scp>eview</scp> with partial reviewership of contributors as they edit various portions of the articles. We conduct experiments on a set of well-labeled Wikipedia articles to evaluate the effectiveness of our quality measurement models in resembling human judgement.",Meiqun Hu; Ee-Peng Lim; Aixin Sun; Hady W. Lauw; Ba-Quy Vuong,2007,Conference,"International Conference on Information and Knowledge Management, pp. 243-252",31,282,https://www.semanticscholar.org/paper/ee70063f74e209ec5dc3edfa13fbb82d60d1a3eb,10.1145/1321440.1321476,
2,"ACM, Google Scholar",Cooperation and quality in wikipedia,"The rise of the Internet has enabled collaboration and cooperation on anunprecedentedly large scale. The online encyclopedia Wikipedia, which presently comprises 7.2 million articles created by 7.04 million distinct editors, provides a consummate example. We examined all 50 million edits made tothe 1.5 million English-language Wikipedia articles and found that the high-quality articles are distinguished by a marked increase in number of edits, number of editors, and intensity of cooperative behavior, as compared to other articles of similar visibility and age. This is significant because in other domains, fruitful cooperation has proven to be difficult to sustain as the size of the collaboration increases. Furthermore, in spite of the vagaries of human behavior, we show that Wikipedia articles accrete edits according to a simple stochastic mechanism in which edits beget edits. Topics of high interest or relevance are thus naturally brought to the forefront of quality.",D. Wilkinson; B. Huberman,2007,Conference,N/A,41,288,https://www.semanticscholar.org/paper/9c61b818ecc29a132592d886fd0d5cb0047b5b6d,10.1145/1296951.1296968,
3,"ACM, Google Scholar",Who does what: Collaboration patterns in the wikipedia and their impact on article quality,"The quality of Wikipedia articles is debatable. On the one hand, existing research indicates that not only are people willing to contribute articles but the quality of these articles is close to that found in conventional encyclopedias. On the other hand, the public has never stopped criticizing the quality of Wikipedia articles, and critics never have trouble finding low-quality Wikipedia articles. Why do Wikipedia articles vary widely in quality? We investigate the relationship between collaboration and Wikipedia article quality. We show that the quality of Wikipedia articles is not only dependent on the different types of contributors but also on how they collaborate. Based on an empirical study, we classify contributors based on their roles in editing individual Wikipedia articles. We identify various patterns of collaboration based on the provenance or, more specifically, who does what to Wikipedia articles. Our research helps identify collaboration patterns that are preferable or detrimental for article quality, thus providing insights for designing tools and mechanisms to improve the quality of Wikipedia articles.",Jun Liu; S. Ram,2011,Journal,"ACM Trans. Manag. Inf. Syst., 2, pp. 11:1-11:23",53,181,https://www.semanticscholar.org/paper/620ba4d148d6e8d0f3103bf6ec47209534a33710,10.1145/1985347.1985352,
4,"ACM, Google Scholar",Size matters: word count as a measure of quality on wikipedia,"Wikipedia, ""the free encyclopedia"", now contains over two million English articles, and is widely regarded as a high-quality, authoritative encyclopedia. Some Wikipedia articles, however, are of questionable quality, and it is not always apparent to the visitor which articles are good and which are bad. We propose a simple metric -- word count -- for measuring article quality. In spite of its striking simplicity, we show that this metric significantly outperforms the more complex methods described in related work.",J. Blumenstock,2008,Conference,"The Web Conference, pp. 1095-1096",6,315,https://www.semanticscholar.org/paper/c6f61f344c919493886bf67d0e64e0242ae83547,10.1145/1367497.1367673,
5,"ACM, Google Scholar, Web of Science",Harnessing the wisdom of crowds in wikipedia: quality through coordination,"Wikipedia's success is often attributed to the large numbers of contributors who improve the accuracy, completeness and clarity of articles while reducing bias. However, because of the coordination needed to write an article collaboratively, adding contributors is costly. We examined how the number of editors in Wikipedia and the coordination methods they use affect article quality. We distinguish between explicit coordination, in which editors plan the article through communication, and implicit coordination, in which a subset of editors structure the work by doing the majority of it. Adding more editors to an article improved article quality only when they used appropriate coordination techniques and was harmful when they did not. Implicit coordination through concentrating the work was more helpful when many editors contributed, but explicit coordination through communication was not. Both types of coordination improved quality more when an article was in a formative stage. These results demonstrate the critical importance of coordination in effectively harnessing the ""wisdom of the crowd"" in online production environments.",A. Kittur; R. Kraut,2008,Conference,Proceedings of the 2008 ACM conference on Computer supported cooperative work,41,749,https://www.semanticscholar.org/paper/5ead79bbdb5d396f3a5900ff1b71ee44e8061dca,10.1145/1460563.1460572,
6,"ACM, Google Scholar, Web of Science",Determinants of wikipedia quality: the roles of global and local contribution inequality,"The success of Wikipedia and the relative high quality of its articles seem to contradict conventional wisdom. Recent studies have begun shedding light on the processes contributing to Wikipedia's success, highlighting the role of coordination and contribution inequality. In this study, we expand on these works in two ways. First, we make a distinction between global (Wikipedia-wide) and local (article-specific) inequality and investigate both constructs. Second, we explore both direct and indirect effects of these inequalities, exposing the intricate relationships between global inequality, local inequality, coordination, and article quality. We tested our hypotheses on a sample of a Wikipedia articles using structural equation modeling and found that global inequality exerts significant positive impact on article quality, while the effect of local inequality is indirect and is mediated by coordination",Ofer Arazy; O. Nov,2010,Conference,"Conference on Computer Supported Cooperative Work, pp. 233-236",22,91,https://www.semanticscholar.org/paper/36dce0334d47e45cd37f9484b464ee3857cbdc37,10.1145/1718918.1718963,
7,"ACM, Google Scholar, Web of Science",Information quality work organization in wikipedia,"The classic problem within the information quality (IQ) research and practice community has been the problem of defining IQ. It has been found repeatedly that IQ is context sensitive and cannot be described, measured, and assured with a single model. There is a need for empirical case studies of IQ work in different systems to develop a systematic knowledge that can then inform and guide the construction of context-specific IQ models. This article analyzes the organization of IQ assurance work in a large-scale, open, collaborative encyclopedia—Wikipedia. What is special about Wikipedia as a resource is that the quality discussions and processes are strongly connected to the data itself and are accessible to the general public. This openness makes it particularly easy for researchers to study a particular kind of collaborative work that is highly distributed and that has a particularly substantial focus, not just on error detection but also on error correction. We believe that the study of those evolving debates and processes and of the IQ assurance model as a whole has useful implications for the improvement of quality in other more conventional databases. © 2008 Wiley Periodicals, Inc.",Besiki Stvilia; M. Twidale; Linda C. Smith; L. Gasser,2008,Journal,"J. Assoc. Inf. Sci. Technol., 59, pp. 983-1001",63,373,https://www.semanticscholar.org/paper/89c5bef3053bf3e4953204c7357ff838523b7488,10.1002/asi.20813,
8,"ACM, Google Scholar",On measuring the quality of Wikipedia articles,"This paper discusses an approach to modeling and measuring information quality of Wikipedia articles. The approach is based on the idea that the quality of Wikipedia articles with distinctly different profiles needs to be measured using different information quality models. We report on our initial study, which involved two categories of Wikipedia articles: ""stabilized"" (those, whose content has not undergone major changes for a significant period of time) and ""controversial"" (the articles, which have undergone vandalism, revert wars, or whose content is subject to internal discussions between Wikipedia editors). We present simple information quality models and compare their performance on a subset of Wikipedia articles with the information quality evaluations provided by human users. Our experiment shows, that using special-purpose models for information quality captures user sentiment about Wikipedia articles better than using a single model for both categories of articles.",Gabriel De la Calzada; Alex Dekhtyar,2010,Conference,"Workshop on Information Credibility on the Web, pp. 11-18",51,65,https://www.semanticscholar.org/paper/68c1ca9cf4ef3bb12e17ea38c6155b3374ccff88,10.1145/1772938.1772943,http://users.csc.calpoly.edu/%7Edekhtyar/publications/WICOW2010.pdf
9,"ACM, Google Scholar",Don't bite the newbies: how reverts affect the quantity and quality of Wikipedia work,"Reverts are important to maintaining the quality of Wikipedia. They fix mistakes, repair vandalism, and help enforce policy. However, reverts can also be damaging, especially to the aspiring editor whose work they destroy. In this research we analyze 400,000 Wikipedia revisions to understand the effect that reverts had on editors. We seek to understand the extent to which they demotivate users, reducing the workforce of contributors, versus the extent to which they help users improve as encyclopedia editors. Overall we find that reverts are powerfully demotivating, but that their net influence is that more quality work is done in Wikipedia as a result of reverts than is lost by chasing editors away. However, we identify key conditions -- most specifically new editors being reverted by much more experienced editors - under which reverts are particularly damaging. We propose that reducing the damage from reverts might be one effective path for Wikipedia to solve the newcomer retention problem.",Aaron L Halfaker; A. Kittur; J. Riedl,2011,Conference,N/A,28,205,https://www.semanticscholar.org/paper/b2149024c793fb323b4fbe940c42ecf566860105,10.1145/2038558.2038585,
10,"ACM, Google Scholar",Assessing the quality of Wikipedia articles with lifecycle based metrics,"The main feature of the free online-encyclopedia Wikipedia is the wiki-tool, which allows viewers to edit the articles directly in the web browser. As a weakness of this openness for example the possibility of manipulation and vandalism cannot be ruled out, so that the quality of any given Wikipedia article is not guaranteed. Hence the automatic quality assessment has been becoming a high active research field. In this paper we offer new metrics for an efficient quality measurement. The metrics are based on the lifecycles of low and high quality articles, which refer to the changes of the persistent and transient contributions throughout the entire life span.",T. Wöhner; Ralf Peters,2009,Journal,N/A,27,126,https://www.semanticscholar.org/paper/777a29dff88b22c0cf2e38683cbaf183447a985b,10.1145/1641309.1641333,
11,"ACM, Google Scholar",Statistical measure of quality in Wikipedia,"Wikipedia is commonly viewed as the main online encyclopedia. Its content quality, however, has often been questioned due to the open nature of its editing model. A high--quality contribution by an expert may be followed by a low-quality contribution made by an amateur or a vandal; therefore the quality of each article may fluctuate over time as it goes through iterations of edits by different users. With the increasing use of Wikipedia, the need for a reliable assessment of the quality of the content is also rising. In this study, we model the evolution of content quality in Wikipedia articles in order to estimate the fraction of time during which articles retain high-quality status. To evaluate the model, we assess the quality of Wikipedia's featured and non-featured articles. We show how the model reproduces consistent results with what is expected. As a case study, we use the model in a CalSWIM mashup the content of which is taken from both highly reliable sources and Wikipedia, which may be less so. Integrating CalSWIM with a trust management system enables it to use not only recency but also quality as its criteria, and thus filter out vandalized or poor-quality content.",S. Javanmardi; C. Lopes,2010,Conference,N/A,26,51,https://www.semanticscholar.org/paper/54333cd5b3f6d0148f0af632455faafb8dce7567,10.1145/1964858.1964876,
12,Google Scholar,Information quality discussions in wikipedia,"We examine the Information Quality aspects of Wikipedia. By a study of the discussion pages and other process-oriented pages within the Wikipedia project, it is possible to determine the information quality dimensions that participants in the editing process care about, how they talk about them, what tradeoffs they make between these dimensions and how the quality assessment and improvement process operates. This analysis helps in understanding how high quality is maintained in a project where anyone may participate with no prior vetting. It also carries implications for improving the quality of more conventional datasets.",Besiki Stvilia; M. Twidale; L. Gasser; Linda C. Smith,2005,Journal,N/A,44,91,https://www.semanticscholar.org/paper/ebe949e01d044a25daa0e6cabb3091cc319d361f,,
13,"ACM, Google Scholar",Tell me more: an actionable quality model for Wikipedia,"In this paper we address the problem of developing actionable quality models for Wikipedia, models whose features directly suggest strategies for improving the quality of a given article. We first survey the literature in order to understand the notion of article quality in the context of Wikipedia and existing approaches to automatically assess article quality. We then develop classification models with varying combinations of more or less actionable features, and find that a model that only contains clearly actionable features delivers solid performance. Lastly we discuss the implications of these results in terms of how they can help improve the quality of articles across Wikipedia.",Morten Warncke-Wang; D. Cosley; J. Riedl,2013,Conference,Proceedings of the 9th International Symposium on Open Collaboration,41,110,https://www.semanticscholar.org/paper/00f5c7311b9ecc2d91cb3ab7cac6bb6acec28580,10.1145/2491055.2491063,
14,"ACM, Google Scholar, Web of Science",Automatic quality assessment of content created collaboratively by web communities: a case study of wikipedia,"The old dream of a universal repository containing all the human knowledge and culture is becoming possible through the Internet and the Web. Moreover, this is happening with the direct collaborative, participation of people. Wikipedia is a great example. It is an enormous repository of information with free access and edition, created by the community in a collaborative manner. However, this large amount of information, made available democratically and virtually without any control, raises questions about its relative quality. In this work we explore a significant number of quality indicators, some of them proposed by us and used here for the first time, and study their capability to assess the quality of Wikipedia articles. Furthermore, we explore machine learning techniques to combine these quality indicators into one single assessment judgment. Through experiments, we show that the most important quality indicators are the easiest ones to extract, namely, textual features related to length, structure and style. We were also able to determine which indicators did not contribute significantly to the quality assessment. These were, coincidentally, the most complex features, such as those based on link analysis. Finally, we compare our combination method with state-of-the-art solution and show significant improvements in terms of effective quality prediction.",D. H. Dalip; Marcos André Gonçalves; Marco Cristo; P. Calado,2009,Conference,"ACM/IEEE Joint Conference on Digital Libraries, pp. 295-304",31,125,https://www.semanticscholar.org/paper/412e1cf936f9a0aa4b289f14136174232e8f4e38,10.1145/1555400.1555449,
15,Google Scholar,Learning to Predict the Quality of Contributions to Wikipedia,"Although some have argued that Wikipedia’s open edit policy is one of the primary reasons for its success, it also raises concerns about quality — vandalism, bias, and errors can be problems. Despite these challenges, Wikipedia articles are often (perhaps surprisingly) of high quality, which many attribute to both the dedicatedWikipedia community and “good Samaritan” users. As Wikipedia continues to grow, however, it becomes more difficult for these users to keep up with the increasing number of articles and edits. This motivates the development of tools to assist users in creating and maintaining quality. In this paper, we propose metrics that quantify the quality of contributions to Wikipedia through implicit feedback from the community. We then learn discriminative probabilistic models that predict the quality of a new edit using features of the changes made, the author of the edit, and the article being edited. Through estimating parameters for these models, we also gain an understanding of factors that influence quality. We advocate using edit quality predictions and information gleaned from model analysis not to place restrictions on editing, but to instead alert users to potential quality problems, and to facilitate the development of additional incentives for contributors. We evaluate the edit quality prediction models on the Spanish Wikipedia. Experiments demonstrate that the models perform better when given access to content-based features of the edit, rather than only features of contributing user. This suggests that a user-based solution to the Wikipedia quality problem may not be sufficient. Introduction and Motivation Collaborative content generation systems such as Wikipedia are promising because they facilitate the integration of information from many disparate sources. Wikipedia is remarkable because anyone can edit an article. Some argue that this open edit policy is one of the key reasons for its success (Roth 2007; Riehle 2006). However, this openness does raise concerns about quality — vandalism, bias, and errors can be problems (Denning et al. 2005; Riehle 2006; Kittur et al. 2007). Despite the challenges associated with an open edit policy, Wikipedia articles are often of high quality (Giles 2005). Many suggest that this is a result of dedicated users that make many edits, monitor articles for changes, and engage Copyright c © 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. in debates on article discussion pages. These users are sometimes referred to as “zealots” (Anthony, Smith, and Williamson 2007), and studies claim that they are motivated by a system of peer recognition that bears resemblance to the academic community (Forte and Bruckman 2005). However, the contributions of “good Samaritan” users, who edit articles but have no desire to participate in the community, cannot not be underestimated (Anthony, Smith, and Williamson 2007). As Wikipedia continues to grow, however, it becomes more difficult for these users to keep up with the increasing number of articles and edits. Zealots comprise a relatively small portion of all Wikipedia users. Good Samaritan users are not likely to seek out errors, but instead rely on stumbling upon them. It is interesting to consider whether aiding users in detecting and focusing effort on quality problems could improve Wikipedia. In this paper, we examine the problem of estimating the quality of a new edit. Immediately, we face the problem of defining edit quality. It has been argued that there is no general definition of information quality, and hence quality must be defined using empirical observations of community interactions (Stvilia et al. 2008). Therefore, we define quality using implicit feedback from the Wikipedia community itself. That is, by observing the community’s response to a particular edit, we can estimate the edit’s quality. The quality metrics we propose are based on the assumption that edits to an article that are retained in subsequent versions of the article are of high quality, whereas edits that are quickly removed are of low quality. We use these community-defined measures of edit quality to learn statistical models that can predict the quality of a new edit. Quality is predicted using features of the edit itself, the author of the edit, and the article being edited. Through learning to predict quality, we also learn about factors that influence quality. Specifically, we provide analysis of model parameters to determine which features are the most useful for predicting quality. We advocate using edit quality predictions and information gleaned from model analysis not to place restrictions on editing, but to assist users in improving quality. That is, we aim to maintain a low barrier to participation, as those users not interested in the Wikipedia community can still be valuable contributors (Anthony, Smith, and Williamson 2007). Restrictions might also discourage new users, and drive away users who were drawn to the idea of a openly editable encyclopedia. Consequently, we suggest that the quality models be used to help users focus on predicted quality problems or to encourage participation. We evaluate the edit quality prediction models and provide analysis using the Spanish Wikipedia. Experiments demonstrate that the models attain better results when given access to content-based features, in addition to features of the contributing user. This suggests that a user-based solution to the Wikipedia quality problem may not be sufficient. Although we focus on Wikipedia in this paper, we think of this as an instance of a new problem: automatically predicting the quality of contributions in a collaborative environment.",Gregory Druck; G. Miklau; A. McCallum,2008,Journal,N/A,11,52,https://www.semanticscholar.org/paper/029c4971f7edd68c667baf15de392dbf104072f7,,
16,"Google Scholar, Web of Science",Measuring Quality of Collaboratively Edited Documents: The Case of Wikipedia,"Wikipedia is a great example of large scale collaboration, where people from all over the world together build the largest and maybe the most important human knowledge repository in the history. However, a number of studies showed that the quality of Wikipedia articles is not equally distributed. While many articles are of good quality, many others need to be improved. Assessing the quality of Wikipedia articles is very important for guiding readers towards articles of high quality and suggesting authors and reviewers which articles need to be improved. Due to the huge size of Wikipedia, an effective automatic assessment method to measure Wikipedia articles quality is needed. In this paper, we present an automatic assessment method of Wikipedia articles quality by analyzing their content in terms of their format features and readability scores. Our results show improvements both in terms of accuracy and information gain compared with other existing approaches.",Quang-Vinh Dang; C. Ignat,2016,Conference,"2016 IEEE 2nd International Conference on Collaboration and Internet Computing (CIC), pp. 266-275",69,32,https://www.semanticscholar.org/paper/31f226174f2da4ceca288b93769371c388d6c4fb,10.1109/CIC.2016.044,https://hal.archives-ouvertes.fr/hal-01388614/file/CIC2016.pdf
17,"ACM, Google Scholar, Web of Science",Predicting quality flaws in user-generated content: the case of wikipedia,"The detection and improvement of low-quality information is a key concern in Web applications that are based on user-generated content; a popular example is the online encyclopedia Wikipedia. Existing research on quality assessment of user-generated content deals with the classification as to whether the content is high-quality or low-quality. This paper goes one step further: it targets the prediction of quality flaws, this way providing specific indications in which respects low-quality content needs improvement. The prediction is based on user-defined cleanup tags, which are commonly used in many Web applications to tag content that has some shortcomings. We apply this approach to the English Wikipedia, which is the largest and most popular user-generated knowledge source on the Web. We present an automatic mining approach to identify the existing cleanup tags, which provides us with a training corpus of labeled Wikipedia articles. We argue that common binary or multiclass classification approaches are ineffective for the prediction of quality flaws and hence cast quality flaw prediction as a one-class classification problem. We develop a quality flaw model and employ a dedicated machine learning approach to predict Wikipedia's most important quality flaws. Since in the Wikipedia setting the acquisition of significant test data is intricate, we analyze the effects of a biased sample selection. In this regard we illustrate the classifier effectiveness as a function of the flaw distribution in order to cope with the unknown (real-world) flaw-specific class imbalances. The flaw prediction performance is evaluated with 10,000 Wikipedia articles that have been tagged with the ten most frequent quality flaws: provided test data with little noise, four flaws can be detected with a precision close to 1.",Maik Anderka; Benno Stein; Nedim Lipka,2012,Conference,"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 981-990",159,96,https://www.semanticscholar.org/paper/ab675867bf0d77b8d6c6b5b7dc2b00075ad422c4,10.1145/2348283.2348413,
18,"Google Scholar, Web of Science",Quality and Importance of Wikipedia Articles in Different Languages,,Włodzimierz Lewoniewski; Krzysztof Węcel; W. Abramowicz,2016,Conference,"International Conference on Information and Software Technologies, pp. 613-624",23,26,https://www.semanticscholar.org/paper/dd5a0bd3b918f4bc43568d461e60345fdeb55f21,10.1007/978-3-319-46254-7_50,
19,"ACM, Google Scholar","A jury of your peers: quality, experience and ownership in Wikipedia","Wikipedia is a highly successful example of what mass collaboration in an informal peer review system can accomplish. In this paper, we examine the role that the quality of the contributions, the experience of the contributors and the ownership of the content play in the decisions over which contributions become part of Wikipedia and which ones are rejected by the community. We introduce and justify a versatile metric for automatically measuring the quality of a contribution. We find little evidence that experience helps contributors avoid rejection. In fact, as they gain experience, contributors are even more likely to have their work rejected. We also find strong evidence of ownership behaviors in practice despite the fact that ownership of content is discouraged within Wikipedia.",Aaron L Halfaker; A. Kittur; R. Kraut; J. Riedl,2009,Journal,N/A,24,104,https://www.semanticscholar.org/paper/b91411b9d80525ed6fa61ab01f2a1c5459fbd05d,10.1145/1641309.1641332,
20,Google Scholar,Automatically Assessing Wikipedia Article Quality by Exploiting Article-Editor Networks,,Xinyi Li; Jintao Tang; Ting Wang; Zhunchen Luo; M. de Rijke,2015,Conference,"European Conference on Information Retrieval, pp. 574-580",16,37,https://www.semanticscholar.org/paper/f99a7a7ceca5bfe2e1c8c4d0ea5b67203dbc4f34,10.1007/978-3-319-16354-3_64,https://ilps.science.uva.nl/wp-content/uploads/sites/8/2015/11/DIR2015-proceedings.pdf
21,"ACM, Google Scholar, Web of Science",Information Quality in Wikipedia: The Effects of Group Composition and Task Conflict,"The success of Wikipedia demonstrates that self-organizing production communities can produce high-quality information-based products. Research on Wikipedia has proceeded largely atheoretically, focusing on (1) the diversity in members' knowledge bases as a determinant of Wikipedia's content quality, (2) the task-related conflicts that occur during the collaborative authoring process, and (3) the different roles members play in Wikipedia. We develop a theoretical model that explains how these three factors interact to determine the quality of Wikipedia articles. The results from the empirical study of 96 Wikipedia articles suggest that (1) diversity should be encouraged, as the creative abrasion that is generated when cognitively diverse members engage in task-related conflict leads to higher-quality articles, (2) task conflict should be managed, as conflict—notwithstanding its contribution to creative abrasion—can negatively affect group output, and (3) groups should maintain a balance of both administrative- and content-oriented members, as both contribute to the collaborative process.",Ofer Arazy; O. Nov; Raymond A. Patterson; M. Yeo,2011,Journal,"Journal of Management Information Systems, 27, pp. 71 - 98",117,204,https://www.semanticscholar.org/paper/26450adc2040f7f7d2ed7366bbe90d5091dbd3a1,10.2753/MIS0742-1222270403,
22,"ACM, Google Scholar",Interpolating Quality Dynamics in Wikipedia and Demonstrating the Keilana Effect,"For open, volunteer generated content like Wikipedia, quality is a prominent concern. To measure Wikipedia's quality, researchers have historically relied on expert evaluation or assessments of article quality by Wikipedians themselves. While both of these methods have proven effective for answering many questions about Wikipedia's quality and processes, they are both problematic: expert evaluation is expensive and Wikipedian quality assessments are sporadic and unpredictable. Studies that explore Wikipedia's quality level or the processes that result in quality improvements have only examined small snapshots of Wikipedia and often rely on complex propensity models to deal with the unpredictable nature of Wikipedians' own assessments. In this paper, I describe a method for measuring article quality in Wikipedia historically and at a finer granularity than was previously possible. I use this method to demonstrate an important coverage dynamic in Wikipedia (specifically, articles about women scientists) and offer this method, dataset, and open API to the research community studying Wikipedia quality dynamics.",Aaron L Halfaker,2017,Conference,Proceedings of the 13th International Symposium on Open Collaboration,21,32,https://www.semanticscholar.org/paper/1324bad076e1706e6b95144b738c7ae15cf4bb6e,10.1145/3125433.3125475,http://dl.acm.org/ft_gateway.cfm?id=3125475&type=pdf
23,"ACM, Google Scholar",An end-to-end learning solution for assessing the quality of Wikipedia articles,"Wikipedia is considered as the largest knowledge repository in the history of humanity and plays a crucial role in modern daily life. Assigning the correct quality class to Wikipedia articles is an important task in order to provide guidance for both authors and readers of Wikipedia. The manual review cannot cope with the editing speed of Wikipedia. An automatic classification is required to classify the quality of Wikipedia articles. Most existing approaches rely on traditional machine learning with manual feature engineering, which requires a lot of expertise and effort. Furthermore, it is known that there is no general perfect feature set because information leak always occurs in feature extraction phase. Also, for each language of Wikipedia, a new feature set is required. In this paper, we present an approach relying on deep learning for quality classification of Wikipedia articles. Our solution relies on Recurrent Neural Networks (RNN) which is an end-to-end learning technique that eliminates disadvantages of feature engineering. Our approach learns directly from raw data without human intervention and is language-neutral. Experimental results on English, French and Russian Wikipedia datasets show that our approach outperforms state-of-the-art solutions.",Quang-Vinh Dang; C. Ignat,2017,Conference,Proceedings of the 13th International Symposium on Open Collaboration,60,28,https://www.semanticscholar.org/paper/4244e4b33e00bc221f9e5fd78adde241ee499cc5,10.1145/3125433.3125448,https://hal.inria.fr/hal-01559693/file/OpenSym2017%20%281%29.pdf
24,"ACM, Google Scholar, Web of Science",Quality assessment of Wikipedia articles without feature engineering,"As Wikipedia became the largest human knowledge repository, quality measurement of its articles received a lot of attention during the last decade. Most research efforts focused on classification of Wikipedia articles quality by using a different feature set. However, so far, no “golden feature set” was proposed. In this paper, we present a novel approach for classifying Wikipedia articles by analysing their content rather than by considering a feature set. Our approach uses recent techniques in natural language processing and deep learning, and achieved a comparable result with the state-of-the-art.",Quang-Vinh Dang; C. Ignat,2016,Conference,"2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL), pp. 27-30",25,54,https://www.semanticscholar.org/paper/425064f6236d6457344d45c3517097936d42aea3,10.1145/2910896.2910917,https://hal.inria.fr/hal-01351226/file/JCDL2016.pdf
25,"Google Scholar, Web of Science",Relating Wikipedia article quality to edit behavior and link structure,,Thorsten Ruprechter; Tiago Santos; D. Helic,2020,Journal,"Applied Network Science, 5, pp. 1-20",61,8,https://www.semanticscholar.org/paper/d2742c2e19966865ab0db00fa515c79678eec9dd,10.1007/s41109-020-00305-y,
26,"ACM, Google Scholar, Web of Science",Assessing the quality of information on wikipedia: A deep‐learning approach,"Currently, web document repositories have been collaboratively created and edited. One of these repositories, Wikipedia, is facing an important problem: assessing the quality of Wikipedia. Existing approaches exploit techniques such as statistical models or machine leaning algorithms to assess Wikipedia article quality. However, existing models do not provide satisfactory results. Furthermore, these models fail to adopt a comprehensive feature framework. In this article, we conduct an extensive survey of previous studies and summarize a comprehensive feature framework, including text statistics, writing style, readability, article structure, network, and editing history. Selected state‐of‐the‐art deep‐learning models, including the convolutional neural network (CNN), deep neural network (DNN), long short‐term memory (LSTMs) network, CNN‐LSTMs, bidirectional LSTMs, and stacked LSTMs, are applied to assess the quality of Wikipedia. A detailed comparison of deep‐learning models is conducted with regard to different aspects: classification performance and training performance. We include an importance analysis of different features and feature sets to determine which features or feature sets are most effective in distinguishing Wikipedia article quality. This extensive experiment validates the effectiveness of the proposed model.",Ping Wang; Xiaodan Li,2020,Journal,"Journal of the Association for Information Science and Technology, 71",73,18,https://www.semanticscholar.org/paper/32c2952ea7128c0ed674548158c3e9d1e4ba3a13,10.1002/asi.24210,
27,Google Scholar,Automatically Assessing the Quality of Wikipedia Articles,"Since its inception in 2001, Wikipedia has fast become one of the Internet's most dominant sources of information. Dubbed ""the free encyclopedia"", Wikipedia contains millions of articles that are written, edited, and maintained by volunteers. Due in part to the open, collaborative process by which content is generated, many have questioned the reliability of these articles. The high variance in quality between articles is a potential source of confusion that likely leaves many visitors unable to distinguish between good articles and bad. In this work, we describe how a very simple metric – word count – can be used to as a proxy for article quality, and discuss the implications of this result for Wikipedia in particular, and quality assessment in general.",J. Blumenstock,2008,Journal,N/A,9,33,https://www.semanticscholar.org/paper/2ba510e022be06fa03dae878c5d9193bccef2cb5,,
28,"ACM, Google Scholar, Web of Science",Information quality assessment of community generated content: A user study of Wikipedia,"This study examines the ways in which information consumers evaluate the quality of content in a collaborative-writing environment, in this case Wikipedia. Sixty-four users were asked to assess the quality of five articles from the Hebrew Wikipedia, to indicate the highest- and lowest-quality article of the five and explain their choices. Participants viewed both the article page, and the article’s history page, so that their decision was based both on the article’s current content and on its development. The analysis shows that the attributes that most frequently assisted the users in deciding about the quality of the items were not unique to Wikipedia: attributes such as amount of information, satisfaction with content and external links were mentioned frequently, as with other information quality studies on the web. The findings also support the claim that quality is a subjective concept which depends on the user’s unique point of view. Attributes such as number of edits and number of unique editors received two contradictory meanings – both few edits/editors and many edits/editors were mentioned as attributes of high-quality articles.",Eti Yaari; Shifra Baruchson-Arbib; J. Bar-Ilan,2011,Journal,"Journal of Information Science, 37, pp. 487 - 498",32,78,https://www.semanticscholar.org/paper/4c43a063f1cd5cd888f526543be6160ecd0de9a9,10.1177/0165551511416065,
29,"Google Scholar, Web of Science",Using big data and network analysis to understand Wikipedia article quality,,Jun Liu; S. Ram,2018,Journal,"Data Knowl. Eng., 115, pp. 80-93",71,23,https://www.semanticscholar.org/paper/72ccb62a36d13a4563db9a926db72a6a57fe21fd,10.1016/J.DATAK.2018.02.004,
30,Google Scholar,Measuring article quality in Wikipedia: Lexical clue model,"Wikipedia is the most entry-abundant on-line encyclopedia. Some studies published by Nature proved that the scientific entries in Wikipedia are of good quality comparable to those in the Encyclopedia Britannica which are mainly maintained by experts. But the manual partition of the articles in Wikipedia from a WikiProject implies that high-quality articles are usually reached grade by grade via being repeatedly revised. So many work address to automatically measuring the article quality in Wikipedia based on some assumption of the relationship between the article quality and contributors' reputations, view behaviors, article status, inter-article link, or so on. In this paper, a lexical clue based measuring method is proposed to assess article quality in Wikipedia. The method is inspired the idea that the good articles have more regular statistic features on lexical usage than the primary ones due to the more revise by more people. We select 8 lexical features derived from the statistic on word usages in articles as the factors that can reflect article quality in Wikipedia. A decision tree is trained based on the lexical clue model. Using the decision tree, our experiments on a well-labeled collection of 200 Wikipedia articles shows that our method has more than 83% precise and recall.",Yanxiang Xu; Tiejian Luo,2011,Conference,"2011 3rd Symposium on Web Society, pp. 141-146",16,33,https://www.semanticscholar.org/paper/a2a786d8cfe123bfd58b481c5f03e4d6f969e0e0,10.1109/SWS.2011.6101286,
31,Google Scholar,A Hybrid Model for Quality Assessment of Wikipedia Articles,"The task of document quality assessment is a highly complex one, which draws on analysis of aspects including linguistic content, document structure, fact correctness, and community norms. We explore the task in the context of a Wikipedia article assessment task, and propose a hybrid approach combining deep learning with features proposed in the literature. Our method achieves 6.5% higher accuracy than the state of the art in predicting the quality classes of English Wikipedia articles over a novel dataset of around 60k Wikipedia articles. We also discuss limitations with this task setup, and possible directions for establishing more robust document quality assessment evaluations.",Aili Shen; Jianzhong Qi; Timothy Baldwin,2017,Conference,"Australasian Language Technology Association Workshop, pp. 43-52",44,26,https://www.semanticscholar.org/paper/894603d927860010ed3554a9922a992838188d81,,
32,"ACM, Google Scholar, Web of Science",Measuring article quality in Wikipedia using the collaboration network,"Collaboratively edited articles such as in Wikipedia suffer from well-identified problems regarding their quality, e.g., information accuracy, reputability of third-party sources, vandalism. Due to the huge number of articles and the intensive edit rate, the manual evaluation of article content quality is inconceivable. In this paper, we tackle the problem of automatically establishing the quality of Wikipedia articles. Evidences are shown to consider the interactions between authors and articles to assess the quality score. Collaborations between authors and reviewers are also considered to reinforce the discriminative process. This work gives a generic formulation of the Mutual Reinforcement principle held between articles quality and authors authority and take explicitly advantage of the co-edits graph generated by individuals. Experiments conducted on a set of representative data from Wikipedia show the effectiveness of our approach.",Baptiste de La Robertie; Y. Pitarch; O. Teste,2015,Conference,"2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), pp. 464-471",17,32,https://www.semanticscholar.org/paper/f2b5cb81200b34fbc5e2e1468218d9be27fc23d6,10.1145/2808797.2808895,
33,"ACM, Google Scholar, Web of Science",QuWi: quality control in Wikipedia,"We propose and evaluate QuWi (Quality in Wikipedia), a framework for quality control in Wikipedia. We build upon a previous proposal by Mizzaro [11], who proposed a method for substituting and/or complementing peer review in scholarly publishing. Since articles in Wikipedia are never finished, and their authors change continuously, we define a modified algorithm that takes into account the different domain, with particular attention to the fact that authors contribute identifiable pieces of information that can be further modified by other authors. The algorithm assigns quality scores to articles and contributors. The scores assigned to articles can be used, e.g., to let the reader understand how reliable are the articles he or she is looking at, or to help contributors in identifying low quality articles to be enhanced. The scores assigned to users measure the average quality of their contributions to Wikipedia and can be used, e.g., for conflict resolution policies based on the quality of involved users. Our proposed algorithm is experimentally evaluated by analyzing the obtained quality scores on articles for deletion and featured articles, also on six temporal Wikipedia snapshots. Preliminary results demonstrate that the proposed algorithm seems to appropriately identify high and low quality articles, and that high quality authors produce more long-lived contributions than low quality authors.",Alberto Cusinato; V. D. Mea; F. Salvatore; S. Mizzaro,2009,Conference,"Workshop on Information Credibility on the Web, pp. 27-34",19,24,https://www.semanticscholar.org/paper/254a11522cd21547d32c21ae500d8b2846f80189,10.1145/1526993.1527001,
34,"Google Scholar, Web of Science",Modelling the Quality of Attributes in Wikipedia Infoboxes,,Krzysztof Węcel; Włodzimierz Lewoniewski,2015,Conference,"Business Information Systems, pp. 308-320",12,21,https://www.semanticscholar.org/paper/a57b3142070beec458939e0a898991e0246f2ad8,10.1007/978-3-319-26762-3_27,
35,"ACM, Google Scholar",Towards automatic quality assurance in Wikipedia,"Featured articles in Wikipedia stand for high information quality, and it has been found interesting to researchers to analyze whether and how they can be distinguished from ""ordinary"" articles. Here we point out that article discrimination falls far short of writer support or automatic quality assurance: Featured articles are not identified, but are made. Following this motto we compile a comprehensive list of information quality flaws in Wikipedia, model them according to the latest state of the art, and devise one-class classification technology for their identification.",Maik Anderka; Benno Stein; Nedim Lipka,2011,Conference,Proceedings of the 20th international conference companion on World wide web,13,28,https://www.semanticscholar.org/paper/804fcac24b2e3db16b226dff9c71d85aa333a075,10.1145/1963192.1963196,
36,"ACM, Google Scholar, Web of Science",What makes a good biography?: multidimensional quality analysis based on wikipedia article feedback data,"With more than 22 million articles, the largest collaborative knowledge resource never sleeps, experiencing several article edits every second. Over one fifth of these articles describes individual people, the majority of which are still alive. Such articles are, by their nature, prone to corruption and vandalism. Manual quality assurance by experts can barely cope with this massive amount of data. Can it be effectively replaced by feedback from the crowd? Can we provide meaningful support for quality assurance with automated text processing techniques? Which properties of the articles should then play a key role in the machine learning algorithms and why? In this paper, we study the user-perceived quality of Wikipedia articles based on a novel Wikipedia user feedback dataset. In contrast to previous work on quality assessment which mostly relied on judgements of active Wikipedia authors, we analyze ratings of ordinary Wikipedia users along four quality dimensions (Complete, Well written, Trustworthy and Objective). We first present an empirical analysis of the novel dataset with over 36 million Wikipedia article ratings. We then select a subset of biographical articles and perform classification experiments to predict their quality ratings along each of the dimensions, exploring multiple linguistic, surface and network properties of the rated articles. Additionally, we study the classification performance and differences for the biographies of living and dead people as well as those for men and women. We demonstrate the effectiveness of our approach by the F-scores of 0.94, 0.89, 0.73, and 0.73 for the dimensions Complete, Well written, Trustworthy, and Objective. Based on the results, we believe that the quality assessment of big textual data can be effectively supported by current text classification and language processing tools.",Lucie Flekova; Oliver Ferschke; Iryna Gurevych,2014,Conference,Proceedings of the 23rd international conference on World wide web,47,25,https://www.semanticscholar.org/paper/2a4bb00fb6f8471e7b2e7a7479e2042271611c81,10.1145/2566486.2567972,
37,Google Scholar,Who does what: Collaboration patterns in the wikipedia and their impact on data quality,,Jun Liu; S. Ram,2009,Conference,"International Conference on Wireless Information Technology and Systems, ",0,34,https://www.semanticscholar.org/paper/c93d98cee9400be6e52fa73479ecb02d1eed5b07,,
38,"Google Scholar, Web of Science",NwQM: A Neural Quality Assessment Framework for Wikipedia,"Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics. Articles have been categorized by editors into several quality classes, which indicate their reliability as encyclopedic content. This manual designation is an onerous task because it necessitates profound knowledge about encyclopedic language, as well navigating circuitous set of wiki guidelines. In this paper we propose Neural wikipedia QualityMonitor (NwQM), a novel deep learning model which accumulates signals from several key information sources such as article text, meta data and images to obtain improved Wikipedia article representation. We present comparison of our approach against a plethora of available solutions and show 8% improvement over state-of-the-art approaches with detailed ablation studies.",Bhanu Prakash Reddy Guda; Sasi Bhusan; Soumya Sarkar; Animesh Mukherjee,2020,Conference,"Conference on Empirical Methods in Natural Language Processing, pp. 8396-8406",40,5,https://www.semanticscholar.org/paper/c44a14c16f0ae980c8075a3e8e83b41ce4a8d035,10.18653/v1/2020.emnlp-main.674,https://www.aclweb.org/anthology/2020.emnlp-main.674.pdf
39,"Google Scholar, Web of Science",Quality of information sources about mental disorders: a comparison of Wikipedia with centrally controlled web and printed sources,"Background Although mental health information on the internet is often of poor quality, relatively little is known about the quality of websites, such as Wikipedia, that involve participatory information sharing. The aim of this paper was to explore the quality of user-contributed mental health-related information on Wikipedia and compare this with centrally controlled information sources. Method Content on 10 mental health-related topics was extracted from 14 frequently accessed websites (including Wikipedia) providing information about depression and schizophrenia, Encyclopaedia Britannica, and a psychiatry textbook. The content was rated by experts according to the following criteria: accuracy, up-to-dateness, breadth of coverage, referencing and readability. Results Ratings varied significantly between resources according to topic. Across all topics, Wikipedia was the most highly rated in all domains except readability. Conclusions The quality of information on depression and schizophrenia on Wikipedia is generally as good as, or better than, that provided by centrally controlled websites, Encyclopaedia Britannica and a psychiatry textbook.",N. Reavley; A. Mackinnon; A. Morgan; M. Alvarez-Jimenez; S. Hetrick; E. Killackey; B. Nelson; R. Purcell; M. Yap; Anthony F Jorm,2011,Journal,"Psychological Medicine, 42, pp. 1753 - 1762",45,127,https://www.semanticscholar.org/paper/53df4208e5b82af4f902a9255e51fa1616aec8ba,10.1017/S003329171100287X,http://minerva-access.unimelb.edu.au/bitstreams/55a6cb89-80d6-519c-80df-c52a0c8b51ea/download
41,"Google Scholar, Web of Science",History-Based Article Quality Assessment on Wikipedia,"Wikipedia is widely considered as the biggest encyclopedia on Internet. Quality assessment of articles on Wikipedia has been studied for years. Conventional methods addressed this task by feature engineering and statistical machine learning algorithms. However, manually defined features are difficult to represent the long edit history of an article. Recently, researchers proposed an end-to-end neural model which used a Recurrent Neural Network(RNN) to learn the representation automatically. Although RNN showed its power in modeling edit history, the end-to-end method is time and resource consuming. In this paper, we propose a new history-based method to represent an article. We also take advantage of an RNN to handle the long edit history, but we do not abandon feature engineering. We still represent each revision of an article by manually defined features. This combination of deep neural model and feature engineering enables our model to be both simple and effective. Experiments demonstrate our model has better or comparable performance than previous works, and has the potential to work as a real-time service. Plus, we extend our model to do quality prediction.",Shiyue Zhang; Zheng Hu; Chunhong Zhang; K. Yu,2018,Conference,"2018 IEEE International Conference on Big Data and Smart Computing (BigComp), pp. 1-8",30,12,https://www.semanticscholar.org/paper/85323c0f359fd0ab782d07a048bd11dcb0ea5014,10.1109/BIGCOMP.2018.00010,
42,"ACM, Google Scholar",A breakdown of quality flaws in Wikipedia,"The online encyclopedia Wikipedia is a successful example of the increasing popularity of user generated content on the Web. Despite its success, Wikipedia is often criticized for containing low-quality information, which is mainly attributed to its core policy of being open for editing by everyone. The identification of low-quality information is an important task since Wikipedia has become the primary source of knowledge for a huge number of people around the world. Previous research on quality assessment in Wikipedia either investigates only small samples of articles, or else focuses on single quality aspects, like accuracy or formality. This paper targets the investigation of quality flaws, and presents the first complete breakdown of Wikipedia's quality flaw structure. We conduct an extensive exploratory analysis, which reveals (1) the quality flaws that actually exist, (2) the distribution of flaws in Wikipedia, and (3) the extent of flawed content. An important finding is that more than one in four English Wikipedia articles contains at least one quality flaw, 70% of which concern article verifiability.",Maik Anderka; Benno Stein,2012,Conference,N/A,30,32,https://www.semanticscholar.org/paper/3218ef46bcf1f2b5c9c9f77fbf8a0a5d51744d99,10.1145/2184305.2184309,
43,Google Scholar,FlawFinder: A Modular System for Predicting Quality Flaws in Wikipedia,"With over 23 million articles in 285 languages, Wikipedia is the largest free knowledge base on the web. Due to its open nature, everybody is allowed to access and edit the contents of this huge encyclopedia. As a downside of this open access policy, quality assessment of the content becomes a critical issue and is hardly manageable without computational assistance. In this paper, we present FlawFinder, a modular system for automatically predicting quality flaws in unseen Wikipedia articles. It competed in the inaugural edition of the Quality Flaw Prediction Task at the PAN Challenge 2012 and achieved the best precision of all systems and the second place in terms of recall and F1-score.",Oliver Ferschke; Iryna Gurevych; M. Rittberger,2012,Conference,"Conference and Labs of the Evaluation Forum, ",30,30,https://www.semanticscholar.org/paper/275fb8e6cd6290db26d6c75bf7343eea6c90c806,,
45,Google Scholar,Wikipedia in Academic Studies: Corrupting or Improving the Quality of Teaching and Learning?,"The founder of the free multilingual encyclopedia project Wikipedia, Jimmy Wales, anticipates considerable changes of the academic learning culture. He presumes that “teaching at universities will change, that professors will become mentors accompanying the development of their students” and that students will “discover the world for themselves following their own interest.” (Wales, 2008). Since 2001, Wikipedia has become one of the most popular websites and Web 2.0 applications worldwide. While the use of open contents and encyclopedic information as provided by Wikipedia has caused considerable problems within the academic community (e.g. the plagiarism problem and abSTRaCT",Klaus Wannemacher; Frank Schulenburg,2010,Journal,N/A,2,14,https://www.semanticscholar.org/paper/bbcea5ee78d72d3d928e69c3ff79fa47e83e64d7,10.4018/978-1-61520-678-0.CH017,
46,"Google Scholar, Web of Science",Relative Quality and Popularity Evaluation of Multilingual Wikipedia Articles,"Despite the fact that Wikipedia is often criticized for its poor quality, it continues to be one of the most popular knowledge bases in the world. Articles in this free encyclopedia on various topics can be created and edited in about 300 different language versions independently. Our research has showed that in language sensitive topics, the quality of information can be relatively better in the relevant language versions. However, in most cases, it is difficult for the Wikipedia readers to determine the language affiliation of the described subject. Additionally, each language edition of Wikipedia can have own rules in the manual assessing of the content’s quality. There are also differences in grading schemes between language versions: some use a 6–8 grade system to assess articles, and some are limited to 2–3. This makes automatic quality comparison of articles between various languages a challenging task, particularly if we take into account a large number of unassessed articles; some of the Wikipedia language editions have over 99% of articles without a quality grade. The paper presents the results of a relative quality and popularity assessment of over 28 million articles in 44 selected language versions. Comparative analysis of the quality and the popularity of articles in popular topics was also conducted. Additionally, the correlation between quality and popularity of Wikipedia articles of selected topics in various languages was investigated. The proposed method allows us to find articles with information of better quality that can be used to automatically enrich other language editions of Wikipedia.",Włodzimierz Lewoniewski; Krzysztof Węcel; W. Abramowicz,2017,Journal,"Informatics, 4, pp. 43",27,32,https://www.semanticscholar.org/paper/a37c5e68f712724157a4a6824f58a2425a830168,10.3390/informatics4040043,https://www.mdpi.com/2227-9709/4/4/43/pdf?version=1512800355
47,Google Scholar,Wikipedia model for collective intelligence: a review of information quality,"Online information seekers increasingly utilise the online encyclopaedia Wikipedia as a key reference source. Wikipedia's special feature is that it is based on the collective intelligence (CI) of lay citizens. Its consensus-building participatory knowledge-building processes replace traditional encyclopaedia processes founded on the knowledge of experts and gatekeeping practices. However there have been reports of concerns with the level of information quality provided by Wikipedia articles. This paper explores information quality for Wikipedia theoretically. First, it conceptualises the Wikipedia model of knowledge production and second, it analyses information quality for the model. Finally, the paper recommends some improvements for the model and discusses other implications for knowledge management theory and practice.",S. Lichtenstein; C. Parker,2009,Journal,"Int. J. Knowl. Learn., 5, pp. 254-272",66,20,https://www.semanticscholar.org/paper/d6d558ea18117584d78f290ceaab2b2478d9889c,10.1504/IJKL.2009.031199,
48,"Google Scholar, Web of Science",A hybrid approach to classifying Wikipedia article quality flaws with feature fusion framework,,Ping Wang; Muyan Li; Xiaodan Li; Heshen Zhou; Jingrui Hou,2021,Journal,"Expert Syst. Appl., 181, pp. 115089",30,6,https://www.semanticscholar.org/paper/88f8abd78cdeac3e003a4383d989a4239109375c,10.1016/J.ESWA.2021.115089,
49,Google Scholar,Requirements for the Linguistic Quality Control of Wikipedia Article,"The article focuses on key Wikipedia issues. One of them concerning Wikipedia article quality management has been considered. The importance of professional linguistic skills has been emphasized. Taking into account information quality model and crucial characteristics of well-written text, the linguistic quality control model has been provided. Three groups with the relevant features of the model have been allocated. The basic requirements for the linguistic quality control model have been suggested.",S. Albota; A. Peleshchyshyn,2019,Conference,"2019 IEEE 14th International Conference on Computer Sciences and Information Technologies (CSIT), 3, pp. 16-19",10,1,https://www.semanticscholar.org/paper/e6b0bac35a9e9e0aeae6695a981ea2aa91a9fa70,10.1109/STC-CSIT.2019.8929771,
50,"Google Scholar, Web of Science",The Quality and Readability of English Wikipedia Anatomy Articles,"Forty anatomy articles were sampled from English Wikipedia and assessed quantitatively and qualitatively. Quantitatively, each article’s edit history was analyzed by Wikipedia X‐tools, references and media were counted manually, and two readability indices were used to evaluate article readability. This analysis revealed that each article was updated 8.3 ± 6.8 times per month, and referenced with 33.5 ± 24.3 sources, such as journal articles and textbooks. Each article contained on average 14.0 ± 7.6 media items. The readability indices including: (1) Flesch–Kincaid Grade Level Readability Test and (2) Flesch Reading Ease Readability Formula demonstrated that the articles had low readability and were more appropriate for college students and above. Qualitatively, the sampled articles were evaluated by experts using a modified DISCERN survey. According to the modified DISCERN, 13 articles (32.5%), 24 articles (60%), 3 articles (7.5%), were rated as “good,” “moderate,” and “poor,” respectively. There were positive correlations between the DISCERN score and the number of edits (r = 0.537), number of editors (r = 0.560), and article length (r = 0.536). Strengths reported by the panel included completeness and coverage in 11 articles (27.5%), anatomical details in 10 articles (25%), and clinical details in 5 articles (12.5%). The panel also noted areas which could be improved, such as providing missing information in 28 articles (70%), inaccuracies in 10 articles (25%), and lack or poor use of images in 17 articles (42.5%). In conclusion, this study revealed that many Wikipedia anatomy articles were difficult to read. Each article’s quality was dependent on edit frequency and article length. Learners and students should be cautious when using Wikipedia articles for anatomy education due to these limitations.",A. Suwannakhan; Daniel Casanova-Martínez; Laphatrada Yurasakpong; Punchalee Montriwat; Krai Meemon; T. Limpanuparb,2020,Journal,"Anatomical Sciences Education, 13",61,14,https://www.semanticscholar.org/paper/b19f3cf7ec13c083f067875a80667c2ad540d6fc,10.1002/ase.1910,
51,"Google Scholar, Web of Science",Readability and quality of wikipedia pages on neurosurgical topics,,Omeed Modiri; D. Guha; N. Alotaibi; G. Ibrahim; N. Lipsman; A. Fallah,2018,Journal,"Clinical Neurology and Neurosurgery, 166, pp. 66-70",16,29,https://www.semanticscholar.org/paper/4364ec858a65cd9edae04e3fa6672613428c5423,10.1016/j.clineuro.2018.01.021,
52,Google Scholar,The Quality of Open Source Production: Zealots and Good Samaritans in the Case of Wikipedia,"New forms of production based in electronic technology, such as open-source and opencontent production, convert private commodities (typically software) into essentially public goods. A number of studies find that, like in other collective goods, incentives for reputation and group identity motivate contributions to open source goods, thereby overcoming the social dilemma inherent in producing such goods. In this paper we examine how contributor motivations affect the quality of contributions to the opencontent online encyclopedia Wikipedia. We find that quality is associated with contributor motivations, but in a surprisingly inconsistent way. Registered users’ quality increases with more contributions, consistent with the idea of participants motivated by reputation and commitment to the Wikipedia community. Surprisingly, however, we find the highest quality from the vast numbers of anonymous “Good Samaritans” who contribute only once. Our findings that Good Samaritans as well as committed “zealots” contribute high quality content to Wikipedia suggest that it is the quantity as well as the quality of contributors that positively affects the quality of open source production.",D. Anthony; Sean W. Smith; T. Williamson,2007,Unknown,N/A,52,49,https://www.semanticscholar.org/paper/ecddad562c391d87380de95c2053c3642afdb17c,,
53,"ACM, Google Scholar",Wikipedia and Westminster: Quality and Dynamics of Wikipedia Pages about UK Politicians,"Wikipedia is a major source of information providing a large variety of content online, trusted by readers from around the world. Readers go to Wikipedia to get reliable information about different subjects, one of the most popular being living people, and especially politicians. While a lot is known about the general usage and information consumption on Wikipedia, less is known about the life-cycle and quality of Wikipedia articles in the context of politics. The aim of this study is to quantify and qualify content production and consumption for articles about politicians, with a specific focus on UK Members of Parliament (MPs). First, we analyze spatio-temporal patterns of readers' and editors' engagement with MPs' Wikipedia pages, finding huge peaks of attention during election times, related to signs of engagement on other social media (e.g. Twitter). Second, we quantify editors' polarisation and find that most editors specialize in a specific party and choose specific news outlets as references. Finally we observe that the average citation quality is pretty high, with statements on 'Early life and career' missing citations most often (18%).",Pushkal Agarwal; M. Redi; Nishanth R. Sastry; E. Wood; Andrew Blick,2020,Conference,Proceedings of the 31st ACM Conference on Hypertext and Social Media,31,7,https://www.semanticscholar.org/paper/8b3292600f9b7e1d5a1fa5ff7ac2edd49380eb59,10.1145/3372923.3404817,
54,"Google Scholar, Web of Science",Network analysis of user generated content quality in Wikipedia,"Purpose – Social media platforms allow near‐unfettered creation and exchange of user generated content (UGC). Drawing from network science, the purpose of this paper is to examine whether high and low quality UGC differ in their connectivity structures in Wikipedia (which consists of interconnected user generated articles).Design/methodology/approach – Using Featured Articles as a proxy for high quality, a network analysis was undertaken of the revision history of six different language Wikipedias, to offer a network‐centric explanation for the emergence of quality in UGC.Findings – The network structure of interactions between articles and contributors plays an important role in the emergence of quality. Specifically the analysis reveals that high‐quality articles cluster in hubs that span structural holes.Research limitations/implications – The analysis does not capture the strength of interactions between articles and contributors. The implication of this limitation is that quality is viewed as a binar...",M. Ingawale; Amitava Dutta; R. Roy; P. Seetharaman,2013,Journal,"Online Inf. Rev., 37, pp. 602-619",44,21,https://www.semanticscholar.org/paper/53013037649e17b1194ae579a9a6e20bd7bf3e0d,10.1108/OIR-03-2011-0182,
55,"Google Scholar, Web of Science",Improving the Quality of Consumer Health Information on Wikipedia: Case Series,"Background Wikipedia is one of the most consulted health resources in the world. Since the public is using health information from Wikipedia to make health care decisions, improving the quality of that health information is in the public interest. The open editable content design of Wikipedia and quality control processes in place provide an opportunity to add high-value, evidence-based information and take an active role in improving the health care information infrastructure. Objective The aim of this project was to enhance Wikipedia health pages using high-quality, current research findings and track the persistence of those edits and number of page views after the changes to assess the reach of this initiative. Methods We conducted Wikipedia Editathons with 3 different cohorts of Physical Therapy (PT) students to add high-quality health information to existing Wikipedia pages. Students synthesized best evidence information and updated and/or corrected existing Wikipedia entries on specific health pages. To evaluate the impact of these contributions, we examined two factors: (1) response to our contributions from the Wikipedia editing community, including number and type of subsequent edits as well as persistence of the student contributions and (2) number of page views by the public from the time of the page edits. Results A total of 98 PT students in 3 different cohorts engaged in Editathons, editing 24 health pages. Of the 24 edits, 22 persisted at the end of the observation period (from time of entry to May 31, 2018) and received nearly 8 million page views. Each health page had an average of 354,724 page views. Conclusions The Wikipedia Editathon is an effective way to continuously enhance the quality of health information available on Wikipedia. It is also an excellent way of bridging health technology with best-evidence medical facts and disseminating accurate, useful information to the public.",S. S. Weiner; J. Horbacewicz; Lane Rasberry; Yocheved Bensinger-Brody,2019,Journal,"Journal of Medical Internet Research, 21",14,18,https://www.semanticscholar.org/paper/967262de8d06ad0a0c182dc826a5a7bda4397d2f,10.2196/12450,https://www.jmir.org/2019/3/e12450/PDF
56,Google Scholar,Quality in Internet Collective Goods : Zealots and Good Samaritans in the Case of Wikipedia,"One important innovation in information and communication technology developed over the past decade was organizational rather than merely technological. Open source production is remarkable because it converts a private commodity (typically software) into a public good. A number of studies examine the factors motivating contributions to open source production goods, but we argue it is important to understand the causes of high quality contributions to such goods. In this paper, we analyze quality in the open source online encyclopedia Wikipedia. We find that, for users who create an online persona through a registered user name, the quality of contributions increases as the number of contributions increase, consistent with the idea of experts motivated by reputation and committed to the Wikipedia community. Unexpectedly, however, we find the highest quality contributions come from the vast numbers of anonymous "" Good Samaritans "" who contribute infrequently. Our findings that Good Samaritans as well as committed "" Zealots "" contribute high quality content to Wikipedia suggest that open source production is remarkable as much for its organizational as its technological innovation that enables vast numbers of anonymous one-time contributors to create high quality, essentially public goods.",D. Anthony; Sean W. Smith; T. Williamson,2005,Unknown,N/A,49,75,https://www.semanticscholar.org/paper/da03cd44d8a163a185bd6e9d040c2ce22fc0ad14,,
57,"Google Scholar, Web of Science",Knowledge categorization affects popularity and quality of Wikipedia articles,"The existence of a shared classification system is essential to knowledge production, transfer, and sharing. Studies of knowledge classification, however, rarely consider the fact that knowledge categories exist within hierarchical information systems designed to facilitate knowledge search and discovery. This neglect is problematic whenever information about categorical membership is itself used to evaluate the quality of the items that the category contains. The main objective of this paper is to show that the effects of category membership depend on the position that a category occupies in the hierarchical knowledge classification system of Wikipedia—an open knowledge production and sharing platform taking the form of a freely accessible on-line encyclopedia. Using data on all English-language Wikipedia articles, we examine how the position that a category occupies in the classification hierarchy affects the attention that articles in that category attract from Wikipedia editors, and their evaluation of quality of the Wikipedia articles. Specifically, we show that Wikipedia articles assigned to coarse-grained categories (i. e., categories that occupy higher positions in the hierarchical knowledge classification system) garner more attention from Wikipedia editors (i. e., attract a higher volume of text editing activity), but receive lower evaluations (i. e., they are considered to be of lower quality). The negative relation between attention and quality implied by this result is consistent with current theories of social categorization, but it also goes beyond available results by showing that the effects of categorization on evaluation depend on the position that a category occupies in a hierarchical knowledge classification system.",J. Lerner; A. Lomi,2018,Journal,"PLoS ONE, 13",70,21,https://www.semanticscholar.org/paper/fa1db666c361fd03226c8e7bd81c5eb515719ac6,10.1371/journal.pone.0190674,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0190674&type=printable
58,"ACM, Google Scholar",On improving wikipedia search using article quality,"Wikipedia is presently the largest free-and-open online encyclopedia collaboratively edited and maintained by volunteers. While Wikipedia offers full-text search to its users, the accuracy of its relevance-based search can be compromised by poor quality articles edited by non-experts and inexperienced contributors. In this paper, we propose a framework that re-ranks Wikipedia search results considering article quality. We develop two quality measurement models, namely Basic and P<scp>eer</scp>R<scp>eview</scp>, to derive article quality based on co-authoring data gathered from articles' edit history. Compared withWikipedia's full-text search engine, Google and Wikiseek, our experimental results showed that (i) quality-only ranking produced by P<scp>eer</scp>R<scp>eview</scp> gives comparable performance to that of Wikipedia and Wikiseek; (ii) P<scp>eer</scp>R<scp>eview</scp> combined with relevance ranking outperforms Wikipedia's full-text search significantly, delivering search accuracy comparable to Google.",Meiqun Hu; Ee-Peng Lim; Aixin Sun; Hady W. Lauw; Ba-Quy Vuong,2007,Conference,"ACM International Workshop on Web Information and Data Management, pp. 145-152",25,19,https://www.semanticscholar.org/paper/2b05d442c55216c5819d0924016989f8dec3b23b,10.1145/1316902.1316926,
59,Google Scholar,Computational Trust in Web Content Quality: A Comparative Evalutation on the Wikipedia Project,Computational Trust in Web Content Quality: A Comparative Evalutation on the Wikipedia Project,Pierpaolo Dondio; Stephen Barrett,2007,Journal,"Informatica (Slovenia), 31, pp. 151-160",23,47,https://www.semanticscholar.org/paper/2bcb4e038b95d2ac3e7019580529af9a9be1e9d8,10.21427/D70W52,
60,"ACM, Google Scholar",Mutual evaluation of editors and texts for assessing quality of Wikipedia articles,"In this paper, we propose a method to identify good quality Wikipedia articles by mutually evaluating editors and texts. A major approach for assessing article quality is a text survival ratio based approach. In this approach, when a text survives beyond multiple edits, the text is assessed as good quality. This approach assumes that poor quality texts are deleted by editors with high possibility. However, many vandals delete good quality texts frequently, then the survival ratios of good quality texts are improperly decreased by vandals. As a result, many good quality texts are unfairly assessed as poor quality. In our method, we consider editor quality for calculating text quality, and decrease the impacts on text qualities by the vandals who has low quality. Using this improvement, the accuracy of the text quality should be improved. However, an inherent problem of this idea is that the editor qualities are calculated by the text qualities. To solve this problem, we mutually calculate the editor and text qualities until they converge. We did our experimental evaluation, and we confirmed that the proposed method could accurately assess the text qualities.",Yumiko Suzuki; M. Yoshikawa,2012,Conference,N/A,26,20,https://www.semanticscholar.org/paper/6e56f240fc876a9ac99b8377108672015142e708,10.1145/2462932.2462956,
61,"ACM, Google Scholar",Mining the Factors Affecting the Quality of Wikipedia Articles,"In order to observe the variation of factors affecting the quality of Wikipedia articles during the information quality improvement process, we proposed 28 metrics from four aspects, including lingual, structural, historical and reputational features, and then weighted each metrics indifferent stages by using neural network. We found lingual features weighted more in the lower quality stages, and structural features, along with historical features, became more important while article quality improved. However, reputational features did not act as important as expected. The findings indicate that the information quality is mainly affected by completeness, and well-written is a basic requirement in the initial stage. Reputation of authors or editors is not so important in Wikipedia because of its horizontal structure.",Kewen Wu; Qinghua Zhu; Y. Zhao; Hua Zheng,2010,Conference,"2010 International Conference of Information Science and Management Engineering, 1, pp. 343-346",8,13,https://www.semanticscholar.org/paper/cc68c28bc765082fa85c23fb71d7cdf212895185,10.1109/ISME.2010.114,
62,"Google Scholar, Web of Science",Measures for Quality Assessment of Articles and Infoboxes in Multilingual Wikipedia,,Włodzimierz Lewoniewski,2018,Conference,"Business Information Systems, pp. 619-633",69,10,https://www.semanticscholar.org/paper/3eaffa79334d8a91267b800d34c56bb41faa3c5b,10.1007/978-3-030-04849-5_53,
63,"Google Scholar, Web of Science",Measuring the quality of scientific references in Wikipedia: an analysis of more than 115M citations to over 800 000 scientific articles,"Wikipedia is a widely used online reference work which cites hundreds of thousands of scientific articles across its entries. The quality of these citations has not been previously measured, and such measurements have a bearing on the reliability and quality of the scientific portions of this reference work. Using a novel technique, a massive database of qualitatively described citations, and machine learning algorithms, we analyzed 1 923 575 Wikipedia articles which cited a total of 824 298 scientific articles in our database and found that most scientific articles cited by Wikipedia articles are uncited or untested by subsequent studies, and the remainder show a wide variability in contradicting or supporting evidence. Additionally, we analyzed 51 804 643 scientific articles from journals indexed in the Web of Science and found that similarly most were uncited or untested by subsequent studies, while the remainder show a wide variability in contradicting or supporting evidence.",J. M. Nicholson; A. Uppala; Matthias Sieber; P. Grabitz; M. Mordaunt; S. Rife,2020,Journal,"The FEBS Journal, 288",21,2,https://www.semanticscholar.org/paper/76e53f6e46b7b8a8ecfe735a23d4531dbd241911,10.1111/febs.15608,
64,"ACM, Google Scholar, Web of Science",Classifying Wikipedia Article Quality With Revision History Networks,"We present a novel model for classifying the quality of Wikipedia articles based on structural properties of a network representation of the article's revision history. We create revision history networks (an adaptation of Keegan et. al's article trajectory networks [7]), where nodes correspond to individual editors of an article, and edges join the authors of consecutive revisions. Using descriptive statistics generated from these networks, along with general properties like the number of edits and article size, we predict which of six quality classes (Start, Stub, C-Class, B-Class, Good, Featured) articles belong to, attaining a classification accuracy of 49.35% on a stratified sample of articles. These results suggest that structures of collaboration underlying the creation of articles, and not just the content of the article, should be considered for accurate quality classification.",Narun K. Raman; Nathaniel Sauerberg; Jonah Fisher; Sneha Narayan,2020,Conference,Proceedings of the 16th International Symposium on Open Collaboration,22,7,https://www.semanticscholar.org/paper/8a74f75cb449bff4833be8f77ccf5f2493d75a19,10.1145/3412569.3412581,
65,"Google Scholar, Web of Science",WikipediaViz: Conveying article quality for casual Wikipedia readers,"As Wikipedia has become one of the most used knowledge bases worldwide, the problem of the trustworthiness of the information it disseminates becomes central. With WikipediaViz, we introduce five visual indicators integrated to the Wikipedia layout that can keep casual Wikipedia readers aware of important metainformation about the articles they read. The design of WikipediaViz was inspired by two participatory design sessions with expert Wikipedia writers and sociologists who explained the clues they used to quickly assess the trustworthiness of articles. According to these results, we propose five metrics for Maturity and Quality assessment of Wikipedia articles and their accompanying visualizations to provide the readers with important clues about the editing process at a glance. We also report and discuss about the results of the user studies we conducted. Two preliminary pilot studies show that all our subjects trust Wikipedia articles almost blindly. With the third study, we show that WikipediaViz significantly reduces the time required to assess the quality of articles while maintaining a good accuracy.",Fanny Chevalier; Stéphane Huot; Jean-Daniel Fekete,2010,Conference,"2010 IEEE Pacific Visualization Symposium (PacificVis), pp. 49-56",26,37,https://www.semanticscholar.org/paper/07a813d7de9ac072ed970217024a51a835433654,10.1109/PACIFICVIS.2010.5429611,http://hal.inria.fr/docs/00/55/06/98/PDF/WikipediaViz-PacificVis2010.pdf
66,"Google Scholar, Web of Science",Multilingual Ranking of Wikipedia Articles with Quality and Popularity Assessment in Different Topics,"On Wikipedia, articles about various topics can be created and edited independently in each language version. Therefore, the quality of information about the same topic depends on the language. Any interested user can improve an article and that improvement may depend on the popularity of the article. The goal of this study is to show what topics are best represented in different language versions of Wikipedia using results of quality assessment for over 39 million articles in 55 languages. In this paper, we also analyze how popular selected topics are among readers and authors in various languages. We used two approaches to assign articles to various topics. First, we selected 27 main multilingual categories and analyzed all their connections with sub-categories based on information extracted from over 10 million categories in 55 language versions. To classify the articles to one of the 27 main categories, we took into account over 400 million links from articles to over 10 million categories and over 26 million links between categories. In the second approach, we used data from DBpedia and Wikidata. We also showed how the results of the study can be used to build local and global rankings of the Wikipedia content.",Włodzimierz Lewoniewski; Krzysztof Węcel; W. Abramowicz,2019,Journal,"Comput., 8, pp. 60",90,22,https://www.semanticscholar.org/paper/d5b3f3e9403eda8b45184951b269ee7997452c6b,10.3390/COMPUTERS8030060,https://www.mdpi.com/2073-431X/8/3/60/pdf
67,"ACM, Google Scholar, Web of Science",Automatically assessing the quality of Wikipedia contents,"With the development of Web 2.0 technologies, people have gone from being mere content users to content generators. In this context, the evaluation of the quality of (potential) information available online has become a crucial issue. Nowadays, one of the biggest online resources that users rely on as a knowledge base is Wikipedia. The collaborative aspect at the basis of Wikipedia can let to the possible creation of low-quality articles or even misinformation if the process of monitoring the generation and the revision of articles is not performed in a precise and timely way. For this reason, in this paper, the problem of automatically evaluating the quality of Wikipedia contents is considered, by proposing a supervised approach based on Machine Learning to perform the classification of articles on qualitative bases. With respect to prior literature, a wider set of features connected to Wikipedia articles has been taken into account, as well as previously unconsidered aspects connected to the generation of a labeled dataset to train the model, and the use of Gradient Boosting, which produced encouraging results.",Elias Bassani; Marco Viviani,2019,Conference,Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing,27,6,https://www.semanticscholar.org/paper/2a9be636902fe22e4d04d4f9721e4e914512e3d7,10.1145/3297280.3297357,
68,"Google Scholar, Web of Science",Pharmacy students can improve access to quality medicines information by editing Wikipedia articles,,D. Apollonio; Keren Broyde; A. Azzam; Michael De Guia; James M Heilman; T. Brock,2018,Journal,"BMC Medical Education, 18",43,17,https://www.semanticscholar.org/paper/3ad3f5e0fdc59499a4b7d40df50b61f9e3ade7d2,10.1186/s12909-018-1375-z,
69,"Google Scholar, Web of Science",Wisdom of crowds: the effect of participant composition and contribution behavior on Wikipedia article quality,"This paper aims to explore the effect of participant composition and contribution behavior of the different types of participants on the quality of knowledge generation in online communities.,This study samples all the featured articles in Chinese Wikipedia and performs a Cox regression to reveal how participant composition and contribution behavior affect the quality of articles in different contexts.,The results show that an increase in the number of participants increases the possibility of either enhancing or reducing the article quality. In most cases, the greater the proportion of core members (people who frequently participate in editing), the higher the possibility of enhancing the article quality. Occasional participants’ editorial behavior hinders quality promotion, this negative effect weakens when such editorial behavior becomes more frequent.,The findings help to better leverage the role of online communities in practice and to achieve knowledge collaboration in a more efficient manner. For example, an appropriate centralized organizational form should be established in online communities to improve the efficiency of crowd contributions. And it is worth developing mechanism to encourage participants to frequently participate in editing the article.,This study contributes to the research on the organizational forms of online communities by showing the effect of participant composition and behavior in the new form of organizing on knowledge generation. This study also contributes to the research on wisdom of crowds by revealing who in a group of participants, in what context, and by what means influence knowledge generation.",Yan Lin; Chenxi Wang,2020,Journal,"J. Knowl. Manag., 24, pp. 324-345",54,4,https://www.semanticscholar.org/paper/da5790edd4224160d1da8107ba083736c9b8683e,10.1108/jkm-08-2019-0416,
70,"ACM, Google Scholar, Web of Science",Quality Evaluation of Wikipedia Articles through Edit History and Editor Groups,,SetsyWei Wang; M. Iwaihara,2010,Conference,"Asia-Pacific Web Conference, pp. 188-199",18,16,https://www.semanticscholar.org/paper/527699259b21d363d57ac38db4986bb433c3ded5,10.1007/978-3-642-20291-9_20,
71,"Google Scholar, Web of Science",Application of SEO Metrics to Determine the Quality of Wikipedia Articles and Their Sources,,Włodzimierz Lewoniewski; Ralf-Christian Härting; Krzysztof Węcel; Christopher Reichstein; W. Abramowicz,2018,Conference,"International Conference on Information and Software Technologies, pp. 139-152",21,7,https://www.semanticscholar.org/paper/662ee60986d028487b4d06fd970e6645aa03728d,10.1007/978-3-319-99972-2_11,
72,"ACM, Google Scholar, Web of Science",Diversity of editors and teams versus quality of cooperative work: experiments on wikipedia,,M. Sydow; Katarzyna Baraniak; Paweł Teisseyre,2017,Journal,"Journal of Intelligent Information Systems, 48, pp. 601-632",31,10,https://www.semanticscholar.org/paper/4e5cdf30ab1fbd13404068c1c7894f8eeb33d0bd,10.1007/s10844-016-0428-1,https://link.springer.com/content/pdf/10.1007%2Fs10844-016-0428-1.pdf
73,"Google Scholar, Web of Science",Readability and quality of Wikipedia articles on pelvic floor disorders,,S. Handler; S. Eckhardt; Y. Takashima; A. Jackson; Christina Truong; T. Yazdany,2021,Journal,"International Urogynecology Journal, 32, pp. 3249 - 3258",32,7,https://www.semanticscholar.org/paper/a6deb56a4f7ffb0859002c508ba9754b45906080,10.1007/s00192-021-04776-0,
74,"ACM, Google Scholar, Web of Science",A deep learning-based quality assessment model of collaboratively edited documents: A case study of Wikipedia,"Wikipedia is becoming increasingly critical in helping people obtain information and knowledge. Its leading advantage is that users can not only access information but also modify it. However, this presents a challenging issue: how can we measure the quality of a Wikipedia article? The existing approaches assess Wikipedia quality by statistical models or traditional machine learning algorithms. However, their performance is not satisfactory. Moreover, most existing models fail to extract complete information from articles, which degrades the model’s performance. In this article, we first survey related works and summarise a comprehensive feature framework. Then, state-of-the-art deep learning models are introduced and applied to assess Wikipedia quality. Finally, a comparison among deep learning models and traditional machine learning models is conducted to validate the effectiveness of the proposed model. The models are compared extensively in terms of their training and classification performance. Moreover, the importance of each feature and the importance of different feature sets are analysed separately.",Ping Wang; Xiaodan Li; Renli Wu,2019,Journal,"Journal of Information Science, 47, pp. 176 - 191",64,2,https://www.semanticscholar.org/paper/0abf43df2e9c0fee1262e9d127c05383ffc78251,10.1177/0165551519877646,
75,Google Scholar,Overview of the 1th International Competition on Quality Flaw Prediction in Wikipedia,"The paper overviews the task ""Quality Flaw Prediction in Wikipedia"" of the PAN'12 competition. An evaluation corpus is introduced which comprises 1 592 226 English Wikipedia articles, of which 208 228 have been tagged to con- tain one of ten important quality flaws. Moreover, the performance of three qual- ity flaw classifiers is evaluated.",Maik Anderka; Benno Stein,2012,Conference,"Conference and Labs of the Evaluation Forum, ",16,16,https://www.semanticscholar.org/paper/d87a2a5d19419259895a23ff0003092a094518f2,,
76,"ACM, Google Scholar",Automatically Labeling Low Quality Content on Wikipedia By Leveraging Patterns in Editing Behaviors,"Wikipedia articles aim to be definitive sources of encyclopedic content. Yet, only 0.6% of Wikipedia articles have high quality according to its quality scale due to insufficient number of Wikipedia editors and enormous number of articles. Supervised Machine Learning (ML) quality improvement approaches that can automatically identify and fix content issues rely on manual labels of individual Wikipedia sentence quality. However, current labeling approaches are tedious and produce noisy labels. Here, we propose an automated labeling approach that identifies the semantic category (e.g., adding citations, clarifications) of historic Wikipedia edits and uses the modified sentences prior to the edit as examples that require that semantic improvement. Highest-rated article sentences are examples that no longer need semantic improvements. We show that training existing sentence quality classification algorithms on our labels improves their performance compared to training them on existing labels. Our work shows that editing behaviors of Wikipedia editors provide better labels than labels generated by crowdworkers who lack the context to make judgments that the editors would agree with.",Sumit Asthana; Sabrina Tobar Thommel; Aaron L Halfaker; Nikola Banovic,2021,Conference,"Proceedings of the ACM on Human-Computer Interaction, 5, pp. 1 - 23",49,0,https://www.semanticscholar.org/paper/65063772f09683c581f281ec3aa640549cd9aaee,10.1145/3479503,http://arxiv.org/pdf/2108.02252
77,"ACM, Google Scholar",Your process is showing: controversy management and perceived quality in wikipedia,"Large-scale collaboration systems often separate their content from the deliberation around how that content was produced. Surfacing this deliberation may engender trust in the content generation process if the deliberation process appears fair, well-reasoned, and thorough. Alternatively, it could encourage doubts about content quality, especially if the process appears messy or biased. In this paper we report the results of an experiment where we found that surfacing deliberation generally led to decreases in perceptions of quality for the article under consideration, especially - but not only - if the discussion revealed conflict. The effect size depends on the type of editors' interactions. Finally, this decrease in actual article quality rating was accompanied by self-reported improved perceptions of the article and Wikipedia overall.",W. Towne; A. Kittur; Peter Kinnaird; J. Herbsleb,2013,Conference,Proceedings of the 2013 conference on Computer supported cooperative work,36,17,https://www.semanticscholar.org/paper/d38b8b87051c3597811385018593daa3e491ac14,10.1145/2441776.2441896,
78,"ACM, Google Scholar, Web of Science",Article quality classification on Wikipedia: introducing document embeddings and content features,"The quality of articles on the Wikipedia platform is vital for its success. Currently, the assessment of quality is performed manually by the Wikipedia community, where editors classify articles into pre-defined quality classes. However, this approach is hardly scalable and hence, approaches for the automatic classification have been investigated. In this paper, we extend this previous line of research on article quality classification by extending the set of features with novel content and edit features (e.g., document em-beddings of articles). We propose a classification approach utilizing gradient boosted trees based on this novel, extended set of features extracted from Wikipedia articles. Based on an established dataset containing Wikipedia articles and quality classes, we show that our approach is able to substantially outperform previous approaches (also including recent deep learning methods). Furthermore, we shed light on the contribution of individual features and show that the proposed features indeed capture the quality of an article well.",Manuel Schmidt; Eva Zangerle,2019,Conference,Proceedings of the 15th International Symposium on Open Collaboration,24,5,https://www.semanticscholar.org/paper/beb2803e597f83e85c76f6db6834bb88a7aa8f6b,10.1145/3306446.3340831,
79,"ACM, Google Scholar, Web of Science",Assessing quality score of Wikipedia article using mutual evaluation of editors and texts,"In this paper, we propose a method for assessing quality scores of Wikipedia articles by mutually evaluating editors and texts. Survival ratio based approach is a major approach to assessing article quality. In this approach, when a text survives beyond multiple edits, the text is assessed as good quality, because poor quality texts have a high probability of being deleted by editors. However, many vandals, low quality editors, delete good quality texts frequently, which improperly decreases the survival ratios of good quality texts. As a result, many good quality texts are unfairly assessed as poor quality. In our method, we consider editor quality score for calculating text quality score, and decrease the impact on text quality by vandals. Using this improvement, the accuracy of the text quality score should be improved. However, an inherent problem with this idea is that the editor quality scores are calculated by the text quality scores. To solve this problem, we mutually calculate the editor and text quality scores until they converge. In this paper, we prove that the text quality score converges. We did our experimental evaluation, and confirmed that our proposed method could accurately assess the text quality scores.",Yumiko Suzuki; M. Yoshikawa,2013,Conference,Proceedings of the 22nd ACM international conference on Information & Knowledge Management,8,17,https://www.semanticscholar.org/paper/68b167406f708ce39421126bdbb576930c8347a0,10.1145/2505515.2505610,
80,"ACM, Google Scholar",Measuring the Quality of Edits to Wikipedia,"Wikipedia is unique among reference works both in its scale and in the openness of its editing interface. The question of how it can achieve and maintain high-quality encyclopedic articles is an area of active research. In order to address this question, researchers need to build consensus around a sensible metric to assess the quality of contributions to articles. This measure must not only reflect an intuitive concept of ""quality,"" but must also be scalable and run efficiently. Building on prior work in this area, this paper uses human raters through Amazon Mechanical Turk to validate an efficient, automated quality metric.",Susan Biancani,2014,Conference,Proceedings of The International Symposium on Open Collaboration,11,10,https://www.semanticscholar.org/paper/cd8748fd30092dff2739a2471cbefc78c25e1e53,10.1145/2641580.2641621,
81,Google Scholar,Mining and Predicting Temporal Patterns in the Quality Evolution of Wikipedia Articles,"In online open production communities like Wikipedia, quality is often an important criterion to assess the success of peer collaboration activities. Over the years we have accumulated a nice body ...",Haifeng Zhang; Yuqing Ren; R. Kraut,2018,Conference,"Hawaii International Conference on System Sciences, pp. 1-10",46,7,https://www.semanticscholar.org/paper/0eed508e65f53dd92fa129523129664879695849,10.5465/AMBPP.2018.13746ABSTRACT,https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1569&context=hicss-53
82,Google Scholar,Quality Assessment of Wikipedia Articles Using h-index,"In this paper, we propose a method for assessing quality values of Wikipedia articles from edit history using h-index. One of the major methods for assessing Wikipedia article quality is a peer-review based method. In this method, we assume that if an editor’s texts are left by the other editors, the texts are approved by the editors, then the editor is decided as a good editor. However, if an editor edits multiple articles, and the editor is approved at a small number of articles, the quality value of the editor deeply depends on the quality of the texts. In this paper, we apply h-index, which is a simple but resistant to excessive values, to the peer-review based Wikipedia article assessment method. Although h-index can identify whether an editor is a good quality editor or not, h-index cannot identify whether the editor is a vandal or an inactive editor. To solve this problem, we propose p-ratio for identifying which editors are vandals or inactive editors. From our experiments, we confirmed that by integrating h-index with p-ratio, the accuracy of article quality assessment in our method outperforms the existing peer-review based method.",Yumiko Suzuki,2015,Journal,"J. Inf. Process., 23, pp. 22-30",31,20,https://www.semanticscholar.org/paper/18827d99db45b45c9b5c00bd05508fbf524e32b7,10.2197/IPSJJIP.23.22,https://www.jstage.jst.go.jp/article/ipsjjip/23/1/23_22/_pdf
83,"ACM, Google Scholar, Web of Science",WikiLyzer: Interactive Information Quality Assessment in Wikipedia,"Digital libraries and services enable users to access large amounts of data on demand. Yet, quality assessment of information encountered on the Internet remains an elusive open issue. For example, Wikipedia, one of the most visited platforms on the Web, hosts thousands of user-generated articles and undergoes 12 million edits/contributions per month. User-generated content is undoubtedly one of the keys to its success, but also a hindrance to good quality: contributions can be of poor quality because anyone, even anonymous users, can participate. Though Wikipedia has defined guidelines as to what makes the perfect article, authors find it difficult to assert whether their contributions comply with them and reviewers cannot cope with the ever growing amount of articles pending review. Great efforts have been invested in algorithmic methods for automatic classification of Wikipedia articles (as featured or non-featured) and for quality flaw detection. However, little has been done to support quality assessment of user-generated content through interactive tools that combine automatic methods and human intelligence. We developed WikiLyzer, a Web toolkit comprising three interactive applications designed to assist (i) knowledge discovery experts in creating and testing metrics for quality measurement, (ii) Wikipedia users searching for good articles, and (iii) Wikipedia authors that need to identify weaknesses to improve a particular article. A design study sheds a light on how experts could create complex quality metrics with our tool, while a user study reports on its usefulness to identify high-quality content.",Cecilia di Sciascio; D. Strohmaier; M. Errecalde; Eduardo Veas,2017,Conference,Proceedings of the 22nd International Conference on Intelligent User Interfaces,34,8,https://www.semanticscholar.org/paper/452ca7a696f5ea260f0b334c67ef3b71a05c676b,10.1145/3025171.3025201,
84,"Google Scholar, Web of Science","‘WP2Cochrane’, a tool linking Wikipedia to the Cochrane Library: Results of a bibliometric analysis evaluating article quality and importance","Medical information on English Wikipedia was accessed over 2 billion times in 2018. Our goal was to develop an automated system to assist Wikipedia volunteers to improve articles with high-quality sources from journals such as The Cochrane Library. We created an automated indexing system by linking available reviews from the Cochrane library with disease-related Wikipedia articles and evaluating the relationship between the quality and importance of these articles with the number of relevant and cited Cochrane reviews. We first conducted a bibliometric analysis, identifying disease-related Wikipedia articles and relevant/cited Cochrane reviews. Citations were thematically coded, and descriptive statistics were calculated. Finally, separate multinomial logistic regression analyses were conducted for article quality and importance. The indexing system identified 4381 disease-related Wikipedia articles, 1193 (27%) of which cited a Cochrane review. Higher quality Wikipedia articles were more likely to cite a Cochrane review (p = 0.002), while lower quality articles were less likely to cite a Cochrane review (p < 0.0005). A greater number of Cochrane reviews are available for more ‘important’ Wikipedia articles (p < 0.005), and these articles were more likely to cite a Cochrane review (p < 0.005). This approach to an indexing system can be leveraged by Wikipedia contributors and editors seeking to update disease-related Wikipedia articles with relevant Cochrane reviews (thus improving their quality), and online information seekers in need of additional information to supplement their Wikipedia search.",Arash Joorabchi; C. Doherty; Jennifer Dawson,2019,Journal,"Health Informatics Journal, 26, pp. 1881 - 1897",58,6,https://www.semanticscholar.org/paper/0f57c6a9447ecc5219d4ad2bc3b7ee5a5805d073,10.1177/1460458219892711,
85,Google Scholar,An Edit-centric Approach for Wikipedia Article Quality Assessment,"We propose an edit-centric approach to assess Wikipedia article quality as a complementary alternative to current full document-based techniques. Our model consists of a main classifier equipped with an auxiliary generative module which, for a given edit, jointly provides an estimation of its quality and generates a description in natural language. We performed an empirical study to assess the feasibility of the proposed model and its cost-effectiveness in terms of data and quality requirements.",Edison Marrese-Taylor; Pablo Loyola; Y. Matsuo,2019,Conference,"Conference on Empirical Methods in Natural Language Processing, pp. 381-386",25,9,https://www.semanticscholar.org/paper/1ec28bdcacf6fd2c72c227ccd74a2ad1ee8d4b1b,10.18653/v1/D19-5550,https://www.aclweb.org/anthology/D19-5550.pdf
87,"Google Scholar, Web of Science",StRE: Self Attentive Edit Quality Prediction in Wikipedia,"Wikipedia can easily be justified as a behemoth, considering the sheer volume of content that is added or removed every minute to its several projects. This creates an immense scope, in the field of natural language processing toward developing automated tools for content moderation and review. In this paper we propose Self Attentive Revision Encoder (StRE) which leverages orthographic similarity of lexical units toward predicting the quality of new edits. In contrast to existing propositions which primarily employ features like page reputation, editor activity or rule based heuristics, we utilize the textual content of the edits which, we believe contains superior signatures of their quality. More specifically, we deploy deep encoders to generate representations of the edits from its text content, which we then leverage to infer quality. We further contribute a novel dataset containing ∼ 21M revisions across 32K Wikipedia pages and demonstrate that StRE outperforms existing methods by a significant margin – at least 17% and at most 103%. Our pre-trained model achieves such result after retraining on a set as small as 20% of the edits in a wikipage. This, to the best of our knowledge, is also the first attempt towards employing deep language models to the enormous domain of automated content moderation and review in Wikipedia.",Soumya Sarkar; Bhanu Prakash Reddy Guda; Sandipan Sikdar; Animesh Mukherjee,2019,Conference,"ArXiv, abs/1906.04678",39,8,https://www.semanticscholar.org/paper/e2988816a10bf5a50bd952601ac06026e5493782,10.18653/v1/P19-1387,https://www.aclweb.org/anthology/P19-1387.pdf
88,"ACM, Google Scholar",The role of conflict in determining consensus on quality in Wikipedia articles,"This paper presents research that investigated the role of conflict in the editorial process of the online encyclopedia, Wikipedia. The study used a grounded approach to analyzing 147 conversations about quality from the archived history of the Wikipedia article Australia. It found that conflict in Wikipedia is a generative friction, regulated by references to policy as part of a coordinated effort within the community to improve the quality of articles.",Kim Osman,2013,Conference,Proceedings of the 9th International Symposium on Open Collaboration,51,10,https://www.semanticscholar.org/paper/d7d82669f167bd609e3d593d0d0cc6743d58c8ef,10.1145/2491055.2491067,https://eprints.qut.edu.au/64185/1/Osman_WikiSym13_Theroleofconflict.pdf
89,"ACM, Google Scholar",Quality assessment of wikipedia articles: a deep learning approach by Quang Vinh Dang and Claudia-Lavinia Ignat with Martin Vesely as coordinator,"Wikipedia is indeed a very important knowledge sharing platform. However, since its start in 2001, the quality of Wikipedia is questioned because its content is created potentially by everyone who ...",DangQuang Vinh; IgnatClaudia-Lavinia,2016,Journal,"ACM Sigweb Newsletter, ",30,5,https://www.semanticscholar.org/paper/ce000225b8d092b4872ff49081238b73ba8185c5,10.1145/2996442.2996447,https://hal.archives-ouvertes.fr/hal-01393227/file/sigweb_newsletter_latex.pdf
90,"ACM, Google Scholar",Evaluating the trustworthiness of Wikipedia articles through quality and credibility,"Wikipedia has become a very popular destination for Web surfers seeking knowledge about a wide variety of subjects. While it contains many helpful articles with accurate information, it also consists of unreliable articles with inaccurate or incomplete information. A casual observer might not be able to differentiate between the good and the bad. In this work, we identify the necessity and challenges for trust assessment in Wikipedia, and propose a framework that can help address these challenges by identifying relevant features and providing empirical means to meet the requirements for such an evaluation. We select relevant variables and perform experiments to evaluate our approach. The results demonstrate promising performance that is better than comparable approaches and could possibly be replicated with other social media applications.",S. Moturu; Huan Liu,2009,Journal,N/A,9,18,https://www.semanticscholar.org/paper/76382864f6d33a8b33296fedadf7dc36be6cbb13,10.1145/1641309.1641349,
91,Google Scholar,The quality of content in open online collaboration platforms: approaches to NLP-supported information quality management in Wikipedia,"Over the past decade, the paradigm of the World Wide Web has shifted from static web pages towards participatory and collaborative content production. The main properties of this user generated content are a low publication threshold and little or no editorial control. While this has improved the variety and timeliness of the available information, it causes an even higher variance in quality than the already heterogeneous quality of traditional web content. Wikipedia is the prime example for a successful, large-scale, collaboratively created resource that reflects the spirit of the open collaborative content creation paradigm. Even though recent studies have confirmed that the overall quality of Wikipedia is high, there is still a wide gap that must be bridged before Wikipedia reaches the state of a reliable, citable source. A key prerequisite to reaching this goal is a quality management strategy that can cope both with the massive scale of Wikipedia and its open and almost anarchic nature. This includes an efficient communication platform for work coordination among the collaborators as well as techniques for monitoring quality problems across the encyclopedia. This dissertation shows how natural language processing approaches can be used to assist information quality management on a massive scale. In the first part of this thesis, we establish the theoretical foundations for our work. We first introduce the relatively new concept of open online collaboration with a particular focus on collaborative writing and proceed with a detailed discussion of Wikipedia and its role as an encyclopedia, a community, an online collaboration platform, and a knowledge resource for language technology applications. We then proceed with the three main contributions of this thesis. Even though there have been previous attempts to adapt existing information quality frameworks to Wikipedia, no quality model has yet incorporated writing quality as a central factor. Since Wikipedia is not only a repository of mere facts but rather consists of full text articles, the writing quality of these articles has to be taken into consideration when judging article quality. As the first main contribution of this thesis, we therefore define a comprehensive article quality model that aims to consolidate both the quality of writing and the quality criteria defined in multiple Wikipedia guidelines and policies into a single model. The model comprises 23 dimensions segmented into the four layers of intrinsic quality, contextual quality, writing quality and organizational quality. As a second main contribution, we present an approach for automatically identifying quality flaws in Wikipedia articles. Even though the general idea of quality detection has been introduced in previous work, we dissect the approach to find that the task is inherently prone to a topic bias which results in unrealistically high cross-validated evaluation results that do not reflect the classifier’s real performance on real world data. We solve this problem with a novel data sampling approach based on the full article revision history that is able to avoid this bias. It furthermore allows us not only to identify flawed articles but also to find reliable counterexamples that do not exhibit the respective quality flaws. For automatically detecting quality flaws in unseen articles, we present FlawFinder, a modular system for supervised text classification. We evaluate the system on a novel corpus of Wikipedia articles with neutrality and style flaws. The results confirm the initial hypothesis that the reliable classifiers tend to exhibit a lower cross-validated performance than the biased ones but the scores more closely resemble their actual performance in the wild. As a third main contribution, we present an approach for automatically segmenting and tagging the user contributions on article Talk pages to improve the work coordination among Wikipedians. These unstructured discussion pages are not easy to navigate and information is likely to get lost over time in the discussion archives. By automatically identifying the quality problems that have been discussed in the past and the solutions that have been proposed, we can help users to make informed decisions in the future. Our contribution in this area is threefold: (i) We describe a novel algorithm for segmenting the unstructured dialog on Wikipedia Talk pages using their revision history. In contrast to related work, which mainly relies on the rudimentary markup, this new algorithm can reliably extract meta data, such as the identity of a user, and is moreover able to handle discontinuous turns. (ii) We introduce a novel scheme for annotating the turns in article discussions with dialog act labels for capturing the coordination efforts of article improvement. The labels reflect the types of criticism discussed in a turn, for example missing information or inappropriate language, as well as any actions proposed for solving the quality problems. (iii) Based on this scheme, we created two automatically segmented and manually annotated discussion corpora extracted from the Simple English Wikipedia (SEWD) and the English Wikipedia (EWD). We evaluate how well text classification approaches can learn to assign the dialog act labels from our scheme to unseen discussion pages and achieve a cross-validated performance of F1 = 0.82 on the SEWD corpus while we obtain an average performance of F1 = 0.78 on the larger and more complex EWD corpus.",Oliver Ferschke,2014,Thesis,N/A,190,17,https://www.semanticscholar.org/paper/363f84266063148bddf8f76bba519d960e1f6981,,
92,"Google Scholar, Web of Science",A matter of words: NLP for quality evaluation of Wikipedia medical articles,,V. Cozza; M. Petrocchi; A. Spognardi,2016,Conference,"International Conference on Web Engineering, pp. 448-456",37,8,https://www.semanticscholar.org/paper/4724b3d392a48884c7463a2ba78668c7fddd4f3c,10.1007/978-3-319-38791-8_31,https://arxiv.org/pdf/1603.01987
94,"ACM, Google Scholar, Web of Science",Indicator of quality for environmental articles on Wikipedia at the higher education level,"Wikipedia is important in higher education because students and scholars often use it. Nevertheless, the issue of Wikipedia’s quality is an obstacle for its use at the higher education level. In order to contribute to this discussion, we have proposed ‘Verifiability by respected sources’ as an indicator for assessing the quality of Wikipedia articles at the higher education level and conducted an analysis of the most frequently visited articles in the category of Environment on Wikipedia. Results show that these articles contain many unreferenced statements, so their usage at the higher education level is problematic. Therefore, we also propose specific steps for relevant actors that could help to improve the quality of Wikipedia.",Eduard Petiška; B. Moldan,2019,Journal,"Journal of Information Science, 47, pp. 269 - 280",69,3,https://www.semanticscholar.org/paper/9df731914171c0dfb635f15e4e2ae245353d15fd,10.1177/0165551519888607,
95,"ACM, Google Scholar, Web of Science",Mining team characteristics to predict Wikipedia article quality,"In this study, we were interested in studying which characteristics of virtual teams are good predictors for the quality of their production. The experiment involved obtaining the Spanish Wikipedia database dump and applying different data mining techniques suitable for large data sets to label the whole set of articles according to their quality (comparing them with the Featured/Good Articles, or FA/GA). Then we created the attributes that describe the characteristics of the team who produced the articles and using decision tree methods, we obtained the most relevant characteristics of the teams that produced FA/GA. The team's maximum efficiency and the total length of contribution are the most important predictors. This article contributes to the literature on virtual team organization.",Grace Gimon Betancourt; Armando Segnine; Carlos Trabuco; Amira Rezgui; Nicolas Jullien,2016,Conference,Proceedings of the 12th International Symposium on Open Collaboration,46,8,https://www.semanticscholar.org/paper/cdc1216d54d10a3a6f8e742941010f5d83002940,10.1145/2957792.2971802,https://hal.archives-ouvertes.fr/hal-01354368/file/mining-team-characteristics%20vf.pdf
96,"Google Scholar, Web of Science",Estimating the Quality of Articles in Russian Wikipedia Using the Logical-Linguistic Model of Fact Extraction,,N. Khairova; Włodzimierz Lewoniewski; Krzysztof Węcel,2017,Conference,"Business Information Systems, pp. 28-40",24,10,https://www.semanticscholar.org/paper/1aa2271c2a5fe2f7c69556334c4cf8b6e008c0ef,10.1007/978-3-319-59336-4_3,
97,"ACM, Google Scholar","Quality Change: Norm or Exception? Measurement, Analysis and Detection of Quality Change in Wikipedia","Wikipedia has been turned into an immensely popular crowd-sourced encyclopedia for information dissemination on numerous versatile topics in the form of subscription free content. It allows anyone to contribute so that the articles remain comprehensive and updated. For enrichment of content without compromising standards, the Wikipedia community enumerates a detailed set of guidelines, which should be followed. Based on these, articles are categorized into several quality classes by the Wikipedia editors with increasing adherence to guidelines. This quality assessment task by editors is laborious as well as demands platform expertise. As a first objective, in this paper, we study evolution of a Wikipedia article with respect to such quality scales. Our results show novel non-intuitive patterns emerging from this exploration. As a second objective we attempt to develop an automated data driven approach for the detection of the early signals influencing the quality change of articles. We posit this as a change point detection problem whereby we represent an article as a time series of consecutive revisions and encode every revision by a set of intuitive features. Finally, various change point detection algorithms are used to efficiently and accurately detect the future change points. We also perform various ablation studies to understand which group of features are most effective in identifying the change points. To the best of our knowledge, this is the first work that rigorously explores English Wikipedia article quality life cycle from the perspective of quality indicators and provides a novel unsupervised page level approach to detect quality switch, which can help in automatic content monitoring in Wikipedia thus contributing significantly to the CSCW community.",Paramita Das; Bhanu Prakash Reddy Guda; Sasi Bhusan Seelaboyina; Soumya Sarkar; Animesh Mukherjee,2021,Conference,"Proceedings of the ACM on Human-Computer Interaction, 6, pp. 1 - 36",87,0,https://www.semanticscholar.org/paper/ce03436b8284633ac9e492f1511028583f689a01,10.1145/3512959,http://arxiv.org/pdf/2111.01496
99,"ACM, Google Scholar, Web of Science",Assessing the Quality of Wikipedia Editors through Crowdsourcing,"In this paper, we propose a method for assessing the quality of Wikipedia editors. By effectively determining whether the text meaning persists over time, we can determine the actual contribution by editors. This is used in this paper to detect vandal. However, the meaning of text does not always change if a term in the text is added or removed. Therefore, we cannot capture the changes of text meaning automatically, so we cannot detect whether the meaning of text survives or not. To solve this problem, we use crowdsourcing to manually detect changes of text meaning. In our experiment, we confirmed that our proposed method improves the accuracy of detecting vandals by about 5%.",Yumiko Suzuki; Satoshi Nakamura,2016,Conference,Proceedings of the 25th International Conference Companion on World Wide Web,17,10,https://www.semanticscholar.org/paper/d7a5280305184070f9b0989de49f39f87a85c985,10.1145/2872518.2891113,
100,"Google Scholar, Web of Science",Quality flaw prediction in Spanish Wikipedia: A case of study with verifiability flaws,,E. Ferretti; L. Cagnina; Viviana Paiz; Sebastián Delle Donne; Rodrigo Zacagnini; M. Errecalde,2018,Journal,"Inf. Process. Manag., 54, pp. 1169-1181",38,8,https://www.semanticscholar.org/paper/96624b05a21c20da28291207bff2199c68074b57,10.1016/j.ipm.2018.08.003,
101,"ACM, Google Scholar, Web of Science",Structural Analysis of Wikigraph to Investigate Quality Grades of Wikipedia Articles,"The quality of Wikipedia articles is manually evaluated which is time inefficient as well as susceptible to human bias. An automated assessment of these articles may help in minimizing the overall time and manual errors. In this paper, we present a novel approach based on the structural analysis of Wikigraph to automate the estimation of the quality of Wikipedia articles. We examine the network built using the complete set of English Wikipedia articles and identify the variation of network signatures of the articles with respect to their quality. Our study shows that these signatures are useful for estimating the quality grades of un-assessed articles with an accuracy surpassing the existing approaches in this direction. The results of the study may help in reducing the need for human involvement for quality assessment tasks.",Anamika Chhabra; S. Srivastava; S. Iyengar; P. Saini,2021,Conference,Companion Proceedings of the Web Conference 2021,46,1,https://www.semanticscholar.org/paper/8ddfbf5d323e0a94160ffe10a13353793c954f30,10.1145/3442442.3452345,
102,"Google Scholar, Web of Science",Utilizing the Wikidata System to Improve the Quality of Medical Content in Wikipedia in Diverse Languages: A Pilot Study,"Background Wikipedia is an important source of medical information for both patients and medical professionals. Given its wide reach, improving the quality, completeness, and accessibility of medical information on Wikipedia could have a positive impact on global health. Objective We created a prototypical implementation of an automated system for keeping drug-drug interaction (DDI) information in Wikipedia up to date with current evidence about clinically significant drug interactions. Our work is based on Wikidata, a novel, graph-based database backend of Wikipedia currently in development. Methods We set up an automated process for integrating data from the Office of the National Coordinator for Health Information Technology (ONC) high priority DDI list into Wikidata. We set up exemplary implementations demonstrating how the DDI data we introduced into Wikidata could be displayed in Wikipedia articles in diverse languages. Finally, we conducted a pilot analysis to explore if adding the ONC high priority data would substantially enhance the information currently available on Wikipedia. Results We derived 1150 unique interactions from the ONC high priority list. Integration of the potential DDI data from Wikidata into Wikipedia articles proved to be straightforward and yielded useful results. We found that even though the majority of current English Wikipedia articles about pharmaceuticals contained sections detailing contraindications, only a small fraction of articles explicitly mentioned interaction partners from the ONC high priority list. For 91.30% (1050/1150) of the interaction pairs we tested, none of the 2 articles corresponding to the interacting substances explicitly mentioned the interaction partner. For 7.21% (83/1150) of the pairs, only 1 of the 2 associated Wikipedia articles mentioned the interaction partner; for only 1.48% (17/1150) of the pairs, both articles contained explicit mentions of the interaction partner. Conclusions Our prototype demonstrated that automated updating of medical content in Wikipedia through Wikidata is a viable option, albeit further refinements and community-wide consensus building are required before integration into public Wikipedia is possible. A long-term endeavor to improve the medical information in Wikipedia through structured data representation and automated workflows might lead to a significant improvement of the quality of medical information in one of the world’s most popular Web resources.",Alexander Pfundner; Tobias Schönberg; J. Horn; R. Boyce; M. Samwald,2015,Journal,"Journal of Medical Internet Research, 17",28,18,https://www.semanticscholar.org/paper/4f2d62bd9f0225a9358e4e520e6e5289247c3281,10.2196/jmir.4163,
103,Google Scholar,The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia,"With the increasing amount of user generated reference texts in the web, automatic quality assessment has become a key challenge. However, only a small amount of annotated data is available for training quality assessment systems. Wikipedia contains a large amount of texts annotated with cleanup templates which identify quality flaws. We show that the distribution of these labels is topically biased, since they cannot be applied freely to any arbitrary article. We argue that it is necessary to consider the topical restrictions of each label in order to avoid a sampling bias that results in a skewed classifier and overly optimistic evaluation results. We factor out the topic bias by extracting reliable training instances from the revision history which have a topic distribution similar to the labeled articles. This approach better reflects the situation a classifier would face in a real-life application.",Oliver Ferschke; Iryna Gurevych; M. Rittberger,2013,Conference,"Annual Meeting of the Association for Computational Linguistics, pp. 721-730",21,16,https://www.semanticscholar.org/paper/51bf2226cb153f0d5c7532b4b71075a7b76b069e,,
105,"Google Scholar, Web of Science",Effects of Contributor Experience on the Quality of Health-Related Wikipedia Articles,"Background Consulting the Internet for health-related information is a common and widespread phenomenon, and Wikipedia is arguably one of the most important resources for health-related information. Therefore, it is relevant to identify factors that have an impact on the quality of health-related Wikipedia articles. Objective In our study we have hypothesized a positive effect of contributor experience on the quality of health-related Wikipedia articles. Methods We mined the edit history of all (as of February 2017) 18,805 articles that were listed in the categories on the portal health & fitness in the English language version of Wikipedia. We identified tags within the articles’ edit histories, which indicated potential issues with regard to the respective article’s quality or neutrality. Of all of the sampled articles, 99 (99/18,805, 0.53%) articles had at some point received at least one such tag. In our analysis we only considered those articles with a minimum of 10 edits (10,265 articles in total; 96 tagged articles, 0.94%). Additionally, to test our hypothesis, we constructed contributor profiles, where a profile consisted of all the articles edited by a contributor and the corresponding number of edits contributed. We did not differentiate between rollbacks and edits with novel content. Results Nonparametric Mann-Whitney U-tests indicated a higher number of previously edited articles for editors of the nontagged articles (mean rank tagged 2348.23, mean rank nontagged 5159.29; U=9.25, P<.001). However, we did not find a significant difference for the contributors’ total number of edits (mean rank tagged 4872.85, mean rank nontagged 5135.48; U=0.87, P=.39). Using logistic regression analysis with the respective article’s number of edits and number of editors as covariates, only the number of edited articles yielded a significant effect on the article’s status as tagged versus nontagged (dummy-coded; Nagelkerke R2 for the full model=.17; B [SE B]=-0.001 [0.00]; Wald c2 [1]=19.70; P<.001), whereas we again found no significant effect for the mere number of edits (Nagelkerke R2 for the full model=.15; B [SE B]=0.000 [0.01]; Wald c2 [1]=0.01; P=.94). Conclusions Our findings indicate an effect of contributor experience on the quality of health-related Wikipedia articles. However, only the number of previously edited articles was a predictor of the articles’ quality but not the mere volume of edits. More research is needed to disentangle the different aspects of contributor experience. We have discussed the implications of our findings with respect to ensuring the quality of health-related information in collaborative knowledge-building platforms.",Peter Holtz; B. Fetahu; J. Kimmerle,2018,Journal,"Journal of Medical Internet Research, 20",25,10,https://www.semanticscholar.org/paper/71e8bb5240db948593ad47d03b9a703ee7b28020,10.2196/jmir.9683,https://www.jmir.org/2018/5/e171/PDF
106,"Google Scholar, Web of Science",Relative Quality Assessment of Wikipedia Articles in Different Languages Using Synthetic Measure,,Włodzimierz Lewoniewski; Krzysztof Węcel,2017,Conference,"Business Information Systems, pp. 282-292",16,5,https://www.semanticscholar.org/paper/78a15535dc2d82f2522c3eb20caa65baad1bda44,10.1007/978-3-319-69023-0_24,
107,Google Scholar,On the Use of Reliable-Negatives Selection Strategies in the PU Learning Approach for Quality Flaws Prediction in Wikipedia,"Learning from positive and unlabeled examples (PU learning) has proven to be an effective method in several Web mining applications. In particular, in the 1st International Competition on Quality Flaw Prediction in Wikipedia in 2012, a tailored PU learning approach performed best amongst the competitors. A key feature of that approach is the introduction of sampling strategies within the original PU learning procedure. The paper in hand revisits the winner approach of 2012 and elaborates on neglected aspects in order to provide evidence for the usefulness of sampling in PU learning. In this regard, we propose a modification to this PU learning approach, and we show how the different sampling strategies affect the flaw prediction effectiveness. Our analysis is based on the original evaluation corpus of the 2012-competition on quality flaw prediction. A main outcome is that under the best sampling strategy, our new modified version of PU learning increases in average the flaw prediction effectiveness by 18.31%, when compared against the winning approach of the competition.",E. Ferretti; M. Errecalde; Maik Anderka; Benno Stein,2014,Conference,"2014 25th International Workshop on Database and Expert Systems Applications, pp. 211-215",14,15,https://www.semanticscholar.org/paper/8df3a3c41c7d00b3f1b1ae7ee7894ccc69b82474,10.1109/DEXA.2014.52,
108,"ACM, Google Scholar, Web of Science",Measuring Wikipedia Article Quality in One Dimension by Extending ORES with Ordinal Regression,"Organizing complex peer production projects and advancing scientific knowledge of open collaboration each depend on the ability to measure quality. Wikipedia community members and academic researchers have used article quality ratings for purposes like tracking knowledge gaps and studying how political polarization shapes collaboration. Even so, measuring quality presents many methodological challenges. The most widely used systems use quality assesements on discrete ordinal scales, but such labels can be inconvenient for statistics and machine learning. Prior work handles this by assuming that different levels of quality are “evenly spaced” from one another. This assumption runs counter to intuitions about degrees of effort needed to raise Wikipedia articles to different quality levels. I describe a technique extending the Wikimedia Foundations’ ORES article quality model to address these limitations. My method uses weighted ordinal regression models to construct one-dimensional continuous measures of quality. While scores from my technique and from prior approaches are correlated, my approach improves accuracy for research datasets and provides evidence that the “evenly spaced” assumption is unfounded in practice on English Wikipedia. I conclude with recommendations for using quality scores in future research and include the full code, data, and models.",Nathan TeBlunthuis,2021,Conference,Proceedings of the 17th International Symposium on Open Collaboration,45,0,https://www.semanticscholar.org/paper/2c6de714ad28778eaddb2b886b7168a79cc8b4ab,10.1145/3479986.3479991,http://arxiv.org/pdf/2108.10684
109,"Google Scholar, Web of Science",Enrichment of Information in Multilingual Wikipedia Based on Quality Analysis,,Włodzimierz Lewoniewski,2017,Conference,"Business Information Systems, pp. 216-227",33,7,https://www.semanticscholar.org/paper/537cb10b82056ca9e8759c0276e6889e19cce93f,10.1007/978-3-319-69023-0_19,
111,"ACM, Google Scholar",GreenWiki: a tool to support users' assessment of the quality of Wikipedia articles,"In this work, we present GreenWiki, which is a wiki with a panel of quality indicators to assist the reader of a Wikipedia article in assessing its quality.",D. H. Dalip; R. L. Santos; Diogo Rennó Rocha de Oliveira; Valéria Freitas Amaral; Marcos André Gonçalves; R. Prates; R. Minardi; J. Almeida,2011,Conference,"ACM/IEEE Joint Conference on Digital Libraries, pp. 469-470",4,4,https://www.semanticscholar.org/paper/ccfcd884d5532e7b993495dcb44fbba1a8774c39,10.1145/1998076.1998190,
112,"Google Scholar, Web of Science",Social-collaborative determinants of content quality in online knowledge production systems: comparing Wikipedia and Stack Overflow,,S. Matei; A. A. Jabal; E. Bertino,2018,Journal,"Social Network Analysis and Mining, 8, pp. 1-16",38,8,https://www.semanticscholar.org/paper/1da7fc0b4ed71de73b519726d5f868e9c1af2659,10.1007/s13278-018-0512-3,
113,"Google Scholar, Web of Science",Improving information quality of Wikipedia articles with cooperative principle,"Purpose The purpose of this paper is to investigate the impact of cooperative principle on the information quality (IQ) by making objects more relevant for consumer needs, in particular case Wikipedia articles for students. Design/methodology/approach The authors performed a quantitative study with participants being invited to complete an online survey. Each rater evaluated three selected and re-written articles from Wikipedia by four IQ dimensions (accuracy, completeness, objectivity, and representation). Grice’s maxims and submaxims were used to re-write articles and make them more relevant for student cognitive needs. The results were analyzed with statistical methods of mean, standard deviation, Cronbach’s α, and ICC (two-way random model of single measure). Findings The study demonstrates that Wikipedia articles can be made more relevant for student needs by using cooperative principle with increase in IQ and also achieving higher consistency of students’ scores as recent research. In particular, students in the research perceived the abstract, constructed with cooperative principle, more objective and complete as reported in recent research. Practical implications The work can benefit encyclopedia editors to improve IQ of existing articles as well as consumers that would obtain more relevant information in less reading time. Originality/value This is one of the first attempts to empirically investigate the application of cooperate principle to make objects more relevant for consumer needs and impact of this on IQ. IQ improvement evidence is provided and impacts on IQ dimensions such as objectivity, completeness, accuracy, and representation for research community to validate and compare results.",M. Fidler; D. Lavbič,2017,Journal,"Online Inf. Rev., 41, pp. 797-811",72,9,https://www.semanticscholar.org/paper/e32ec265e99218e301a621486c3363a4a6ae9049,10.1108/OIR-01-2016-0003,http://arxiv.org/pdf/1807.03774
114,Google Scholar,An Empirical Study to Predict the Quality of Wikipedia Articles,,Imran Khan; Shahid Hussain; Hina Gul; Muhammad Shahid; Muhammad Jamal,2019,Conference,"WorldCIST, pp. 485-492",19,3,https://www.semanticscholar.org/paper/31e3f37070433dae787aa8228765ccac9272461f,10.1007/978-3-030-16187-3_47,
115,Google Scholar,On the Use of PU Learning for Quality Flaw Prediction in Wikipedia,"Edgardo Ferretti and Marcelo Errecalde thank Universidad Nacional de San Luis (PROICO 30310). The collaboration of UNSL, INAOE and UPV has been funded by the European Commission as part of the WIQ-EI project (project no. 269180) within the FP7 People Programme. Manuel Montes is partially supported by CONACYT, No. 134186. The work of Paolo Rosso was carried out also in the framework of the MICINN Text-Enterprise (TIN2009-13391-C04-03) research project and the Microcluster VLC/Campus (International Campus of Excellence) on Multimodal Intelligent Systems.",E. Ferretti; D. H. Fusilier; R. Guzmán-Cabrera; M. Montes-y-Gómez; M. Errecalde; Paolo Rosso,2012,Conference,"Conference and Labs of the Evaluation Forum, 1178",18,14,https://www.semanticscholar.org/paper/72d5756b021722d8c986fd3764b95c11aa8b0722,,
116,Google Scholar,"Thai Wikipedia Quality Measurement using Fuzzy Logic (人工知能学会全国大会(第26回)文化,科学技術と未来) -- (International Organized Session「Special Session on Web Intelligence & Data Mining」)",,Kanchana Saengthongpattana; N. Soonthornphisaj,2012,Journal,N/A,0,3,https://www.semanticscholar.org/paper/5c7b2bce626e9f44590fc263a94920d5727e852b,,
117,"ACM, Google Scholar",QualityRank: assessing quality of wikipedia articles by mutually evaluating editors and texts,"In this paper, we propose a method to identify high-quality Wikipedia articles by mutually evaluating editors and texts. A major approach for assessing articles using edit history is a text survival ratio based approach. However, the problem is that many high-quality articles are identified as low quality, because many vandals delete high-quality texts, then the survival ratios of high-quality texts are decreased by vandals. Our approach's strongest point is its resistance to vandalism. Using our method, if we calculate text quality values using editor quality values, vandals do not affect any quality values of the other editors, then the accuracy of text quality values should improve. However, the problem is that editor quality values are calculated by text quality values, and text quality values are calculated by editor quality values. To solve this problem, we mutually calculate editor and text quality values until they converge. Using this method, we can calculate a quality value of a text that takes into consideration that of its editors.",Yumiko Suzuki; M. Yoshikawa,2012,Conference,"ACM Conference on Hypertext & Social Media, pp. 307-308",4,7,https://www.semanticscholar.org/paper/0e24d74a6235462045441c836e504e0b8517e35f,10.1145/2309996.2310047,
118,Google Scholar,,,,,,,,,,,
119,"Google Scholar, Web of Science",Assessing Information Quality of Wikipedia Articles through Google’s E-A-T Model,"Along with the emergence of Web 2.0, User Generated Content (UGC) is becoming increasingly important for knowledge sharing. Wikipedia being the world’s largest-ever community-based collaborative encyclopedia, is also one of the biggest UGC databases in the world. Wikipedia is dealing with a significant problem of Information Quality (IQ) because of its open-source and collaborative nature. When carrying out attacks such as link spamming, malicious users take advantage of Wikipedia’s popularity on the WWW. As a result, Wikipedia is generally not recommended for academic-related work. There are, however, some articles that are both rich in information and quality. Existing approaches for assessing Wikipedia’s IQ involve statistical models and machine learning algorithms; however, the existing models do not produce satisfactory results. In this study, a novel theoretical model based on Google’s E-A-T framework is introduced to assess Wikipedia’s IQ. The model comprises three IQ constructs Expertise, Authority and Trustworthiness. Based on the empirical findings and study results, a set of IQ dimensions that influence the above three IQ constructs, as well as 45 IQ attributes to measure the IQ dimensions, were identified. The IQ attributes were automatically and inexpensively extracted from the content and meta-data statistics of Wikipedia articles using a Selenium 3.14 web automation script. A sample of 2000 articles comprising 1000 Featured Articles (FA) and 1000 non-FA articles from six WikiProjects was used for the data analysis. The proposed model was compared with three previously published models in terms of classification and clustering accuracy. It received classification and clustering accuracies of 95% and 93% respectively which is a drastic improvement over the existing models. Furthermore, an average inter-rater agreement of 84% was observed. Thus, the proposed model’s effectiveness is fairly validated by this extensive experiment. This study contributes to the related knowledge area by introducing a novel framework to assess Wikipedia articles’ IQ. The study’s limitations include the domain specificity of the chosen dataset and focusing solely on the English language. However, the results can be generalized by improving the dataset by size and replicating the study for the other domains and languages supported by Wikipedia.",Chinthani Sugandhika; S. Ahangama,2022,Journal,"IEEE Access, PP, pp. 1-1",58,1,https://www.semanticscholar.org/paper/2269aaf18fa7f0168d870e09840d7dcad1e2aa3b,10.1109/ACCESS.2022.3172962,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09770051.pdf
120,"Google Scholar, Web of Science",How to Inspect and Measure Data Quality about Scientific Publications: Use Case of Wikipedia and CRIS Databases,"The quality assurance of publication data in collaborative knowledge bases and in current research information systems (CRIS) becomes more and more relevant by the use of freely available spatial information in different application scenarios. When integrating this data into CRIS, it is necessary to be able to recognize and assess their quality. Only then is it possible to compile a result from the available data that fulfills its purpose for the user, namely to deliver reliable data and information. This paper discussed the quality problems of source metadata in Wikipedia and CRIS. Based on real data from over 40 million Wikipedia articles in various languages, we performed preliminary quality analysis of the metadata of scientific publications using a data quality tool. So far, no data quality measurements have been programmed with Python to assess the quality of metadata from scientific publications in Wikipedia and CRIS. With this in mind, we programmed the methods and algorithms as code, but presented it in the form of pseudocode in this paper to measure the quality related to objective data quality dimensions such as completeness, correctness, consistency, and timeliness. This was prepared as a macro service so that the users can use the measurement results with the program code to make a statement about their scientific publications metadata so that the management can rely on high-quality data when making decisions.",Otmane Azeroual; Włodzimierz Lewoniewski,2020,Journal,"Algorithms, 13, pp. 107",42,3,https://www.semanticscholar.org/paper/56bf3c2223d01ca19aa69e62fba08c5f065422ce,10.3390/a13050107,https://www.mdpi.com/1999-4893/13/5/107/pdf?version=1588759588
121,"ACM, Google Scholar, Web of Science",Using Hyperlink Texts to Improve Quality of Identifying Document Topics Based on Wikipedia,This paper presents a method to identify the topics of documents based on Wikipedia category network. It is to improve the method previously proposed by Schonhofen by taking into account the weights of words in hyperlink texts in Wikipedia articles. The experiments on Computing and Team Sport domains have been carried out and showed that our proposed method outperforms the Schonhofen’s one.,Dat T. Huynh; T. Cao; P. H. Pham; Toan N. Hoang,2009,Conference,"2009 International Conference on Knowledge and Systems Engineering, pp. 249-254",8,10,https://www.semanticscholar.org/paper/7adf1412ebcacfd38284dfcbc8962b1b88e6e3ac,10.1109/KSE.2009.20,
122,Google Scholar,Measuring Quality of Wikipedia Articles by Feature Fusion‐based Stack Learning,"Online open‐source knowledge repository such as Wikipedia has become an increasingly important source for users to access knowledge. However, due to its large volume, it is challenging to evaluate Wikipedia article quality manually. To fill this gap, we propose a novel approach named “feature fusion‐based stack learning” to assess the quality of Wikipedia articles. Pre‐trained language models including BERT (Bidirectional Encoder Representations from Transformers) and ELMo (Embeddings from Language Models) are applied to extract semantic information in Wikipedia content. The feature fusion framework consisting of semantic and statistical features is built and fed into an out‐of‐sample (OOS) stacking model, which includes both machine learning and deep learning models. We compare the performance of proposed model with some existing models with different metrics extensively, and conduct ablation studies to prove the effectiveness of our framework and OOS stacking. Generally, the experiment shows that our method is much better than state‐of‐the‐art models.",Jingrui Hou; Jiangnan Li; Ping Wang,2021,Conference,"Proceedings of the Association for Information Science and Technology, 58",61,1,https://www.semanticscholar.org/paper/38035f89a0531a61e4dfe6c373eb36b0a5f584a4,10.1002/pra2.449,
123,Google Scholar,An evaluation of the quality of consumer health information on Wikipedia,"Background Wikipedia is a multilingual, open-content, online encyclopedia that exists as a wiki. It is written collaboratively by people with varying degrees of expertise. Indeed, anyone who can access Wikipedia's Web site may alter any of its content. Wikipedia contains a significant number of articles on health-related topics, but the quality of this information is unknown. Purpose ofthe Study The accuracy and completeness of the information on Wikipedia is intuitively circumspect since experts and non-experts alike may contribute to the site's content. Thus, the intent of my research was to systematically evaluate the accuracy and completeness of a sample of health-related articles on Wikipedia. Methodology I selected a previously published methodology for use in my study. The advantage of this choice is that I could use the results from that study as control data for my evaluation of Wikipedia. As an additional control, I evaluated the content of the Healthwise® knowledge base (a collection of consumer health articles that is esteemed by many health professionals for its high-quality). The articles reviewed concerned the following four health topics: breast cancer, childhood asthma, depression, and obesity. Evaluation criteria were defined a priori. For each health topic, several key elements (totaling 100 across all four topics) were identified as desirable components of the content. A panel of health professionals and consumer advocates developed this list of elements. For each element, a rater scored the",Daren Nicholson,2006,Journal,N/A,7,6,https://www.semanticscholar.org/paper/6daa230a4e22031754e43685e5bd36b8395fbe61,,
125,"ACM, Google Scholar","How do metrics of link analysis correlate to quality, relevance and popularity in wikipedia?","Many links between Web pages can be viewed as indicative of the quality and importance of the pages they pointed to. Accordingly, several studies have proposed metrics based on links to infer web page content quality. However, as far as we know, the only work that has examined the correlation between such metrics and content quality consisted of a limited study that left many open questions. In spite of these metrics having been shown successful in the task of ranking pages which were provided as answers to queries submitted to search engines, it is not possible to determine the specific contribution of factors such as quality, popularity, and importance to the results. This difficulty is partially due to the fact that such information is hard to obtain for Web pages in general. Unlike ordinary Web pages, the quality, importance and popularity of Wikipedia articles are evaluated by human experts or might be easily estimated. Thus, it is feasible to verify the relation between link analysis metrics and such factors in Wikipedia articles, our goal in this work. To accomplish that, we implemented several link analysis algorithms and compared their resulting rankings with the ones created by human evaluators regarding factors such as quality, popularity and importance. We found that the metrics are more correlated to quality and popularity than to importance, and the correlation is moderate.",Raíza Hanada; Marco Cristo; M. G. Pimentel,2013,Conference,"Brazilian Symposium on Multimedia and the Web, pp. 105-112",14,10,https://www.semanticscholar.org/paper/2578b4d957fc7fbcc7a5be2dbcea5c8f8cc4c48c,10.1145/2526188.2526198,
127,"Google Scholar, Web of Science","On the Relation of Edit Behavior, Link Structure, and Article Quality on Wikipedia",,Thorsten Ruprechter; Tiago Santos; D. Helic,2019,Conference,"International Workshop on Complex Networks & Their Applications, pp. 242-254",36,2,https://www.semanticscholar.org/paper/176c68a518d81215e585f09ce7db2415c8fdde2f,10.1007/978-3-030-36683-4_20,
128,"Google Scholar, Web of Science",A Psycho-Lexical Approach to the Assessment of Information Quality on Wikipedia,"The great popularity of Wikipedia makes it one of the dominant knowledge source around the World. However, since one of the core principles of Wikipedia is being open for anyone to maintain it, Wikipedia cannot fully ensure the reliability of its articles, and thus sometimes suffered criticism for containing low-quality information. It is therefore essential to assess the quality of Wikipedia articles automatically. In this paper we describe how we approach that problem by using a psycho-lexical resource, i.e., the Language Inquiry and Word Count (LIWC) dictionary. By training a classifier on different LIWC categories, we discuss the implications of each category for Wikipedia quality assessment.",Qi Su; Pengyuan Liu,2015,Conference,"2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT), 3, pp. 184-187",18,5,https://www.semanticscholar.org/paper/f5a926ec49dc153326a0aca080377dc0be12e75d,10.1109/WI-IAT.2015.23,
129,Google Scholar,On the Evolution of Quality Flaws and the Effectiveness of Cleanup Tags in the English Wikipedia,"The improvement of information quality is a major task for the free online encyclopedia Wikipedia. Recent studies targeted the analysis and detection of specific quality flaws in Wikipedia articles. To date, quality flaws have been exclusively investigated in current Wikipedia articles, based on a snapshot representing the state of Wikipedia at a certain time. This paper goes further, and provides the first comprehensive breakdown of the evolution of quality flaws in Wikipedia. We utilize cleanup tags to analyze the quality flaws that have been tagged by the Wikipedia community in the English Wikipedia, from its launch in 2001 until 2011. This leads to interesting findings regarding (1) the development of Wikipedia’s quality flaw structure and (2) the usage and the effectiveness of cleanup tags. Specifically, we show that inline tags are more effective than tag boxes, and provide statistics about the considerable volume of rare and non-specific cleanup tags. We expect that this work will support the Wikipedia community in making quality assurance activities more efficient.",Maik Anderka; Benno Stein; M. Busse,2012,Journal,N/A,22,9,https://www.semanticscholar.org/paper/b6aa3e619142d395c01857ec6e7b3b14da8fd0f7,,
130,Google Scholar,,,,,,,,,,,
131,Google Scholar,Assessing the Quality of Wikipedia Pages Using Edit Longevity and Contributor Centrality,"In this paper we address the challenge of assessing the quality of Wikipedia pages using scores derived from edit contribution and con- tributor authoritativeness measures. The hypothesis is that pages with significant contributions from authoritative contributors are likely to be high-quality pages. Contributions are quantified using edit longevity measures and contributor authoritativeness is scored using centrality metrics in either the Wikipedia talk or co-author networks. The results suggest that it is useful to take into account the contributor authori- tativeness when assessing the information quality of Wikipedia content. The percentile visualization of the quality scores provides some insights about the anomalous articles, and can be used to help Wikipedia editors to identify Start and Stub articles that are of relatively good quality.",Xiangju Qin; P. Cunningham,2012,Journal,"ArXiv, abs/1206.2517",20,5,https://www.semanticscholar.org/paper/f621c58c765a3d2a6501a9fbce21681b4d2beb28,,
132,"Google Scholar, Web of Science",Towards Information Quality Assurance in Spanish: Wikipedia,"Featured Articles (FA) are considered to be the best articles that Wikipedia has to offer and in the last years, researchers have found interesting to analyze whether and how they can be distinguished from “ordinary” articles. Likewise, identifying what issues have to be enhanced or fixed in ordinary articles in order to improve their quality is a recent key research trend. Most of the approaches developed to face these information quality problems have been proposed for the English Wikipedia. However, few efforts have been accomplished in Spanish Wikipedia, despite being Spanish, one of the most spoken languages in the world by native speakers. In this respect, we present a breakdown of Spanish Wikipedia’s quality flaw structure. Besides, we carry out studies with three different corpora to automatically assess information quality in Spanish Wikipedia, where FA identification is evaluated as a binary classification task. Our evaluation on a unified setting allows to compare with the English version, the performance achieved by our approach on the Spanish version. The best results obtained show that FA identification in Spanish, can be performed with an F1 score of 0.88 using a document model consisting of only twenty six features and Support Vector Machine as classification algorithm.",E. Ferretti; M. Soria; Sebastián Pérez Casseignau; Lian Pohn; Guido Urquiza; Sergio Alejandro Gómez; M. Errecalde,2017,Journal,"Journal of Computer Science and Technology, 17, pp. 29-36",17,4,https://www.semanticscholar.org/paper/8cba1878de84959de7a5401c9181819ee9bdf205,,
133,Google Scholar,Effects of Implicit Positive Ratings for Quality Assessment of Wikipedia Articles,"In this paper, we propose a method to identify high-quality Wikipedia articles by using implicit positive ratings. One of the major approaches for assessing Wikipedia articles is a text survival ratio based approach. In this approach, when a text survives beyond multiple edits, the text is assessed as high quality. However, the problem is that many low quality articles are misjudged as high quality, because every editor does not always read the whole article. If there is a low quality text at the bottom of a long article, and the text has not seen by the other editors, then the text survives beyond many edits, and the text is assessed as high quality. To solve this problem, we use a section and a paragraph as a unit instead of a whole page. In our method, if an editor edits an article, the system considers that the editor gives positive ratings to the section or the paragraph that the editor edits. From experimental evaluation, we confirmed that the proposed method could improve the accuracy of quality values for articles.",Yumiko Suzuki,2013,Journal,"J. Inf. Process., 21, pp. 342-348",22,3,https://www.semanticscholar.org/paper/ba661edb080125d879bbfe004ae8dc4f1c6b7499,10.2197/IPSJJIP.21.342,https://www.jstage.jst.go.jp/article/ipsjjip/21/2/21_342/_pdf
134,"Google Scholar, Web of Science",Learning with Wikipedia in Higher Education: Academic Performance and Students' Quality Perception,,Antoni Meseguer-Artola; Inma Rodríguez-Ardura; Gisela Ammetller; E. Rimbau-Gilabert,2019,Conference,"Research & Innovation Forum, pp. 117-124",26,1,https://www.semanticscholar.org/paper/a1f353d97743fce8b8ca5ace3b80f1bc9f820f96,10.1007/978-3-030-30809-4_12,
135,Google Scholar,Self-Regulation: How Wikipedia Leverages User-Generated Quality Control Under Section 230,"As Virginia Woolf once wrote, “[T]o enjoy freedom, we have…to control ourselves.” In the market for online information services, Wikipedia has done just that. Wikipedia has achieved astounding success via self-regulation. Wikipedia promotes user-generated quality control not as a legal obligation, but as a commitment to its educational purpose and values of its fact-checking community. In doing so, Wikipedia has leveraged the purpose of Section 230 of the Communications Decency Act into consumer welfare. Section 230 protects sites that engage in ""Good Samaritan"" policing of harmful material, with no requirement on the quality or quantity of such monitoring. Interactive sites should treat the statute an opportunity, rather than mere permission to thrive in the world of Web 2.0: those who can productively self-regulate, should.",Kathleen M. Walsh; Sarah Oh,2010,Unknown,Economics of Networks eJournal,2,4,https://www.semanticscholar.org/paper/f22da6c8f5aea5e111eee39956fea749eae91785,,
136,"ACM, Google Scholar",Assessing the Quality of Wikipedia Articles,"Wikipedia is a very important information reference source for the Internet users. Due to the fact that the content of Wikipedia is the collaborative result from a massive number of participants all over the world, the quality of Wikipedia might be questionable. Over the last decade, many research works are dedicated to solve the issue of Wikipedia quality. In this paper, we present our latest research in determining the quality of Wikipedia articles. The evaluation on the real-world dataset shows that our method outperforms other baseline methods proposed recently.",Quang-Vinh Dang,2021,Conference,2021 The 5th International Conference on Machine Learning and Soft Computing,47,0,https://www.semanticscholar.org/paper/f47fe9e54ef4444ea46e910d9429245bea9d0dfd,10.1145/3453800.3453801,
137,"Google Scholar, Web of Science",Letter to the Editor: Quality of mental health information on Wikipedia,"We read with interest the analysis of the quality of mental health information available online by Reavley et al. (2011) and, as both mental health researchers and Wikipedia editors who have contributed to the articles included in the study, we were encouraged to see this important online resource discussed in Psychological Medicine. As the authors point out, Google searches often yield Wikipedia articles in the first few results ; the ‘schizophrenia ’ article (http://en.wikipedia.org/ wiki/Schizophrenia) had been viewed 369 372 times in the 30 days prior to 20 December 2011 (http://stats. grok.se/). Across all domains except for readability, Wikipedia outperformed Encyclopaedia Britannica, a psychiatry textbook and a number of static information websites from professional bodies. The finding is notable because it contradicts the common stereotype of Wikipedia as being written by lay people, and therefore inaccurate. However, we believe further research is warranted to explore who is editing these articles. While serving as academic researchers, we have made thousands of edits to articles on schizophrenia, neuropsychology, neurodegenerative diseases, conversion disorder and mood-rating scales. This has been done in our spare time out of enjoyment, but we suspect there are many other ‘ lurkers ’ who edit articles using anonymous Internet protocol (IP) addresses to make improvements. It is also worth noting that these are ‘ featured articles ’ which represent the best of Wikipedia, and that not all articles are of this standard. Reavley et al. (2011) suggest professional associations could create Wikipedia task forces and even include statements of approval on articles. Because the ethos of Wikipedia states that a layperson has just as much right to make an edit as a committee of experts, we believe this is probably not the right path to take. We would propose instead that professional bodies support members as individual contributors, rather than ‘ task force members’, to write and maintain informed, current, readable articles in their areas of expertise. This would mean a wider range of professionals could be involved and any perceived conflicts of interest in article content would be avoided. There are three pillars to support such an approach: leadership, education, and incentive. Leadership from the professional bodies of mental health (andmedicine broadly) should recognize that Wikipedia is a go-to source of information that rivals any resource in human history for patients, and that letting poor-quality articles go unimproved is harmful. Education should be provided to train professionals to make appropriate and useful contributions within the context of Wikipedia as a community, rather than just their own academic field. Finally, there must be incentive to motivate experts to contribute ; editing articles should be seen as a positive contribution by promotion boards. Asking experts to donate their time to peerreview and improve the work of others is not new, and peer-reviewing for journals is already something that receives both formal and informal support. There may be an opportunity to improve understanding, reduce stigma and educate patients on a scale not seen before, and although it is a strange new world, we believe it is one with which we all must engage. If readers are interested in participating, searching for ‘WikiProject Medicine ’ (http://en. wikipedia.org/wiki/Wikipedia:WikiProject_Medicine) would be a good place to start.",P. Wicks; V. Bell,2012,Journal,"Psychological Medicine, 42, pp. 891 - 891",1,6,https://www.semanticscholar.org/paper/db0b7c773018080e0401d15272100a2a57272dd5,10.1017/S0033291712000086,https://www.cambridge.org/core/services/aop-cambridge-core/content/view/4C7455808DA13EB100F8C68F6B55FC04/S0033291712000086a.pdf/div-class-title-letter-to-the-editor-quality-of-mental-health-information-on-wikipedia-div.pdf
139,"Google Scholar, Web of Science",Using Morphological and Semantic Features for the Quality Assessment of Russian Wikipedia,,Włodzimierz Lewoniewski; N. Khairova; Krzysztof Węcel; Nataliia Stratiienko; W. Abramowicz,2017,Conference,"International Conference on Information and Software Technologies, pp. 550-560",19,3,https://www.semanticscholar.org/paper/9b06d30b1eafc37b2b90c1f3562d879f7fa2a8aa,10.1007/978-3-319-67642-5_46,
140,"Google Scholar, Web of Science",The Adoption of Wikipedia: A Community- and Information Quality-Based View,"The Web 2.0 model has aroused vast attention as it alters the traditional role of Internet users as pure information receivers. Wikipedia, as one of the most successful case of the Web 2.0 model, creates an online encyclopedia through the collective efforts of volunteers. Shared freely by all Internet users, it forms an online community platform on which users can seek and share knowledge. This study investigates the factors that affect the adoption of Wikipedia. Based on Davis' (1989) TAM, perceived critical mass, community identification, and information quality were incorporated into the research model for explaining the intentions and usage behavior of Wikipedia users. This research is a work-in-progress and a questionnaire survey will be executed, targeting at Internet users who had prior experiences with Wikipedia. The survey is expected to be conducted over the Internet in March, 2008.",Kai Wang; Chien-Liang Lin; Chun-der Chen; Shu-Chen Yang,2008,Conference,"Pacific Asia Conference on Information Systems, pp. 50",56,6,https://www.semanticscholar.org/paper/ea0c5b9d37cb168cc418102e7a934fe7ee2d0e77,,
141,Google Scholar,Cross-lingual Data Quality for Knowledge Base Acceleration Across Wikipedia Editions,"Knowledge-sharing communities like Wikipedia and knowledge bases like Freebase are expected to capture the latest facts about the real world. However, neither of these can keep pace with the rate at which events happen and new knowledge is reported in news and social media. To narrow this gap, we propose an approach to accelerate the online maintenance of knowledge bases. Our method, called LAIKA, is based on link prediction. Wikipedia editions in dierent languages, Wikinews, and other news media come with extensive but noisy interlinkage at the entity level. We utilize this input for recommending, for a given Wikipedia article or knowledge-base entry, new categories, related entities, and cross-lingual interwiki links. LAIKA constructs a large graph from the available input and uses link-overlap measures and random-walk techniques to generate missing links and rank them for recommendations. Experiments with a very large graph from multilingual Wikipedia editions demonstrate the accuracy of our link predictions.",Julianna Göbölös-Szabó; N. Prytkova; M. Spaniol; G. Weikum,2012,Conference,"International Workshop on Quality in Databases, , pp. 1-7",21,7,https://www.semanticscholar.org/paper/e4dda236a443a596dd710ba6dddaba59648caf22,,
142,"ACM, Google Scholar, Web of Science",Quality Assessment of Peer-Produced Content in Knowledge Repositories Using Big Data and Social Networks: The Case of Implicit Collaboration in Wikipedia,"This research provides a method for quality assessment of peer-produced content in knowledge repositories using a complementary view of collaboration. Using the definition of collaboration as the action of working with someone to produce something, we identify the aspects of collaboration that the present research on online communities does not consider. To this end, we introduce and define the concept of implicit collaboration and then identify two dimensions and four possible areas of collaboration. In each area, we identify the relevant social network that captures collaboration. Using customized measures on each of the networks that capture various aspects of collaboration, we quantify the utility of implicit collaboration in assessing article quality. Experiments conducted on the complete population of graded English language Wikipedia articles show that all the identified measures improve the predictive accuracy of the existing models by 11.89 percent while improving the class-wise precision by 9-18 percent and the class-wise recall by 5-26 percent. We also find that our method complements the existing quality assessment approaches well. Our research has implications for developing automated quality assessment methods for peer-produced content using big data and social networks.",Srikar Velichety,2019,Journal,"Data Base, 50, pp. 28-51",79,1,https://www.semanticscholar.org/paper/3739f4702b05af3586dd9ad8d5d37cd924c8c110,10.1145/3371041.3371045,
143,Google Scholar,Knowledge Quality of Collaborative Editing in Wikipedia: an Integrative Perspective of Social Capital and Team Conflict,"Collaborative editing has become one of the most popular forms of knowledge contribution in virtual communities. Wikipedia— the largest online encyclopaedia— is a representative example of collaborative work. Despite the abundant researches on Wikipedia, to the best of our knowledge, no one has considered the integration of social capital and conflict. Besides, extant literatures on knowledge quality just pay attention to task conflict, while relational conflict is rarely mentioned. Meanwhile, our study proposes the nonlinear relationship between task conflict and knowledge quality instead of linear relationships in prior studies. We also postulate the moderating effect of task complexity. Furthermore, there is little empirical research on the influence of social capital on conflict, especially the distinct effects of cognitive and relational capital. This paper aims at proposing a theoretical model to examine the effect of social capital and conflict, meanwhile taking the task complexity into account. We will make our efforts to verify our research model in the following phases, and we believe that the present work can make some contributions to both research and practice.",Liuhan Zhan; Nan Wang; Xiao-Liang Shen; Yongqiang Sun,2015,Conference,"Pacific Asia Conference on Information Systems, pp. 171",27,2,https://www.semanticscholar.org/paper/6ca33772623202bcbcd57f0c996e31518078ff43,,
144,"Google Scholar, Web of Science",Content and Quality of Information about Stroke in Wikipedia across Multiple Languages,"Background: Given the high contribution of stroke to the global burden of disease, there is a need for good-quality information on Web platforms such as Wikipedia. Aims: This study aimed to describe the quality of the Wikipedia articles on stroke written in different languages. Methods: We studied the world’s 30 most spoken languages. With the DISCERN score, we evaluated the quality of the information within the Wikipedia articles. Three investigators assessed each of the texts translated to English. We also registered the word count, the number of references, and if the text referred to the emergency status of stroke, cues to suspect a stroke, and allusions to endovascular treatment. Results: There is a Wikipedia article for stroke in 23 out of the 30 languages. The mean DISCERN score was 35 29.9 ± 9.2. Overall quality ranged from 3/5 in 26.1% to 1/5 in 17.4%. Word count had a mean of 36 3,145.8 ± 3,048.9 words, and the texts included a mean of 43.1 ± 57.3 references; 69.6% of the articles referred to stroke as a medical emergency, 52.2% included awareness symptoms, and 34.8% included endovascular management among the stroke treatments. Three pages included steroids as part of the stroke treatment. The DISCERN score was not correlated with the number of speakers, but it was positively correlated with the number of references (r = 0.90, p < 0.001) and the number of words (r = 0.78, p < 0.001) in the articles. Conclusion: The analyzed Wikipedia articles do not contain relevant and up-to-date information to the general population. Further, the content varies widely across the different languages and is missing for some of them. The missing versions disproportionally affect millions of potential information seekers in undeveloped countries.",J. Marquez-Romero; Ángel Lee; E. Soto-Cabrera; B. Hernández-Curiel; C. Prado-Aguilar,2022,Journal,"European Neurology, 85, pp. 308 - 312",11,0,https://www.semanticscholar.org/paper/796882706963fe29d6febcd63d5f6b6166e76813,10.1159/000521938,
145,"ACM, Google Scholar, Web of Science",Equal opportunities in the access to quality online health information? A multi-lingual study on Wikipedia,"Wikipedia is a free, multilingual, and collaborative online encyclopedia. Nowadays, it is one of the largest sources of online knowledge, often appearing at the top of the results of the major search engines, being one of the most sought-after resources by the public searching for health information. The collaborative nature of Wikipedia raises security concerns since this information is used for decision-making, especially in the health area. Despite being available in hundreds of idioms, there are asymmetries between idioms, namely regarding their quality. In this work, we compare the quality of health information on Wikipedia between idioms with 100 million native speakers or more, and also in Greek, Italian, Korean, Turkish, Persian, Catalan and Hebrew, for historical tradition. Quality metrics are applied to health and medical articles in English, maintained by WikiProject Medicine, and their versions in the above idioms. With this, we contribute to a clarification of the role of Wikipedia in the access to health information. We demonstrate differences in both the quantity and quality of information available between idioms. English is the idiom with the highest quality in general. Urdu, Greek, Indonesian, and Hindi achieved lower values of quality.",Luis Couto; C. Lopes,2021,Conference,Proceedings of the 17th International Symposium on Open Collaboration,38,1,https://www.semanticscholar.org/paper/a5b7aa53662a843c411555aae6508e24e9adcc1c,10.1145/3479986.3480000,https://dl.acm.org/doi/pdf/10.1145/3479986.3480000
146,Google Scholar,Effects of Stigmergic and Explicit Coordination on Wikipedia Article Quality,"Prior research on Wikipedia has noted the importance of both explicit coordination of edits (i.e., through the article Talk page) and stigmergic coordination (i.e., through the article itself). Using a panel data set of article quality and edits for 23 articles over time, we examine the impact of different kinds of edits on article quality. We find that stigmergically-coordinated edits seem to have the biggest effect on quality, but that explicit coordination of major edits also predicts article quality. The findings have implications for both research on coordination in Wikipedia and for supporting editors.",Kevin Crowston; Amira Rezgui,2020,Conference,"Hawaii International Conference on System Sciences, pp. 1-10",28,3,https://www.semanticscholar.org/paper/15ef01000cda62428cdfc3d27cfe21738bd33f2b,10.24251/hicss.2020.287,https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1295&context=hicss-53
147,Google Scholar,Predicting Information Quality Flaws in Wikipedia by Using Classical and Deep Learning Approaches,,Gerónimo Bazán Pereyra; C. Cuello; G. Capodici; Vanessa Jofré; E. Ferretti; Rodolfo Bonnin; M. Errecalde,2019,Conference,"Argentine Congress of Computer Science, pp. 3-18",31,2,https://www.semanticscholar.org/paper/dbf3799d79675e9d4cd8365511fa42556e4967a3,10.1007/978-3-030-48325-8_1,
148,"Google Scholar, Web of Science",Quality assessment of Arabic web content: The case of the Arabic Wikipedia,"With the huge size and large diversity of Arabic web content, machine assessment of document quality acquires added importance. Users are in dire need for quality rating of the material returned in response to their queries. The Wikipedia, with its large metadata, has been a topic of extensive research on document quality assessment. Criteria used include text properties and style parameters, contributor and edit characteristics and multimedia components. In this paper we report on our ongoing work to adapt existing document assessment approaches to Arabic content with concentration on the Arabic Wikipedia and present some of the results. We also try to augment that with features specific to Arabic as well as parameters like author expertise and social media presence. One of our goals is an aggregate measure integrating many of the features into a single document quality index. We plan to use Wikipedia article quality assessment results to train general content assessment methods that can be applied to general content that lacks major Wikipedia features.",A. Yahya; A. Salhi,2014,Conference,"2014 10th International Conference on Innovations in Information Technology (IIT), pp. 36-41",16,3,https://www.semanticscholar.org/paper/b7821e99b90522f4b8c68f91cbf230fcc6447d13,10.1109/INNOVATIONS.2014.6987558,
149,"Google Scholar, Web of Science",Determining Quality of Articles in Polish Wikipedia Based on Linguistic Features,,Włodzimierz Lewoniewski; Krzysztof Węcel; W. Abramowicz,2018,Conference,"International Conference on Information and Software Technologies, pp. 546-558",25,2,https://www.semanticscholar.org/paper/2d12a9dae283284e3eaf6e2bd4057e5d335e8389,10.1007/978-3-319-99972-2_45,https://www.preprints.org/manuscript/201801.0017/v1/download
150,"Google Scholar, Web of Science",Quality Classification of ASEAN Wikipedia Articles using Statistical Features,"The quality of Wikipedia articles is still the main concerned in all languages. Wikipedia relies mostly on human editors and administrators to provide the quality of content. But the magnitude of Wikipedia content makes locating all instances of article very time consuming. Therefore, we need the automatic quality detection that can help users to evaluate the quality of articles. In this paper, we propose the feature set to applied for the ASEAN language Wikipedia articles. We investigate the statistical features such as # of link, # of infobox, length of article, # of headings, # of files, # of contributors, # of viewer, # of written articles found in other languages, and # of templates applied in the article. The experiments are perform using Naïve Bayes and Decision tree algorithm. We found that the accuracy of Decision tree (96.34%) outperform Naïve Bayes (86.47%). Moreover, we found that the statistical features play an important role in quality classification of Vietnamese, Indonesian, Malaysian, Thai, and Tagalog/Philippines Wikipedia articles.",Kanchana Saengthongpattana; T. Supnithi; N. Soonthornphisaj,2018,Conference,"2018 International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP), pp. 1-6",11,1,https://www.semanticscholar.org/paper/f47f2ea88b5412b6a44a264aec67b998a6e3e9ce,10.1109/ISAI-NLP.2018.8692954,
151,Google Scholar,Feature Analysis for Assessing the Quality of Wikipedia Articles through Supervised Classification,"Nowadays, thanks to Web 2.0 technologies, people have the possibility to generate and spread contents on different social media in a very easy way. In this context, the evaluation of the quality of the information that is available online is becoming more and more a crucial issue. In fact, a constant flow of contents is generated every day by often unknown sources, which are not certified by traditional authoritative entities. This requires the development of appropriate methodologies that can evaluate in a systematic way these contents, based on `objective' aspects connected with them. This would help individuals, who nowadays tend to increasingly form their opinions based on what they read online and on social media, to come into contact with information that is actually useful and verified. Wikipedia is nowadays one of the biggest online resources on which users rely as a source of information. The amount of collaboratively generated content that is sent to the online encyclopedia every day can let to the possible creation of low-quality articles (and, consequently, misinformation) if not properly monitored and revised. For this reason, in this paper, the problem of automatically assessing the quality of Wikipedia articles is considered. In particular, the focus is on the analysis of hand-crafted features that can be employed by supervised machine learning techniques to perform the classification of Wikipedia articles on qualitative bases. With respect to prior literature, a wider set of characteristics connected to Wikipedia articles are taken into account and illustrated in detail. Evaluations are performed by considering a labeled dataset provided in a prior work, and different supervised machine learning algorithms, which produced encouraging results with respect to the considered features.",Elias Bassani; Marco Viviani,2018,Journal,"ArXiv, abs/1812.02655",38,1,https://www.semanticscholar.org/paper/dcc5a5d1c7bc3fc9de1080bdf0be71ac2d833385,,
152,"ACM, Google Scholar",Good Quality Complementary Information for Multilingual Wikipedia,,Yumiko Suzuki; Yuya Fujiwara; Y. Konishi; Akiyo Nadamoto,2012,Conference,"WISE, pp. 185-198",15,1,https://www.semanticscholar.org/paper/632e144609040754fb03476054db3e5c52807764,10.1007/978-3-642-35063-4_14,
154,Google Scholar,Use of linguistic criteria for estimating of wikipedia articles quality,"As far as the question of texts and articles quality is urgent today, in process of a research, the concept of quality for Wikipedia articles was analysed. There were marked out linguistic criteria of quality for technical documentation and scientific articles. Nowadays everyone knows about such informational resource as Wikipedia. Since that day when Wikipedia was just an offshoot of Nupedia (project to produce a free encyclopedia), it has become the most well-known and popular internet encyclopedia with 282 active language editions such as German, French, Russian and Polish and of course the biggest one is English edition, that has more than 5 million articles. It is multilingual, web-based, free content encyclopedia project. It takes the 5 place according to the list of the most popular websites [6]. Wikipedia is written collaboratively by largely anonymous volunteers who write without pay. Anyone, with Internet access, can write and make changes to Wikipedia articles, except in limited cases where editing is restricted to prevent disruption or vandalism. Users can contribute anonymously, under a pseudonym, or, if they choose to, with their real identity. Some users visit Wikipedia to share their knowledge, others to get (acquire) [6]. Every day, hundreds of thousands of visitors from the various parts of the world collectively make tens of thousands of edits and create thousands of new articles to augment the knowledge held by the Wikipedia encyclopedia. All users, old or young, with different backgrounds and people of all cultures can make changes in articles or add their own one. Wikipedia's greatest strengths, weaknesses, and differences all arise because it is open to anyone, it has a large contributor base, and its articles are written according to editorial guidelines and policies. According to the Nature (the first to use peer review that compares Wikipedia and Britannica‘s coverage of science), Wikipedia‘s strongest suit is the speed at which it can updated, a factor not considered by Nature‘s reviewers. Of course it has large amount of uncovered flaws, different kinds of factual errors, omissions or misleading statements. Quality issues, however, concern the creators of Wikipedia. That‘s why, in 2006 during the Opening plenary at Wikimania, Jimmy Wales suggested to concentrate on quality of the articles instead of their number [2]. They created assessment system WP: ASSESS. It uses a letter scheme which estimates how complete the article is, assigning to the definite article its grade. According to this system, Wikipedia has 9 grades: FA (Featured Article) [4], A, GA (Good Article), B, C, Start, Stub, FL (Featured List), List. Each of these grades has special criteria. Featured articles are",A. Kolesnik; N. Khairova,2017,Journal,N/A,2,3,https://www.semanticscholar.org/paper/6faf93fd7284ef2ae6a6c0e6937b938c95223350,,
155,"Google Scholar, Web of Science",Cumulative Experience and Recent Behavior and their Relation to Content Quality on Wikipedia,"Cumulative experience is often seen as a major factor for influencing content quality in collaborative projects such as Wikipedia. However, past studies often utilize cumulative experience based on the quantity of work rather than quality and context. Moreover, the perspective on cumulative experience assumes a final destination for user behavior, whereas much of the literature indicates that user behavior changes over time. This paper aims to address these two factors by providing better descriptions and context to determine their effect on content quality. The study rematerialized these factors based on 1 Communications Facility 495, 516 High Street, Bellingham, WA 98225. U.S.A. Telephone: 360-6502401, Email: michael.tsikerdekis@wwu.edu",Michail Tsikerdekis,2017,Journal,"Interact. Comput., 29, pp. 737-754",69,1,https://www.semanticscholar.org/paper/1962d8ff1a78b207088110cda37fb99461e56b73,10.1093/iwc/iwx010,
156,Google Scholar,Quality of monkeypox information in Wikipedia across multiple languages,,E. Ornos; Jared Gabriel L Dela Rosa; J. G. Solidum; Jervy Garcia; Erika P. Ong; R. Valenzuela; O. Tantengco,2022,Journal,"Asian Pacific Journal of Tropical Medicine, 15, pp. 571 - 572",7,0,https://www.semanticscholar.org/paper/f9b483de972a99113231ac512f4fc04081f3ac22,10.4103/1995-7645.361856,
157,"Google Scholar, Web of Science",On Quality Assesement in Wikipedia Articles Based on Markov Random Fields,,Rajmund Kleminski; Tomasz Kajdanowicz; Roman Bartusiak; Przemyslaw Kazienko,2017,Conference,"Asian Conference on Intelligent Information and Database Systems, pp. 782-791",13,1,https://www.semanticscholar.org/paper/8bc3af08dc9d06747a6ca83c941a6e2cc03b29fc,10.1007/978-3-319-54472-4_73,
158,Google Scholar,Quality of Articles in Wikipedia,"The recent research of wikipediais is firs briefly analyzed,especially on the statistics of quality of articles in Wikipedia.Then the automatic evaluating methods of article quality are discussed.The methods mainly include two kinds: the correlation-based analysis and cooperation modeling.Furthermore,we present the open problems of automatic quality evaluation and the possiblepromotions of collective intelligence.",Wu Juebo; Pla Military,2011,Journal,"Geomatics and Information Science of Wuhan University, ",0,1,https://www.semanticscholar.org/paper/4afc868db9e2f8b71e1476e02cd786c42767b560,,
159,Google Scholar,Assessing Quality Values of Wikipedia Articles Using Implicit Positive and Negative Ratings,,Yumiko Suzuki,2012,Conference,"Interational Conference on Web-Age Information Management, pp. 127-138",10,1,https://www.semanticscholar.org/paper/7e0f730ebf1ef8d1fdcfac69cf66eef0f44c4813,10.1007/978-3-642-32281-5_13,
160,Google Scholar,On the Assessment of Information Quality in Spanish Wikipedia,"Featured Articles (FA) are considered to be the best articles that Wikipedia has to offer and in the last years, researchers have found interesting to analyze whether and how they can be distinguished from “ordinary” articles. Likewise, identifying what issues have to be enhanced or fixed in ordinary articles in order to improve their quality is a recent key research trend. Most of the approaches developed in these research trends have been proposed for the English Wikipedia. However, few efforts have been accomplished in Spanish Wikipedia, despite being Spanish, one of the most spoken languages in the world by native speakers. In this respect, we present a first breakdown of Spanish Wikipedia’s quality flaw structure. Besides, we carry out a study to automatically assess information quality in Spanish Wikipedia, where FA identification is evaluated as a binary classification task. The results obtained show that FA identification can be performed with an F1 score of 0.81, using a document model consisting of only twenty six features and AdaBoosted C4.5 decision trees as classification algorithm.",Guido Urquiza; M. Soria; Sebastián Pérez Casseignau; E. Ferretti; Sergio Alejandro Gómez; M. Errecalde,2016,Journal,N/A,20,3,https://www.semanticscholar.org/paper/cc4ec9eee2bea21b2ec974d0dc3e47be58f9b3d0,,
161,"ACM, Google Scholar, Web of Science",Assessing the quality of health-related Wikipedia articles with generic and specific metrics,"Wikipedia is an online, free, multi-language, and collaborative encyclopedia, currently one of the most significant information sources on the web. The open nature of Wikipedia contributions raises concerns about the quality of its information. Previous studies have addressed this issue using manual evaluations and proposing generic measures for quality assessment. In this work, we focus on the quality of health-related content. For this purpose, we use general and health-specific features from Wikipedia articles to propose health-specific metrics. We evaluate these metrics using a set of Wikipedia articles previously assessed by WikiProject Medicine. We conclude that it is possible to combine generic and specific metrics to determine health-related content’s information quality. These metrics are computed automatically and can be used by curators to identify quality issues. Along with the explored features, these metrics can also be used in approaches that automatically classify the quality of Wikipedia health-related articles.",Luis Couto; C. Lopes,2021,Conference,Companion Proceedings of the Web Conference 2021,30,1,https://www.semanticscholar.org/paper/9a1c237dd3692545d024968b8831c5c00fb29f79,10.1145/3442442.3452355,https://dl.acm.org/doi/pdf/10.1145/3442442.3452355
162,Google Scholar,Ontology-Based Classifiers for Wikipedia Article Quality Classification,,Kanchana Saengthongpattana; T. Supnithi; N. Soonthornphisaj,2017,Journal,Advances in Intelligent Systems and Computing,18,1,https://www.semanticscholar.org/paper/261b08ea302a506eb47bee6a548e39b7e96db899,10.1007/978-3-319-94703-7_7,
163,Google Scholar,Ranking Wikipedia article's data quality by learning dimension distributions,"As the largest free user-generated knowledge repository, data quality of Wikipedia has attracted great attention these years. Automatic assessment of Wikipedia article’s data quality is a pressing concern. We observe that every Wikipedia quality class exhibits its specific characteristic along different first-class quality dimensions including accuracy, completeness, consistency and minimality. We propose to extract quality dimension values from article’s content and editing history using dynamic Bayesian network (DBN) and information extraction techniques. Next, we employ multivariate Gaussian distributions to model quality dimension distributions for each quality class, and combine multiple trained classifiers to predict an article’s quality class, which can distinguish different quality classes effectively and robustly. Experiments demonstrate that our approach generates a good performance.",Jingyu Han; Ke-Jia Chen,2014,Journal,"Int. J. Inf. Qual., 3, pp. 207-227",16,1,https://www.semanticscholar.org/paper/7e32e541b6bca921823093a51e273d3adc0294d0,10.1504/IJIQ.2014.064056,
164,Google Scholar,"Accuracy and quality in historical representation: Wikipedia, textbooks and the Investiture Controversy","Wikipedia’s popularity is unquestioned, but a perceived lack of accuracy and reliability in articles on historical topics prevents historians from embracing it more fully. This article argues that accuracy may be only one component of overall quality. While Wikipedia may have demonstrable shortcomings, it also has strengths in areas such as completeness and accessibility. These strengths appear when historical narratives in Wikipedia are compared to other sources of historical information readily available to American undergraduates. The article compares Wikipedia’s entry on the Investiture Controversy to current scholarship and textbook treatments of the theme. On a broader view of quality, Wikipedia appears in a more favorable light than it does when we employ a narrow focus on accuracy about specific dates and events.",David Halsted,2013,Journal,N/A,29,3,https://www.semanticscholar.org/paper/404b03abab9f704831922a20533c8d3354646592,10.16995/DM.50,
165,Google Scholar,Automatically Assessing the Need of Additional Citations for Information Quality Verification in Wikipedia Articles,,Gerónimo Bazán Pereyra; C. Cuello; G. Capodici; Vanessa Jofré; E. Ferretti; M. Errecalde,2019,Journal,N/A,0,1,https://www.semanticscholar.org/paper/8c0b0a019de5e5a59e44ca48fa05df66d48d7f6f,,
166,Google Scholar,,,,,,,,,,,
167,Google Scholar,Comparative assessment of three quality frameworks for statistics derived from big data : the cases of Wikipedia page views and Automatic Identification Systems,"National and international statistical agencies are currently experimenting with the production of statistics derived partly or entirely from big data sources. At the same time there have been initiatives in the official statistics community and elsewhere to extend existing quality frameworks to statistics whose production involves the use of this type of data sources. UNECE's suggested framework for the quality of big data and Eurostat's accreditation of big data sources as input data for official statistics are two examples in this regard. The framework proposed in the report on big data of AAPOR (American Association for Public Opinion Research) is an example coming from outside official statistics. These frameworks have been developed based mostly on theoretical considerations, even if early experiments have provided some input. In this paper, we propose to enrich the experience in the application of these frameworks to particular use cases of statistical products based on big data sources in order to assess their suitability, feasibility and completeness. We apply these three quality frameworks in the context of ""experimental"" cultural statistics based on Wikipedia page views and to data from Automatic Identification Systems (AIS) for the production of transport statistics.",Fernando Reis; L. D. Consiglio; B. Kovachev; A. Wirthmann; Michail Skaliotis,2016,Unknown,N/A,2,1,https://www.semanticscholar.org/paper/fb77e3053150d7c4e42ca164b3d7b92140cfaf16,,
168,"Google Scholar, Web of Science",[Contraception in the German-language Wikipedia: a content and quality analysis].,,N. Döring; Stephan Lehmann; Claudia Schumann-Doermer,2022,Journal,"Bundesgesundheitsblatt, Gesundheitsforschung, Gesundheitsschutz",0,0,https://www.semanticscholar.org/paper/48cf2c6b9df60f7fb1e7ee71732ff60c8a734ff3,10.1007/s00103-022-03537-8,https://link.springer.com/content/pdf/10.1007/s00103-022-03537-8.pdf
169,"Google Scholar, Web of Science",Assessing the Quality of Thai Wikipedia Articles Using Concept and Statistical Features,,Kanchana Saengthongpattana; N. Soonthornphisaj,2014,Conference,"WorldCIST, pp. 513-523",10,2,https://www.semanticscholar.org/paper/9ae7b7133f62363a386922631bf4637e182815a4,10.1007/978-3-319-05951-8_49,
170,"ACM, Google Scholar, Web of Science",Interactive Quality Analytics of User-generated Content,"Digital libraries and services enable users to access large amounts of data on demand. Yet, quality assessment of information encountered on the Internet remains an elusive open issue. For example, Wikipedia, one of the most visited platforms on the Web, hosts thousands of user-generated articles and undergoes 12 million edits/contributions per month. User-generated content is undoubtedly one of the keys to its success but also a hindrance to good quality. Although Wikipedia has established guidelines for the “perfect article,” authors find it difficult to assert whether their contributions comply with them and reviewers cannot cope with the ever-growing amount of articles pending review. Great efforts have been invested in algorithmic methods for automatic classification of Wikipedia articles (as featured or non-featured) and for quality flaw detection. Instead, our contribution is an interactive tool that combines automatic classification methods and human interaction in a toolkit, whereby experts can experiment with new quality metrics and share them with authors that need to identify weaknesses to improve a particular article. A design study shows that experts are able to effectively create complex quality metrics in a visual analytics environment. In turn, a user study evidences that regular users can identify flaws, as well as high-quality content based on the inspection of automatic quality scores.",Cecilia di Sciascio; D. Strohmaier; M. Errecalde; Eduardo Veas,2019,Journal,"ACM Transactions on Interactive Intelligent Systems (TiiS), 9, pp. 1 - 42",61,2,https://www.semanticscholar.org/paper/b8f287f6f3ed28b4e494cfc05a39b4df1e36a87e,10.1145/3150973,
171,Google Scholar,Anfis based models for accessing quality of wikipedia articles,"Wikipedia is a free, web-based, collaborative, multilingual encyclopedia project supported by the non-profit Wikimedia Foundation. Due to the free nature of Wikipedia and allowing open access to ev ...",Noor Ullah,2010,Journal,N/A,0,3,https://www.semanticscholar.org/paper/c5dbe7d3f750a60ec8cfe56fbb8d3834b7ef0dee,,
172,Google Scholar,Structure-Based Features for Predicting the Quality of Articles in Wikipedia,,Baptiste de La Robertie; Y. Pitarch; O. Teste,2017,Journal,N/A,17,0,https://www.semanticscholar.org/paper/83fe1663a16d7e8e3d1988799bd9dc4cdbf2e16a,10.1007/978-3-319-51049-1_6,
173,Google Scholar,Evaluating Article Quality and Editor Reputation in Wikipedia,,Yuqing Lu; Lei Zhang; Juan-Zi Li,2013,Conference,"China Semantic Web Symposium, pp. 215-227",23,0,https://www.semanticscholar.org/paper/59ef2fa73fa249be1f5043da108464b7c3f7427c,10.1007/978-3-642-54025-7_19,
174,Google Scholar,,,,,,,,,,,
175,Google Scholar,Citation Detective : a Public Dataset to Improve and Quantify Wikipedia Citation Quality at Scale,"Machine learning models designed to improve citation quality in Wikipedia, such as text-based classifiers detecting sentences needing citations (“Citation Need” models), have received a lot of attention from both the scientific and the Wikimedia communities. However, due to their highly technical nature, the accessibility of such models is limited, and their usage generally restricted to machine learning researchers and practitioners. To fill this gap, we present Citation Detective , a system designed to periodically run Citation Need models on a large number of articles in English Wikipedia, and release public, usable, monthly data dumps exposing sentences classified as missing citations. By making Citation Need models usable to the broader public, Citation Detective opens up new opportunities for research and applications. We provide an example of a research direction enabled by Citation Detective , by conducting a large-scale analysis of citation quality in Wikipedia, showing that article citation quality is positively correlated with article quality, and that articles in Medicine and Biology are the most well sourced in English Wikipedia. The Citation Detective data and source code will be made publicly available and are being integrated with community tools for citation improvement such as Citation Hunt .",Ai-Jou Chou; Guilherme Gonçalves; Sam Walton; M. Redi,2020,Unknown,N/A,8,2,https://www.semanticscholar.org/paper/2fa03900c85676a9e75dd6ab6bce7009f4cd875c,,
176,Google Scholar,"A Strategy Oriented, Machine Learning Approach to Automatic Quality Assessment of Wikipedia Articles","A Strategy Oriented, Machine Learning Approach to Automatic Quality Assessment of Wikipedia Articles Gabriel De La Calzada This work discusses an approach to modeling and measuring information quality of Wikipedia articles. The approach is based on the idea that the quality of Wikipedia articles with distinctly different profiles needs to be measured using different information quality models. To implement this approach, a software framework written in the Java language was developed to collect and analyze information of Wikipedia articles. We report on our initial study, which involved two categories of Wikipedia articles: ”stabilized” (those, whose content has not undergone major changes for a significant period of time) and ”controversial” (articles that have undergone vandalism, revert wars, or whose content is subject to internal discussions between Wikipedia editors). In addition, we present simple information quality models and compare their performance on a subset of Wikipedia articles with the information quality evaluations provided by human users. Our experiment shows that using special-purpose models for information quality captures user sentiment about Wikipedia articles better than using a single model for both categories of articles.",Gabriel De la Calzada,2009,Journal,N/A,76,0,https://www.semanticscholar.org/paper/ca0385429020b68b3c44e2dfb1550eca74a20b21,10.15368/THESES.2009.32,https://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=1320&context=theses
180,Google Scholar,"Wikipedia-based Corpora for Analyzing Revisions, Discussions and Text Quality in Collaborative Writing","Wikipedia is both a valuable resource and a remarkable example of a non-standard data source for computational linguistics research. It is a unique source of authentic corpus material to study collaborative writing in the Web and offers terabytes of collaboratively constructed content in numerous domains. Due to the magnitude, diversity and the collaborative construction of its content, creating task-specific corpora for computational linguistics from Wikipedia is a non-trivial task. For example, the semiand sometimes unstructured data makes its segmentation difficult. In this presentation, we report about our ongoing work on several Wikipedia-based corpora tailored to support computational linguistics research with respect to:",Johannes Daxenberger; Oliver Ferschke; Iryna Gurevych,2012,Journal,N/A,3,2,https://www.semanticscholar.org/paper/56e0e88e19504c7c21478616ed5c1d41a81c5045,,
181,Google Scholar,Understanding the 'Quality Motion' of Wikipedia Articles Through Semantic Convergence Analysis,,Huijing Deng; B. Tarigan; Mihai Grigore; J. Sutanto,2015,Conference,"Interacción, pp. 64-75",10,1,https://www.semanticscholar.org/paper/00c6bd451cdedd9e4ad551978c41f0acd3f751ca,10.1007/978-3-319-20895-4_7,
182,Google Scholar,Quality contents creation in a commons-based peer production on line environment: the it.wikipedia experience,"The increasing growth of Wikipedia poses many questions about its organizational model and its development as a freeopen knowledge repository. Yochai Benkler describes Wikipedia as a CBPP (commons-based peer production) system: a platform which enables users to easily generate knowledge contents and to manage them collaboratively and on freevoluntary basis. The quality of its output is one of the main concerns related to Wikipedia. How would a CBPP environment guarantee at the same time the openness of its organization and a good level of accreditation? Which aspects of the project have more influence on quality? The paper offers an overview of one of the quality processes in it.wiki (Italian Wikipedia): the Vetrina section (Featured Articles). It also suggests an explanation to quality accreditation issue which questions Benkler's hypothesis. Thanks to a qualitative analysis carried out through in-depth interviews to Wikipedia users and through a period of ethnographic observation, the paper outlines Vetrina's organization and the social factors related to quality definition. The goal of the analysis is to give a better understanding of co-generation of contents processes and at the same time it tries to investigate quality assessment in one of the best known open knowledge on line project.",S. Monaci,2008,Journal,N/A,15,1,https://www.semanticscholar.org/paper/4e8583c37086c87ca9d1f2e9fc8fb0b8e5b0f8d5,,
183,"Google Scholar, Web of Science",The Wikipedia Medical Student: Comparing the Quality of Vascular Surgery Topics Across Two Commonly Used Educational Resources,,P. Jetty; Michael Yacob; S. Lotfi,2014,Journal,"Journal of Vascular Surgery, 60, pp. 1404",0,3,https://www.semanticscholar.org/paper/1d6c8c496b89d888fec2d178673a9a6514f0e353,10.1016/J.JVS.2014.08.030,http://www.jvascsurg.org/article/S0741521414015559/pdf
185,Google Scholar,The Category Structure in Wikipedia: To Analyze and Know Its Quality Using K-Core Decomposition,,Qishun Wang; Xiaohua Wang; Zhi-qun Chen,2013,Conference,"Knowledge Science, Engineering and Management, pp. 453-462",13,1,https://www.semanticscholar.org/paper/7a1851c877117001252cf90fc2ab26599695ed40,10.1007/978-3-642-39787-5_37,
186,Google Scholar,Building Quality in Wikipedia: A Theoretical Approach,,Guangyuan Zhu,2009,Journal,N/A,8,0,https://www.semanticscholar.org/paper/210306511daff2d670b8a3b9e3047deae363ec37,,
187,"ACM, Google Scholar, Web of Science",Quality assessment of Wikipedia content using topic models,"The web has become a large knowledge provider for society, allowing people to not just consume information but also produce it. Collaborative documents bring some significant advantages and decentralization, but they also raise questions concerning its quality. In this work, we explore the quality assessment on collaborative documents using these documents' topics. The proposed approach improved in 3.2% the accuracy of quality assesment of Wikipedia content. Then, the main contribution in this paper is an analysis of how we can use topic modelling in order to improve quality prediction performance.",Lauro C. J. Santos; Taís Christofani; I. S. Silva; D. H. Dalip,2019,Conference,Proceedings of the 25th Brazillian Symposium on Multimedia and the Web,28,2,https://www.semanticscholar.org/paper/5548a6d03196d6f9375cec984921336050baf485,10.1145/3323503.3360628,
189,Google Scholar,,,,,,,,,,,
193,Google Scholar,Quality of References Supporting Urologic Articles on Wikipedia,"Background: Wikipedia is an easily available and commonly used source of medical information for patients and practitioners. Its editing guidelines state that information added to the site should be accompanied by in-line citations to reliable (preferably secondary) sources to meet the website’s standards for verifiability. We hypothesized that a large fraction of these citations would be to popular media and to scientific works of relatively low academic impact. Objective: To characterize the works cited in the most frequently accessed Wikipedia articles on urologic diseases and interventions. Methods: Of the 1500 most-viewed articles in the Medicine WikiProject, 24 were relevant to urology. From these 24, the top 10 most-viewed urology-related articles were selected for citation analysis. Sources in in-line citations, defined as “ref” elements with unique or absent “name” attributes, were categorized by publisher and assigned one of 6 publication types. These types were: original scientific research, systematic reviews, scholarly non-systematic reviews (including books, editorials, and guidelines), medical references directed at the lay person (e.g. WebMD), popular media, and other. Finally, citation counts were retrieved using the Scopus database for references with digital object identifiers (DOI) or PubMed identification numbers (PMID). Results: The 24 articles included in the study were accessed by Wikipedia users an average of 2019 times per article per day (range 987-4,699) and contained an average of 40.5 citations (range 5-189). Of the 567 references cited in the 10 most viewed articles, 25.9% (147) were original research, 8.8% (50) were systematic reviews, 47.1% (267) were other scholarly reviews, 6.0% (34) were lay-directed medical resources, and only 7.9% (45) were popular media. 245 separate publishers were cited (208 scientific), of which the Journal of Urology was the most common (21 citations) followed by the Cochrane Database of Systematic Reviews (20). Of the 262 references with citation counts the Scopus database, 74.8% (196) were cited by at least 10 other scholarly articles. The median reference was cited by 26 other scientific articles (mean 151). Conclusions: The vast majority of sources backing information in urologic articles on Wikipedia are scientific books and peer-reviewed journals targeted to a professional audience. The plurality of these are textbooks and reviews of the literature. The vast majority of scientific articles cited have achieved at least a moderate level of recognition within their field. These findings suggest that the Wikipedia editing process for these articles does favor sources of relatively high scientific quality. Further research is required to determine whether the relatively high quality and quantity of sources cited implies high quality of the Wikipedia articles themselves. []",M. Strother,2014,Journal,N/A,0,0,https://www.semanticscholar.org/paper/75739ccfb7d74b3a870b1f78f50973e64b8e3f74,,
194,Google Scholar,,,,,,,,,,,
195,Google Scholar,Correction to: Application of SEO Metrics to Determine the Quality of Wikipedia Articles and Their Sources,,Włodzimierz Lewoniewski; Ralf-Christian Härting; Krzysztof Węcel; Christopher Reichstein; W. Abramowicz,2018,Journal,N/A,0,0,https://www.semanticscholar.org/paper/75935040ed9d1d3a3e58a9ef780287ba87a51f8d,10.1007/978-3-319-99972-2_49,https://link.springer.com/content/pdf/10.1007%2F978-3-319-99972-2_49.pdf
196,Google Scholar,Quality of open content in Wikipedia: towards a broader view of expertise?,,P. D. Laat,2011,Journal,N/A,0,0,https://www.semanticscholar.org/paper/2152fe52e54c568e2bcde938508de1913f3947a4,,
197,"Google Scholar, Web of Science",Predicting Low-Quality Wikipedia Articles Using User’s Judgements,,Ning Zhang; Lingyun Ruan; Luo Si,2015,Journal,N/A,8,0,https://www.semanticscholar.org/paper/f9ac07db4730ee15b657cab2e20c6ca6ed366336,10.1007/978-3-319-05467-4_6,
199,"ACM, Google Scholar",An investigation of the relationship between the amount of extra-textual data and the quality of Wikipedia articles,"Wikipedia, a web-based collaboratively maintained free encyclopedia, is emerging as one of the most important websites on the internet. However, its openness raises many concerns about the quality of the articles and how to assess it automatically. In the Portuguese-speaking Wikipedia, articles can be rated by bots and by the community. In this paper, we investigate the correlation between these ratings and the count of media items (namely images and sounds) through a series of experiments. Our results show that article ratings and the count of media items are correlated.",Marcelo Yuji Himoro; Raíza Hanada; Marco Cristo; M. G. Pimentel,2013,Conference,"Brazilian Symposium on Multimedia and the Web, pp. 333-336",16,0,https://www.semanticscholar.org/paper/48f6cd91cf9257329719bc69cb3457a3a35edde5,10.1145/2526188.2526218,
200,Google Scholar,Is Wikipedia a High Quality Evidence-Based Resource?,,B. Wiedermann,2017,Journal,N/A,0,0,https://www.semanticscholar.org/paper/44150e8873341b7930d117f98d77cc2333146eb8,,
202,"Google Scholar, Web of Science",Monitoring network structure and content quality of signal processing articles on wikipedia,"Wikipedia has become a widely-used resource on signal processing. However, the freelance-editing model of Wikipedia makes it challenging to maintain a high content quality. We develop techniques to monitor the network structure and content quality of Signal Processing (SP) articles on Wikipedia. Using metrics to quantify the importance and quality of articles, we generate a list of SP articles on Wikipedia arranged in the order of their need for improvement. The tools we use include the HITS and PageRank algorithms for network structure, crowdsourcing for quantifying article importance and known heuristics for article quality.",Tao-Chi Lee; J. Unnikrishnan,2013,Conference,"2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 8766-8770",20,1,https://www.semanticscholar.org/paper/ef732209c1b9fdcd014f9639533c5658593f9ac5,10.1109/ICASSP.2013.6639378,https://infoscience.epfl.ch/record/184967/files/icassp13-spwiki-submit2.pdf
203,Google Scholar,Quick fix or citable source?: Carol Haigh investigates the quality and rigour of using Wikipedia in support of academic research,,C. Haigh,2011,Journal,"Nursing Standard, 25, pp. 61-61",0,0,https://www.semanticscholar.org/paper/afe5f366486d11b6086897219128cc4b7e7a7c3e,10.7748/NS2011.05.25.38.61.P5335,
204,Google Scholar,"Internal Bonding, External Bridging and Functional Diversity: Impact of Social Capital on the Quality of Wikipedia Articles","This research focuses on the question of why Wikipedia articles are different in quality. Since Wikipedia articles are developed in an open and social environment, our work investigates if social capital of Wikipedia contributors plays a role in determining the quality of Wikipedia articles. In this study, we focus on three major factors of social capital with respect to teams of contributors working on different Wikipedia articles: internal bonding, external bridging and functional diversity. An empirical analysis of Wikipedia articles using quantitative techniques suggests that all these three major factors have a significant impact on the quality of Wikipedia articles. These results have implications for developing automated techniques for quality assessment of Wikipedia and also provide insights into improving quality of these articles.",S. Ram; Jun Liu,2011,Conference,International Conference on Wireless Information Technology and Systems,11,0,https://www.semanticscholar.org/paper/127e19e615fb2f90b8b123de16396c878ae95c4d,10.2139/ssrn.2249550,
207,Google Scholar,Towards the automatic evaluation of stylistic quality of natural texts: constructing a special-­purpose corpus of stylistic edits from the Wikipedia revision history,,A. Kotlyarov,2016,Journal,N/A,0,0,https://www.semanticscholar.org/paper/995830ab9b296b17c05fd40d3ec67965bf1d060e,,
208,Google Scholar,CMPSCI 645 Final Report Quality Evaluation of Wikipedia Articles,"An increasing number of people rely on the World Wide Web to access information. That has led to the development of many encyclopedia-like online repositories for specialized information. Arguably the most popular such site is Wikipedia, an online encyclopedia, which relies on collaborative content development. In Wikipedia, anyone (including anonymous authors) can create new articles, or edit and contribute to existing articles. Wikipedia relies on the idea that when millions of users share their knowledge and expertise, the end result is highquality, easily accessible information. In practice, however, malicious or simply ill-informed users can remove good content and add bad content.",Borislava I. Simidchieva; S. Christov,2008,Unknown,N/A,6,0,https://www.semanticscholar.org/paper/37e299d93f15ddfca764eb1b9632bab3e4ba67d4,,
209,Google Scholar,Dark matter at 5800 An investigation of the quality of user-contributed entries on the topic of dark matter in Wikipedia and other types of texts,"Statistics have shown that Wikipedia is very frequently used by the general public and that its articles rank high in online search engines. However, the accuracy and general quality of Wikipedia have been debated over the years. This study aims to investigate the quality of Wikipedia by expert reviewers pertaining to the accuracy, currency, breadth, readability, im-ages, structure, neutrality and relevance of a Wikipedia entry on dark matter. The entry has over 5800 edits. A comparison to two other centrally controlled sources, edited by acclaimed experts was also made. Data was collected by asking a number of qualified experts to review and rate three different texts, one published by NASA, one by Encyclopaedia Britannica and one from the English language version of Wikipedia. An interview with one of the experts was also carried out. The results showed that Wikipedia scored better than the other texts in all examined variables except for readability. Wikipedia was the preferred source by all but one panel members and its credibility was considered high. This review indicates that both NASA’s and Encyclopaedia Britannica’s articles on dark matter had a lower degree of quality than expected considering their brands’ high level of credibility. This report encourages the use of Wikipedia both for reference and as a platform to communicate, revise and correct re-search.",Lovisa Aijmer,2018,Journal,N/A,17,0,https://www.semanticscholar.org/paper/9f02b963b54e92d3d32a58295a29e84b47fcb406,,
210,Google Scholar,Wiki Evolution dataset: English Wikipedia revision articles represented by quality attributes,"Este artigo descreve a criação e disponibilização da base de dados de evolução de artigos da Wikipédia. A base é caracterizada por atributos de qualidades e a classe de qualidade dos artigos em determinada data, sendo cada instância entendida como revisão. Esta base pode ser utilizada para estudos relacionados com classificação automática de qualidade que considerem o histórico de revisão do artigo e entendimento de como o conteúdo e qualidade dos artigos evoluem ao longo do tempo nessa plataforma colaborativa.",Ana Luiza Sanches; Sinval de Deus Vieira Júnior; D. H. Dalip; Bárbara Gabrielle C. O. Lopes,2022,Conference,Anais do IV Dataset Showcase Workshop (DSW 2022),11,0,https://www.semanticscholar.org/paper/9ded22c4dad8607fd8298ad21735e38027eb41b0,10.5753/dsw.2022.225573,https://sol.sbc.org.br/index.php/dsw/article/download/21911/21734
211,Google Scholar,,,,,,,,,,,
262,ACM,Cultural diversity of quality of information on Wikipedias,"This article explores the relationship between linguistic culture and the preferred standards of presenting information based on article representation in major Wikipedias. Using primary research analysis of the number of images, references, internal links, external links, words, and characters, as well as their proportions in Good and Featured articles on the eight largest Wikipedias, we discover a high diversity of approaches and format preferences, correlating with culture. We demonstrate that high‐quality standards in information presentation are not globally shared and that in many aspects, the language culture's influence determines what is perceived to be proper, desirable, and exemplary for encyclopedic entries. As a result, we demonstrate that standards for encyclopedic knowledge are not globally agreed‐upon and “objective” but local and very subjective.",D. Jemielniak; Maciej Wilamowski,2017,Journal,"Journal of the Association for Information Science and Technology, 68",52,26,https://www.semanticscholar.org/paper/ef210d43c0ce301c41e2a7a81ee724874a7e7181,10.1002/asi.23901,
278,ACM,"When the levee breaks: without bots, what happens to Wikipedia's quality control processes?","In the first half of 2011, ClueBot NG -- one of the most prolific counter-vandalism bots in the English-language Wikipedia -- went down for four distinct periods, each period of downtime lasting from days to weeks. In this paper, we use these periods of breakdown as naturalistic experiments to study Wikipedia's heterogeneous quality control network, which we analyze as a multi-tiered system in which distinct classes of reviewers use various reviewing technologies to patrol for different kinds of damage at staggered time periods. Our analysis showed that the overall time-to-revert edits was almost doubled when this software agent was down. Yet while a significantly fewer proportion of edits made during the bot's downtime were reverted, we found that those edits were later eventually reverted. This suggests that other agents in Wikipedia took over this quality control work, but performed it at a far slower rate.",R. Geiger; Aaron L Halfaker,2013,Conference,Proceedings of the 9th International Symposium on Open Collaboration,13,70,https://www.semanticscholar.org/paper/ec2642fb3e3183ecc238ca6c21f59b51d0191c2a,10.1145/2491055.2491061,
338,Web of Science,On the Feasibility of External Factual Support as Wikipedia's Quality Metric,"Developing metrics to estimate the information quality of Wikipedia articles is an interesting and important research area. In this article, we propose and analyse the feasibility, of a new quality metric based on the “external factual support” of an article. The rationale behind this metric is identified, a formal definition of the metric is presented and some implementation aspects are introduced. Preliminary results show the feasibility of our proposal and its potential to discriminate high quality versus low quality Wikipedia’s articles.",Carlos G. Velázquez; L. Cagnina; M. Errecalde,2017,Journal,"Proces. del Leng. Natural, 58, pp. 93-100",20,5,https://www.semanticscholar.org/paper/e1f3c58d3b101ae9452a210297dd1150f056a01d,,
359,Web of Science,Quality of Wikipedia Articles: Analyzing Features and Building a Ground Truth for Supervised Classification,"Wikipedia is nowadays one of the biggest online resources on which users rely as a source of information. The amount of collaboratively generated content that is sent to the online encyclopedia every day can let to the possible creation of low-quality articles (and, consequently, misinformation) if not properly monitored and revised. For this reason, in this paper, the problem of automatically assessing the quality of Wikipedia articles is considered. In particular, the focus is (i) on the analysis of groups of hand-crafted features that can be employed by supervised machine learning techniques to classify Wikipedia articles on qualitative bases, and (ii) on the analysis of some issues behind the construction of a suitable ground truth. Evaluations are performed, on the analyzed features and on a specifically built labeled dataset, by implementing different supervised classifiers based on distinct machine learning algorithms, which produced promising results.",Elias Bassani; Marco Viviani,2019,Conference,"International Conference on Knowledge Discovery and Information Retrieval, pp. 338-346",35,3,https://www.semanticscholar.org/paper/e89231c16e56289654b0bcd62ae0b8d9a2af6e11,10.5220/0008149303380346,
369,Web of Science,,,,,,,,,,,
376,Web of Science,M1042 The Quality of Open Access and Open Source Internet Material in Gastroenterology: Is Wikipedia Appropriate for Knowledge Transfer to Patients?,,K. Czarnecka-Kujawa; R. Abdalian; S. Grover,2008,Journal,"Gastroenterology, 134",0,29,https://www.semanticscholar.org/paper/cc18ef19798c727052ea28968940b18cc5874492,10.1016/S0016-5085(08)61518-8,
379,Web of Science,,,,,,,,,,,
