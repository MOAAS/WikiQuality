Id,Title,Abstract,URL,Include (1),Include (2),Notes
1,Measuring article quality in wikipedia: models and evaluation,"Wikipedia has grown to be the world largest and busiest free encyclopedia, in which articles are collaboratively written and maintained by volunteers online. Despite its success as a means of knowledge sharing and collaboration, the public has never stopped criticizing the quality of Wikipedia articles edited by non-experts and inexperienced contributors. In this paper, we investigate the problem of assessing the quality of articles in collaborative authoring of Wikipedia. We propose three article quality measurement models that make use of the interaction data between articles and their contributors derived from the article edit history. Our Basic model is designed based on the mutual dependency between article quality and their author authority. The PeerReview model introduces the review behavior into measuring article quality. Finally, our ProbReview models extend PeerReview with partial reviewership of contributors as they edit various portions of the articles. We conduct experiments on a set of well-labeled Wikipedia articles to evaluate the effectiveness of our quality measurement models in resembling human judgement.",https://www.semanticscholar.org/paper/ee70063f74e209ec5dc3edfa13fbb82d60d1a3eb,Yes,,
2,Cooperation and quality in wikipedia,"The rise of the Internet has enabled collaboration and cooperation on anunprecedentedly large scale. The online encyclopedia Wikipedia, which presently comprises 7.2 million articles created by 7.04 million distinct editors, provides a consummate example. We examined all 50 million edits made tothe 1.5 million English-language Wikipedia articles and found that the high-quality articles are distinguished by a marked increase in number of edits, number of editors, and intensity of cooperative behavior, as compared to other articles of similar visibility and age. This is significant because in other domains, fruitful cooperation has proven to be difficult to sustain as the size of the collaboration increases. Furthermore, in spite of the vagaries of human behavior, we show that Wikipedia articles accrete edits according to a simple stochastic mechanism in which edits beget edits. Topics of high interest or relevance are thus naturally brought to the forefront of quality.",https://www.semanticscholar.org/paper/9c61b818ecc29a132592d886fd0d5cb0047b5b6d,Yes,,
3,Who does what: Collaboration patterns in the wikipedia and their impact on article quality,"The quality of Wikipedia articles is debatable. On the one hand, existing research indicates that not only are people willing to contribute articles but the quality of these articles is close to that found in conventional encyclopedias. On the other hand, the public has never stopped criticizing the quality of Wikipedia articles, and critics never have trouble finding low-quality Wikipedia articles. Why do Wikipedia articles vary widely in quality? We investigate the relationship between collaboration and Wikipedia article quality. We show that the quality of Wikipedia articles is not only dependent on the different types of contributors but also on how they collaborate. Based on an empirical study, we classify contributors based on their roles in editing individual Wikipedia articles. We identify various patterns of collaboration based on the provenance or, more specifically, who does what to Wikipedia articles. Our research helps identify collaboration patterns that are preferable or detrimental for article quality, thus providing insights for designing tools and mechanisms to improve the quality of Wikipedia articles.",https://www.semanticscholar.org/paper/620ba4d148d6e8d0f3103bf6ec47209534a33710,Maybe,,
4,Size matters: word count as a measure of quality on wikipedia,"Wikipedia, ""the free encyclopedia"", now contains over two million English articles, and is widely regarded as a high-quality, authoritative encyclopedia. Some Wikipedia articles, however, are of questionable quality, and it is not always apparent to the visitor which articles are good and which are bad. We propose a simple metric -- word count -- for measuring article quality. In spite of its striking simplicity, we show that this metric significantly outperforms the more complex methods described in related work.",https://www.semanticscholar.org/paper/c6f61f344c919493886bf67d0e64e0242ae83547,Yes,,
5,Harnessing the wisdom of crowds in wikipedia: quality through coordination,"Wikipedia's success is often attributed to the large numbers of contributors who improve the accuracy, completeness and clarity of articles while reducing bias. However, because of the coordination needed to write an article collaboratively, adding contributors is costly. We examined how the number of editors in Wikipedia and the coordination methods they use affect article quality. We distinguish between explicit coordination, in which editors plan the article through communication, and implicit coordination, in which a subset of editors structure the work by doing the majority of it. Adding more editors to an article improved article quality only when they used appropriate coordination techniques and was harmful when they did not. Implicit coordination through concentrating the work was more helpful when many editors contributed, but explicit coordination through communication was not. Both types of coordination improved quality more when an article was in a formative stage. These results demonstrate the critical importance of coordination in effectively harnessing the ""wisdom of the crowd"" in online production environments.",https://www.semanticscholar.org/paper/5ead79bbdb5d396f3a5900ff1b71ee44e8061dca,Yes,,
6,Determinants of wikipedia quality: the roles of global and local contribution inequality,"The success of Wikipedia and the relative high quality of its articles seem to contradict conventional wisdom. Recent studies have begun shedding light on the processes contributing to Wikipedia's success, highlighting the role of coordination and contribution inequality. In this study, we expand on these works in two ways. First, we make a distinction between global (Wikipedia-wide) and local (article-specific) inequality and investigate both constructs. Second, we explore both direct and indirect effects of these inequalities, exposing the intricate relationships between global inequality, local inequality, coordination, and article quality. We tested our hypotheses on a sample of a Wikipedia articles using structural equation modeling and found that global inequality exerts significant positive impact on article quality, while the effect of local inequality is indirect and is mediated by coordination",https://www.semanticscholar.org/paper/36dce0334d47e45cd37f9484b464ee3857cbdc37,Maybe,,
7,Information quality work organization in wikipedia,"The classic problem within the information quality (IQ) research and practice community has been the problem of defining IQ. It has been found repeatedly that IQ is context sensitive and cannot be described, measured, and assured with a single model. There is a need for empirical case studies of IQ work in different systems to develop a systematic knowledge that can then inform and guide the construction of context-specific IQ models. This article analyzes the organization of IQ assurance work in a large-scale, open, collaborative encyclopedia—Wikipedia. What is special about Wikipedia as a resource is that the quality discussions and processes are strongly connected to the data itself and are accessible to the general public. This openness makes it particularly easy for researchers to study a particular kind of collaborative work that is highly distributed and that has a particularly substantial focus, not just on error detection but also on error correction. We believe that the study of those evolving debates and processes and of the IQ assurance model as a whole has useful implications for the improvement of quality in other more conventional databases. © 2008 Wiley Periodicals, Inc.",https://www.semanticscholar.org/paper/89c5bef3053bf3e4953204c7357ff838523b7488,Maybe,,
8,On measuring the quality of Wikipedia articles,"This paper discusses an approach to modeling and measuring information quality of Wikipedia articles. The approach is based on the idea that the quality of Wikipedia articles with distinctly different profiles needs to be measured using different information quality models. We report on our initial study, which involved two categories of Wikipedia articles: ""stabilized"" (those, whose content has not undergone major changes for a significant period of time) and ""controversial"" (the articles, which have undergone vandalism, revert wars, or whose content is subject to internal discussions between Wikipedia editors). We present simple information quality models and compare their performance on a subset of Wikipedia articles with the information quality evaluations provided by human users. Our experiment shows, that using special-purpose models for information quality captures user sentiment about Wikipedia articles better than using a single model for both categories of articles.",https://www.semanticscholar.org/paper/68c1ca9cf4ef3bb12e17ea38c6155b3374ccff88,Yes,,
9,Don't bite the newbies: how reverts affect the quantity and quality of Wikipedia work,"Reverts are important to maintaining the quality of Wikipedia. They fix mistakes, repair vandalism, and help enforce policy. However, reverts can also be damaging, especially to the aspiring editor whose work they destroy. In this research we analyze 400,000 Wikipedia revisions to understand the effect that reverts had on editors. We seek to understand the extent to which they demotivate users, reducing the workforce of contributors, versus the extent to which they help users improve as encyclopedia editors. Overall we find that reverts are powerfully demotivating, but that their net influence is that more quality work is done in Wikipedia as a result of reverts than is lost by chasing editors away. However, we identify key conditions -- most specifically new editors being reverted by much more experienced editors - under which reverts are particularly damaging. We propose that reducing the damage from reverts might be one effective path for Wikipedia to solve the newcomer retention problem.",https://www.semanticscholar.org/paper/b2149024c793fb323b4fbe940c42ecf566860105,Yes,,
10,Assessing the quality of Wikipedia articles with lifecycle based metrics,"The main feature of the free online-encyclopedia Wikipedia is the wiki-tool, which allows viewers to edit the articles directly in the web browser. As a weakness of this openness for example the possibility of manipulation and vandalism cannot be ruled out, so that the quality of any given Wikipedia article is not guaranteed. Hence the automatic quality assessment has been becoming a high active research field. In this paper we offer new metrics for an efficient quality measurement. The metrics are based on the lifecycles of low and high quality articles, which refer to the changes of the persistent and transient contributions throughout the entire life span.",https://www.semanticscholar.org/paper/777a29dff88b22c0cf2e38683cbaf183447a985b,Yes,,
11,Statistical measure of quality in Wikipedia,"Wikipedia is commonly viewed as the main online encyclopedia. Its content quality, however, has often been questioned due to the open nature of its editing model. A high--quality contribution by an expert may be followed by a low-quality contribution made by an amateur or a vandal; therefore the quality of each article may fluctuate over time as it goes through iterations of edits by different users. With the increasing use of Wikipedia, the need for a reliable assessment of the quality of the content is also rising. In this study, we model the evolution of content quality in Wikipedia articles in order to estimate the fraction of time during which articles retain high-quality status. To evaluate the model, we assess the quality of Wikipedia's featured and non-featured articles. We show how the model reproduces consistent results with what is expected. As a case study, we use the model in a CalSWIM mashup the content of which is taken from both highly reliable sources and Wikipedia, which may be less so. Integrating CalSWIM with a trust management system enables it to use not only recency but also quality as its criteria, and thus filter out vandalized or poor-quality content.",https://www.semanticscholar.org/paper/54333cd5b3f6d0148f0af632455faafb8dce7567,Maybe,,
12,Information quality discussions in wikipedia,"We examine the Information Quality aspects of Wikipedia. By a study of the discussion pages and other process-oriented pages within the Wikipedia project, it is possible to determine the information quality dimensions that participants in the editing process care about, how they talk about them, what tradeoffs they make between these dimensions and how the quality assessment and improvement process operates. This analysis helps in understanding how high quality is maintained in a project where anyone may participate with no prior vetting. It also carries implications for improving the quality of more conventional datasets.",https://www.semanticscholar.org/paper/ebe949e01d044a25daa0e6cabb3091cc319d361f,Maybe,,
13,Tell me more: an actionable quality model for Wikipedia,"In this paper we address the problem of developing actionable quality models for Wikipedia, models whose features directly suggest strategies for improving the quality of a given article. We first survey the literature in order to understand the notion of article quality in the context of Wikipedia and existing approaches to automatically assess article quality. We then develop classification models with varying combinations of more or less actionable features, and find that a model that only contains clearly actionable features delivers solid performance. Lastly we discuss the implications of these results in terms of how they can help improve the quality of articles across Wikipedia.",https://www.semanticscholar.org/paper/00f5c7311b9ecc2d91cb3ab7cac6bb6acec28580,Yes,,
14,Automatic quality assessment of content created collaboratively by web communities: a case study of wikipedia,"The old dream of a universal repository containing all the human knowledge and culture is becoming possible through the Internet and the Web. Moreover, this is happening with the direct collaborative, participation of people. Wikipedia is a great example. It is an enormous repository of information with free access and edition, created by the community in a collaborative manner. However, this large amount of information, made available democratically and virtually without any control, raises questions about its relative quality. In this work we explore a significant number of quality indicators, some of them proposed by us and used here for the first time, and study their capability to assess the quality of Wikipedia articles. Furthermore, we explore machine learning techniques to combine these quality indicators into one single assessment judgment. Through experiments, we show that the most important quality indicators are the easiest ones to extract, namely, textual features related to length, structure and style. We were also able to determine which indicators did not contribute significantly to the quality assessment. These were, coincidentally, the most complex features, such as those based on link analysis. Finally, we compare our combination method with state-of-the-art solution and show significant improvements in terms of effective quality prediction.",https://www.semanticscholar.org/paper/412e1cf936f9a0aa4b289f14136174232e8f4e38,Yes,,
15,Learning to Predict the Quality of Contributions to Wikipedia,"Although some have argued that Wikipedia’s open edit policy is one of the primary reasons for its success, it also raises concerns about quality — vandalism, bias, and errors can be problems. Despite these challenges, Wikipedia articles are often (perhaps surprisingly) of high quality, which many attribute to both the dedicatedWikipedia community and “good Samaritan” users. As Wikipedia continues to grow, however, it becomes more difficult for these users to keep up with the increasing number of articles and edits. This motivates the development of tools to assist users in creating and maintaining quality. In this paper, we propose metrics that quantify the quality of contributions to Wikipedia through implicit feedback from the community. We then learn discriminative probabilistic models that predict the quality of a new edit using features of the changes made, the author of the edit, and the article being edited. Through estimating parameters for these models, we also gain an understanding of factors that influence quality. We advocate using edit quality predictions and information gleaned from model analysis not to place restrictions on editing, but to instead alert users to potential quality problems, and to facilitate the development of additional incentives for contributors. We evaluate the edit quality prediction models on the Spanish Wikipedia. Experiments demonstrate that the models perform better when given access to content-based features of the edit, rather than only features of contributing user. This suggests that a user-based solution to the Wikipedia quality problem may not be sufficient. Introduction and Motivation Collaborative content generation systems such as Wikipedia are promising because they facilitate the integration of information from many disparate sources. Wikipedia is remarkable because anyone can edit an article. Some argue that this open edit policy is one of the key reasons for its success (Roth 2007; Riehle 2006). However, this openness does raise concerns about quality — vandalism, bias, and errors can be problems (Denning et al. 2005; Riehle 2006; Kittur et al. 2007). Despite the challenges associated with an open edit policy, Wikipedia articles are often of high quality (Giles 2005). Many suggest that this is a result of dedicated users that make many edits, monitor articles for changes, and engage Copyright c © 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. in debates on article discussion pages. These users are sometimes referred to as “zealots” (Anthony, Smith, and Williamson 2007), and studies claim that they are motivated by a system of peer recognition that bears resemblance to the academic community (Forte and Bruckman 2005). However, the contributions of “good Samaritan” users, who edit articles but have no desire to participate in the community, cannot not be underestimated (Anthony, Smith, and Williamson 2007). As Wikipedia continues to grow, however, it becomes more difficult for these users to keep up with the increasing number of articles and edits. Zealots comprise a relatively small portion of all Wikipedia users. Good Samaritan users are not likely to seek out errors, but instead rely on stumbling upon them. It is interesting to consider whether aiding users in detecting and focusing effort on quality problems could improve Wikipedia. In this paper, we examine the problem of estimating the quality of a new edit. Immediately, we face the problem of defining edit quality. It has been argued that there is no general definition of information quality, and hence quality must be defined using empirical observations of community interactions (Stvilia et al. 2008). Therefore, we define quality using implicit feedback from the Wikipedia community itself. That is, by observing the community’s response to a particular edit, we can estimate the edit’s quality. The quality metrics we propose are based on the assumption that edits to an article that are retained in subsequent versions of the article are of high quality, whereas edits that are quickly removed are of low quality. We use these community-defined measures of edit quality to learn statistical models that can predict the quality of a new edit. Quality is predicted using features of the edit itself, the author of the edit, and the article being edited. Through learning to predict quality, we also learn about factors that influence quality. Specifically, we provide analysis of model parameters to determine which features are the most useful for predicting quality. We advocate using edit quality predictions and information gleaned from model analysis not to place restrictions on editing, but to assist users in improving quality. That is, we aim to maintain a low barrier to participation, as those users not interested in the Wikipedia community can still be valuable contributors (Anthony, Smith, and Williamson 2007). Restrictions might also discourage new users, and drive away users who were drawn to the idea of a openly editable encyclopedia. Consequently, we suggest that the quality models be used to help users focus on predicted quality problems or to encourage participation. We evaluate the edit quality prediction models and provide analysis using the Spanish Wikipedia. Experiments demonstrate that the models attain better results when given access to content-based features, in addition to features of the contributing user. This suggests that a user-based solution to the Wikipedia quality problem may not be sufficient. Although we focus on Wikipedia in this paper, we think of this as an instance of a new problem: automatically predicting the quality of contributions in a collaborative environment.",https://www.semanticscholar.org/paper/029c4971f7edd68c667baf15de392dbf104072f7,Yes,,
16,Measuring Quality of Collaboratively Edited Documents: The Case of Wikipedia,"Wikipedia is a great example of large scale collaboration, where people from all over the world together build the largest and maybe the most important human knowledge repository in the history. However, a number of studies showed that the quality of Wikipedia articles is not equally distributed. While many articles are of good quality, many others need to be improved. Assessing the quality of Wikipedia articles is very important for guiding readers towards articles of high quality and suggesting authors and reviewers which articles need to be improved. Due to the huge size of Wikipedia, an effective automatic assessment method to measure Wikipedia articles quality is needed. In this paper, we present an automatic assessment method of Wikipedia articles quality by analyzing their content in terms of their format features and readability scores. Our results show improvements both in terms of accuracy and information gain compared with other existing approaches.",https://www.semanticscholar.org/paper/31f226174f2da4ceca288b93769371c388d6c4fb,Yes,,
17,Predicting quality flaws in user-generated content: the case of wikipedia,"The detection and improvement of low-quality information is a key concern in Web applications that are based on user-generated content; a popular example is the online encyclopedia Wikipedia. Existing research on quality assessment of user-generated content deals with the classification as to whether the content is high-quality or low-quality. This paper goes one step further: it targets the prediction of quality flaws, this way providing specific indications in which respects low-quality content needs improvement. The prediction is based on user-defined cleanup tags, which are commonly used in many Web applications to tag content that has some shortcomings. We apply this approach to the English Wikipedia, which is the largest and most popular user-generated knowledge source on the Web. We present an automatic mining approach to identify the existing cleanup tags, which provides us with a training corpus of labeled Wikipedia articles. We argue that common binary or multiclass classification approaches are ineffective for the prediction of quality flaws and hence cast quality flaw prediction as a one-class classification problem. We develop a quality flaw model and employ a dedicated machine learning approach to predict Wikipedia's most important quality flaws. Since in the Wikipedia setting the acquisition of significant test data is intricate, we analyze the effects of a biased sample selection. In this regard we illustrate the classifier effectiveness as a function of the flaw distribution in order to cope with the unknown (real-world) flaw-specific class imbalances. The flaw prediction performance is evaluated with 10,000 Wikipedia articles that have been tagged with the ten most frequent quality flaws: provided test data with little noise, four flaws can be detected with a precision close to 1.",https://www.semanticscholar.org/paper/ab675867bf0d77b8d6c6b5b7dc2b00075ad422c4,Yes,,
18,Quality and Importance of Wikipedia Articles in Different Languages,"This article aims to analyse the importance of the Wikipedia articles in different languages (English, French, Russian, Polish) and the impact of the importance on the quality of articles. Based on the analysis of literature and our own experience we collected measures related to articles, specifying various aspects of quality that will be used to build the models of articles’ importance. For each language version, the influential parameters are selected that may allow automatic assessment of the validity of the article. Links between articles in different languages offer opportunities in terms of comparison and verification of the quality of information provided by various Wikipedia communities. Therefore, the model can be used not only for a relative assessment of the content of the whole article, but also for a relative assessment of the quality of data contained in their structural parts, the so-called infoboxes.",https://www.semanticscholar.org/paper/dd5a0bd3b918f4bc43568d461e60345fdeb55f21,Yes,,
19,"A jury of your peers: quality, experience and ownership in Wikipedia","Wikipedia is a highly successful example of what mass collaboration in an informal peer review system can accomplish. In this paper, we examine the role that the quality of the contributions, the experience of the contributors and the ownership of the content play in the decisions over which contributions become part of Wikipedia and which ones are rejected by the community. We introduce and justify a versatile metric for automatically measuring the quality of a contribution. We find little evidence that experience helps contributors avoid rejection. In fact, as they gain experience, contributors are even more likely to have their work rejected. We also find strong evidence of ownership behaviors in practice despite the fact that ownership of content is discouraged within Wikipedia.",https://www.semanticscholar.org/paper/b91411b9d80525ed6fa61ab01f2a1c5459fbd05d,Yes,,
20,Automatically Assessing Wikipedia Article Quality by Exploiting Article-Editor Networks,"We consider the problem of automatically assessing Wikipedia article quality. We develop several models to rank articles by using the editing relations between articles and editors. First, we create a basic model by modeling the article-editor network. Then we design measures of an editor’s contribution and build weighted models that improve the ranking performance. Finally, we use a combination of featured article information and the weighted models to obtain the best performance. We find that using manual evaluation to assist automatic evaluation is a viable solution for the article quality assessment task on Wikipedia",https://www.semanticscholar.org/paper/f99a7a7ceca5bfe2e1c8c4d0ea5b67203dbc4f34,Yes,,
21,Information Quality in Wikipedia: The Effects of Group Composition and Task Conflict,"The success of Wikipedia demonstrates that self-organizing production communities can produce high-quality information-based products. Research on Wikipedia has proceeded largely atheoretically, focusing on (1) the diversity in members' knowledge bases as a determinant of Wikipedia's content quality, (2) the task-related conflicts that occur during the collaborative authoring process, and (3) the different roles members play in Wikipedia. We develop a theoretical model that explains how these three factors interact to determine the quality of Wikipedia articles. The results from the empirical study of 96 Wikipedia articles suggest that (1) diversity should be encouraged, as the creative abrasion that is generated when cognitively diverse members engage in task-related conflict leads to higher-quality articles, (2) task conflict should be managed, as conflict—notwithstanding its contribution to creative abrasion—can negatively affect group output, and (3) groups should maintain a balance of both administrative- and content-oriented members, as both contribute to the collaborative process.",https://www.semanticscholar.org/paper/26450adc2040f7f7d2ed7366bbe90d5091dbd3a1,No,,Manual
22,Interpolating Quality Dynamics in Wikipedia and Demonstrating the Keilana Effect,"For open, volunteer generated content like Wikipedia, quality is a prominent concern. To measure Wikipedia's quality, researchers have historically relied on expert evaluation or assessments of article quality by Wikipedians themselves. While both of these methods have proven effective for answering many questions about Wikipedia's quality and processes, they are both problematic: expert evaluation is expensive and Wikipedian quality assessments are sporadic and unpredictable. Studies that explore Wikipedia's quality level or the processes that result in quality improvements have only examined small snapshots of Wikipedia and often rely on complex propensity models to deal with the unpredictable nature of Wikipedians' own assessments. In this paper, I describe a method for measuring article quality in Wikipedia historically and at a finer granularity than was previously possible. I use this method to demonstrate an important coverage dynamic in Wikipedia (specifically, articles about women scientists) and offer this method, dataset, and open API to the research community studying Wikipedia quality dynamics.",https://www.semanticscholar.org/paper/1324bad076e1706e6b95144b738c7ae15cf4bb6e,Yes,,"Public dataset, API"
23,An end-to-end learning solution for assessing the quality of Wikipedia articles,"Wikipedia is considered as the largest knowledge repository in the history of humanity and plays a crucial role in modern daily life. Assigning the correct quality class to Wikipedia articles is an important task in order to provide guidance for both authors and readers of Wikipedia. The manual review cannot cope with the editing speed of Wikipedia. An automatic classification is required to classify the quality of Wikipedia articles. Most existing approaches rely on traditional machine learning with manual feature engineering, which requires a lot of expertise and effort. Furthermore, it is known that there is no general perfect feature set because information leak always occurs in feature extraction phase. Also, for each language of Wikipedia, a new feature set is required. In this paper, we present an approach relying on deep learning for quality classification of Wikipedia articles. Our solution relies on Recurrent Neural Networks (RNN) which is an end-to-end learning technique that eliminates disadvantages of feature engineering. Our approach learns directly from raw data without human intervention and is language-neutral. Experimental results on English, French and Russian Wikipedia datasets show that our approach outperforms state-of-the-art solutions.",https://www.semanticscholar.org/paper/4244e4b33e00bc221f9e5fd78adde241ee499cc5,Yes,,DL
24,Quality assessment of Wikipedia articles without feature engineering,"As Wikipedia became the largest human knowledge repository, quality measurement of its articles received a lot of attention during the last decade. Most research efforts focused on classification of Wikipedia articles quality by using a different feature set. However, so far, no “golden feature set” was proposed. In this paper, we present a novel approach for classifying Wikipedia articles by analysing their content rather than by considering a feature set. Our approach uses recent techniques in natural language processing and deep learning, and achieved a comparable result with the state-of-the-art.",https://www.semanticscholar.org/paper/425064f6236d6457344d45c3517097936d42aea3,Yes,,DL
25,Relating Wikipedia article quality to edit behavior and link structure,"Currently, the relation between edit behavior, link structure, and article quality is not well-understood in our community, notwithstanding that this relationship may facilitate editing processes and content quality on Wikipedia. To shed light on this complex relation, we classify article edits and perform an in-depth analysis of editing sequences for 4941 articles. Additionally, we build a network of internal Wikipedia hyperlinks between articles. Using this data, we compute parsimonious metrics to quantify editing and linking behavior. Our analysis unveils that conflicted articles differ substantially from others in almost all metrics, while we also detect slight trends for high-quality articles. With our network analysis we find evidence indicating that controversial and edit war articles frequently span structural holes in the Wikipedia network. Finally, in a prediction experiment we demonstrate the usefulness of edit behavior patterns and network properties in predicting conflict and article quality. With our work, we assist online collaboration communities, especially Wikipedia, in long-term improvement of content quality by offering valuable insights about the interplay of article quality, controversies and edit wars, editing behavior, and network properties via sequence-based edit and network-based article metrics.",https://www.semanticscholar.org/paper/d2742c2e19966865ab0db00fa515c79678eec9dd,Maybe,,
26,Assessing the quality of information on wikipedia: A deep‐learning approach,"Currently, web document repositories have been collaboratively created and edited. One of these repositories, Wikipedia, is facing an important problem: assessing the quality of Wikipedia. Existing approaches exploit techniques such as statistical models or machine leaning algorithms to assess Wikipedia article quality. However, existing models do not provide satisfactory results. Furthermore, these models fail to adopt a comprehensive feature framework. In this article, we conduct an extensive survey of previous studies and summarize a comprehensive feature framework, including text statistics, writing style, readability, article structure, network, and editing history. Selected state‐of‐the‐art deep‐learning models, including the convolutional neural network (CNN), deep neural network (DNN), long short‐term memory (LSTMs) network, CNN‐LSTMs, bidirectional LSTMs, and stacked LSTMs, are applied to assess the quality of Wikipedia. A detailed comparison of deep‐learning models is conducted with regard to different aspects: classification performance and training performance. We include an importance analysis of different features and feature sets to determine which features or feature sets are most effective in distinguishing Wikipedia article quality. This extensive experiment validates the effectiveness of the proposed model.",https://www.semanticscholar.org/paper/32c2952ea7128c0ed674548158c3e9d1e4ba3a13,Yes,,DL
27,Automatically Assessing the Quality of Wikipedia Articles,"Since its inception in 2001, Wikipedia has fast become one of the Internet's most dominant sources of information. Dubbed ""the free encyclopedia"", Wikipedia contains millions of articles that are written, edited, and maintained by volunteers. Due in part to the open, collaborative process by which content is generated, many have questioned the reliability of these articles. The high variance in quality between articles is a potential source of confusion that likely leaves many visitors unable to distinguish between good articles and bad. In this work, we describe how a very simple metric – word count – can be used to as a proxy for article quality, and discuss the implications of this result for Wikipedia in particular, and quality assessment in general.",https://www.semanticscholar.org/paper/2ba510e022be06fa03dae878c5d9193bccef2cb5,No,,Not article (report)
28,Information quality assessment of community generated content: A user study of Wikipedia,"This study examines the ways in which information consumers evaluate the quality of content in a collaborative-writing environment, in this case Wikipedia. Sixty-four users were asked to assess the quality of five articles from the Hebrew Wikipedia, to indicate the highest- and lowest-quality article of the five and explain their choices. Participants viewed both the article page, and the article’s history page, so that their decision was based both on the article’s current content and on its development. The analysis shows that the attributes that most frequently assisted the users in deciding about the quality of the items were not unique to Wikipedia: attributes such as amount of information, satisfaction with content and external links were mentioned frequently, as with other information quality studies on the web. The findings also support the claim that quality is a subjective concept which depends on the user’s unique point of view. Attributes such as number of edits and number of unique editors received two contradictory meanings – both few edits/editors and many edits/editors were mentioned as attributes of high-quality articles.",https://www.semanticscholar.org/paper/4c43a063f1cd5cd888f526543be6160ecd0de9a9,No,,Manual
29,Using big data and network analysis to understand Wikipedia article quality,"The research reported in this paper focuses on the question of why Wikipedia articles are different in quality. Since these articles are developed in an open and social environment, our work investigates if the social capital of contributors plays a role in determining the quality of the articles. We focus on three major types of social capital with respect to teams of contributors working on Wikipedia articles: internal bonding, external bridging and functional diversity. Through a social network analysis of these articles based on a dataset extracted from its edit history, our research finds that all three types of social capital have a significant impact on their quality. In addition, we found that internal bonding interacts positively with external bridging resulting in a multiplier effect on article quality. The findings of our research have implications for developing automated techniques for quality assessment of Wikipedia and also provide insights into improving quality of these articles.",https://www.semanticscholar.org/paper/72ccb62a36d13a4563db9a926db72a6a57fe21fd,Maybe,,
30,Measuring article quality in Wikipedia: Lexical clue model,"Wikipedia is the most entry-abundant on-line encyclopedia. Some studies published by Nature proved that the scientific entries in Wikipedia are of good quality comparable to those in the Encyclopedia Britannica which are mainly maintained by experts. But the manual partition of the articles in Wikipedia from a WikiProject implies that high-quality articles are usually reached grade by grade via being repeatedly revised. So many work address to automatically measuring the article quality in Wikipedia based on some assumption of the relationship between the article quality and contributors' reputations, view behaviors, article status, inter-article link, or so on. In this paper, a lexical clue based measuring method is proposed to assess article quality in Wikipedia. The method is inspired the idea that the good articles have more regular statistic features on lexical usage than the primary ones due to the more revise by more people. We select 8 lexical features derived from the statistic on word usages in articles as the factors that can reflect article quality in Wikipedia. A decision tree is trained based on the lexical clue model. Using the decision tree, our experiments on a well-labeled collection of 200 Wikipedia articles shows that our method has more than 83% precise and recall.",https://www.semanticscholar.org/paper/a2a786d8cfe123bfd58b481c5f03e4d6f969e0e0,Yes,,
31,A Hybrid Model for Quality Assessment of Wikipedia Articles,"The task of document quality assessment is a highly complex one, which draws on analysis of aspects including linguistic content, document structure, fact correctness, and community norms. We explore the task in the context of a Wikipedia article assessment task, and propose a hybrid approach combining deep learning with features proposed in the literature. Our method achieves 6.5% higher accuracy than the state of the art in predicting the quality classes of English Wikipedia articles over a novel dataset of around 60k Wikipedia articles. We also discuss limitations with this task setup, and possible directions for establishing more robust document quality assessment evaluations.",https://www.semanticscholar.org/paper/894603d927860010ed3554a9922a992838188d81,Yes,,DL + Features
32,Measuring article quality in Wikipedia using the collaboration network,"Collaboratively edited articles such as in Wikipedia suffer from well-identified problems regarding their quality, e.g., information accuracy, reputability of third-party sources, vandalism. Due to the huge number of articles and the intensive edit rate, the manual evaluation of article content quality is inconceivable. In this paper, we tackle the problem of automatically establishing the quality of Wikipedia articles. Evidences are shown to consider the interactions between authors and articles to assess the quality score. Collaborations between authors and reviewers are also considered to reinforce the discriminative process. This work gives a generic formulation of the Mutual Reinforcement principle held between articles quality and authors authority and take explicitly advantage of the co-edits graph generated by individuals. Experiments conducted on a set of representative data from Wikipedia show the effectiveness of our approach.",https://www.semanticscholar.org/paper/f2b5cb81200b34fbc5e2e1468218d9be27fc23d6,Yes,,
33,QuWi: quality control in Wikipedia,"We propose and evaluate QuWi (Quality in Wikipedia), a framework for quality control in Wikipedia. We build upon a previous proposal by Mizzaro [11], who proposed a method for substituting and/or complementing peer review in scholarly publishing. Since articles in Wikipedia are never finished, and their authors change continuously, we define a modified algorithm that takes into account the different domain, with particular attention to the fact that authors contribute identifiable pieces of information that can be further modified by other authors. The algorithm assigns quality scores to articles and contributors. The scores assigned to articles can be used, e.g., to let the reader understand how reliable are the articles he or she is looking at, or to help contributors in identifying low quality articles to be enhanced. The scores assigned to users measure the average quality of their contributions to Wikipedia and can be used, e.g., for conflict resolution policies based on the quality of involved users. Our proposed algorithm is experimentally evaluated by analyzing the obtained quality scores on articles for deletion and featured articles, also on six temporal Wikipedia snapshots. Preliminary results demonstrate that the proposed algorithm seems to appropriately identify high and low quality articles, and that high quality authors produce more long-lived contributions than low quality authors.",https://www.semanticscholar.org/paper/254a11522cd21547d32c21ae500d8b2846f80189,Yes,,Visualization
34,Modelling the Quality of Attributes in Wikipedia Infoboxes,"Quality of data in DBpedia depends on underlying information provided in Wikipedia’s infoboxes. Various language editions can provide different information about given subject with respect to set of attributes and values of these attributes. Our research question is which language editions provide correct values for each attribute so that data fusion can be carried out. Initial experiments proved that quality of attributes is correlated with the overall quality of the Wikipedia article providing them. Wikipedia offers functionality to assign a quality class to an article but unfortunately majority of articles have not been graded by community or grades are not reliable. In this paper we analyse the features and models that can be used to evaluate the quality of articles, providing foundation for the relative quality assessment of infobox’s attributes, with the purpose to improve the quality of DBpedia. 
",https://www.semanticscholar.org/paper/a57b3142070beec458939e0a898991e0246f2ad8,Maybe,,
35,Towards automatic quality assurance in Wikipedia,"Featured articles in Wikipedia stand for high information quality, and it has been found interesting to researchers to analyze whether and how they can be distinguished from ""ordinary"" articles. Here we point out that article discrimination falls far short of writer support or automatic quality assurance: Featured articles are not identified, but are made. Following this motto we compile a comprehensive list of information quality flaws in Wikipedia, model them according to the latest state of the art, and devise one-class classification technology for their identification.",https://www.semanticscholar.org/paper/804fcac24b2e3db16b226dff9c71d85aa333a075,Yes,,
36,What makes a good biography?: multidimensional quality analysis based on wikipedia article feedback data,"With more than 22 million articles, the largest collaborative knowledge resource never sleeps, experiencing several article edits every second. Over one fifth of these articles describes individual people, the majority of which are still alive. Such articles are, by their nature, prone to corruption and vandalism. Manual quality assurance by experts can barely cope with this massive amount of data. Can it be effectively replaced by feedback from the crowd? Can we provide meaningful support for quality assurance with automated text processing techniques? Which properties of the articles should then play a key role in the machine learning algorithms and why? In this paper, we study the user-perceived quality of Wikipedia articles based on a novel Wikipedia user feedback dataset. In contrast to previous work on quality assessment which mostly relied on judgements of active Wikipedia authors, we analyze ratings of ordinary Wikipedia users along four quality dimensions (Complete, Well written, Trustworthy and Objective). We first present an empirical analysis of the novel dataset with over 36 million Wikipedia article ratings. We then select a subset of biographical articles and perform classification experiments to predict their quality ratings along each of the dimensions, exploring multiple linguistic, surface and network properties of the rated articles. Additionally, we study the classification performance and differences for the biographies of living and dead people as well as those for men and women. We demonstrate the effectiveness of our approach by the F-scores of 0.94, 0.89, 0.73, and 0.73 for the dimensions Complete, Well written, Trustworthy, and Objective. Based on the results, we believe that the quality assessment of big textual data can be effectively supported by current text classification and language processing tools.",https://www.semanticscholar.org/paper/2a4bb00fb6f8471e7b2e7a7479e2042271611c81,Yes,,
38,NwQM: A Neural Quality Assessment Framework for Wikipedia,"Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics. Articles have been categorized by editors into several quality classes, which indicate their reliability as encyclopedic content. This manual designation is an onerous task because it necessitates profound knowledge about encyclopedic language, as well navigating circuitous set of wiki guidelines. In this paper we propose Neural wikipedia QualityMonitor (NwQM), a novel deep learning model which accumulates signals from several key information sources such as article text, meta data and images to obtain improved Wikipedia article representation. We present comparison of our approach against a plethora of available solutions and show 8% improvement over state-of-the-art approaches with detailed ablation studies.",https://www.semanticscholar.org/paper/c44a14c16f0ae980c8075a3e8e83b41ce4a8d035,Yes,,DL
39,Quality of information sources about mental disorders: a comparison of Wikipedia with centrally controlled web and printed sources,"Background Although mental health information on the internet is often of poor quality, relatively little is known about the quality of websites, such as Wikipedia, that involve participatory information sharing. The aim of this paper was to explore the quality of user-contributed mental health-related information on Wikipedia and compare this with centrally controlled information sources. Method Content on 10 mental health-related topics was extracted from 14 frequently accessed websites (including Wikipedia) providing information about depression and schizophrenia, Encyclopaedia Britannica, and a psychiatry textbook. The content was rated by experts according to the following criteria: accuracy, up-to-dateness, breadth of coverage, referencing and readability. Results Ratings varied significantly between resources according to topic. Across all topics, Wikipedia was the most highly rated in all domains except readability. Conclusions The quality of information on depression and schizophrenia on Wikipedia is generally as good as, or better than, that provided by centrally controlled websites, Encyclopaedia Britannica and a psychiatry textbook.",https://www.semanticscholar.org/paper/53df4208e5b82af4f902a9255e51fa1616aec8ba,No,,Manual
41,History-Based Article Quality Assessment on Wikipedia,"Wikipedia is widely considered as the biggest encyclopedia on Internet. Quality assessment of articles on Wikipedia has been studied for years. Conventional methods addressed this task by feature engineering and statistical machine learning algorithms. However, manually defined features are difficult to represent the long edit history of an article. Recently, researchers proposed an end-to-end neural model which used a Recurrent Neural Network(RNN) to learn the representation automatically. Although RNN showed its power in modeling edit history, the end-to-end method is time and resource consuming. In this paper, we propose a new history-based method to represent an article. We also take advantage of an RNN to handle the long edit history, but we do not abandon feature engineering. We still represent each revision of an article by manually defined features. This combination of deep neural model and feature engineering enables our model to be both simple and effective. Experiments demonstrate our model has better or comparable performance than previous works, and has the potential to work as a real-time service. Plus, we extend our model to do quality prediction.",https://www.semanticscholar.org/paper/85323c0f359fd0ab782d07a048bd11dcb0ea5014,Yes,,DL + Features
42,A breakdown of quality flaws in Wikipedia,"The online encyclopedia Wikipedia is a successful example of the increasing popularity of user generated content on the Web. Despite its success, Wikipedia is often criticized for containing low-quality information, which is mainly attributed to its core policy of being open for editing by everyone. The identification of low-quality information is an important task since Wikipedia has become the primary source of knowledge for a huge number of people around the world. Previous research on quality assessment in Wikipedia either investigates only small samples of articles, or else focuses on single quality aspects, like accuracy or formality. This paper targets the investigation of quality flaws, and presents the first complete breakdown of Wikipedia's quality flaw structure. We conduct an extensive exploratory analysis, which reveals (1) the quality flaws that actually exist, (2) the distribution of flaws in Wikipedia, and (3) the extent of flawed content. An important finding is that more than one in four English Wikipedia articles contains at least one quality flaw, 70% of which concern article verifiability.",https://www.semanticscholar.org/paper/3218ef46bcf1f2b5c9c9f77fbf8a0a5d51744d99,Maybe,,
43,FlawFinder: A Modular System for Predicting Quality Flaws in Wikipedia,"With over 23 million articles in 285 languages, Wikipedia is the largest free knowledge base on the web. Due to its open nature, everybody is allowed to access and edit the contents of this huge encyclopedia. As a downside of this open access policy, quality assessment of the content becomes a critical issue and is hardly manageable without computational assistance. In this paper, we present FlawFinder, a modular system for automatically predicting quality flaws in unseen Wikipedia articles. It competed in the inaugural edition of the Quality Flaw Prediction Task at the PAN Challenge 2012 and achieved the best precision of all systems and the second place in terms of recall and F1-score.",https://www.semanticscholar.org/paper/275fb8e6cd6290db26d6c75bf7343eea6c90c806,Maybe,,
45,Wikipedia in Academic Studies: Corrupting or Improving the Quality of Teaching and Learning?,,https://www.semanticscholar.org/paper/bbcea5ee78d72d3d928e69c3ff79fa47e83e64d7,No,,"No access. Also book chapter, if we end up rejecting those."
46,Relative Quality and Popularity Evaluation of Multilingual Wikipedia Articles,"Despite the fact that Wikipedia is often criticized for its poor quality, it continues to be one of the most popular knowledge bases in the world. Articles in this free encyclopedia on various topics can be created and edited in about 300 different language versions independently. Our research has showed that in language sensitive topics, the quality of information can be relatively better in the relevant language versions. However, in most cases, it is difficult for the Wikipedia readers to determine the language affiliation of the described subject. Additionally, each language edition of Wikipedia can have own rules in the manual assessing of the content’s quality. There are also differences in grading schemes between language versions: some use a 6–8 grade system to assess articles, and some are limited to 2–3. This makes automatic quality comparison of articles between various languages a challenging task, particularly if we take into account a large number of unassessed articles; some of the Wikipedia language editions have over 99% of articles without a quality grade. The paper presents the results of a relative quality and popularity assessment of over 28 million articles in 44 selected language versions. Comparative analysis of the quality and the popularity of articles in popular topics was also conducted. Additionally, the correlation between quality and popularity of Wikipedia articles of selected topics in various languages was investigated. The proposed method allows us to find articles with information of better quality that can be used to automatically enrich other language editions of Wikipedia.",https://www.semanticscholar.org/paper/a37c5e68f712724157a4a6824f58a2425a830168,Maybe,,
47,Wikipedia model for collective intelligence: a review of information quality,"Online information seekers increasingly utilise the online encyclopaedia Wikipedia as a key reference source. Wikipedia's special feature is that it is based on the collective intelligence (CI) of lay citizens. Its consensus-building participatory knowledge-building processes replace traditional encyclopaedia processes founded on the knowledge of experts and gatekeeping practices. However there have been reports of concerns with the level of information quality provided by Wikipedia articles. This paper explores information quality for Wikipedia theoretically. First, it conceptualises the Wikipedia model of knowledge production and second, it analyses information quality for the model. Finally, the paper recommends some improvements for the model and discusses other implications for knowledge management theory and practice.",https://www.semanticscholar.org/paper/d6d558ea18117584d78f290ceaab2b2478d9889c,Maybe,,
48,A hybrid approach to classifying Wikipedia article quality flaws with feature fusion framework,"Article quality has always been a major concern for Wikipedia. To improve article quality, it is critical to first identify defects. Thus, flaw classification has attracted considerable attention. To achieve this, several machine-learning-based approaches are available, including deep learning models based on either manually constructed or autoextracted features. However, adopting only features of either single type may not ensure a comprehensive description of articles. To improve flaw classification, we propose a feature fusion framework combining both handcrafted and autoextracted features. In this research, we first use a rule-based method from a previously proposed framework to extract handcrafted features. Additionally, we obtain autoextracted features using Bidirectional Encoder Representations from Transformers (BERT) and various deep learning models, including bidirectional long short-term memory (Bi LSTM), bidirectional gated recurrent unit (Bi GRU), bidirectional recurrent neural network (Bi RNN), and multihead self-attention models. Finally, the handcrafted features are standardized and concatenated with the autoextracted features. Then, the concatenated features are fed into a feedforward neural network for classification. A detailed comparison of different classifiers is conducted. We compare 12 different classifiers in terms of training performance, classification performance, and model training time. The experiments show that the proposed feature fusion framework can notably improve the effectiveness of quality flaw classification for Wikipedia articles. In particular, a Bi GRU model based on the proposed framework achieves excellent classification accuracy.",https://www.semanticscholar.org/paper/88f8abd78cdeac3e003a4383d989a4239109375c,Yes,,DL + Features
49,Requirements for the Linguistic Quality Control of Wikipedia Article,"The article focuses on key Wikipedia issues. One of them concerning Wikipedia article quality management has been considered. The importance of professional linguistic skills has been emphasized. Taking into account information quality model and crucial characteristics of well-written text, the linguistic quality control model has been provided. Three groups with the relevant features of the model have been allocated. The basic requirements for the linguistic quality control model have been suggested.",https://www.semanticscholar.org/paper/e6b0bac35a9e9e0aeae6695a981ea2aa91a9fa70,No,,Focus
50,The Quality and Readability of English Wikipedia Anatomy Articles,"Forty anatomy articles were sampled from English Wikipedia and assessed quantitatively and qualitatively. Quantitatively, each article’s edit history was analyzed by Wikipedia X‐tools, references and media were counted manually, and two readability indices were used to evaluate article readability. This analysis revealed that each article was updated 8.3 ± 6.8 times per month, and referenced with 33.5 ± 24.3 sources, such as journal articles and textbooks. Each article contained on average 14.0 ± 7.6 media items. The readability indices including: (1) Flesch–Kincaid Grade Level Readability Test and (2) Flesch Reading Ease Readability Formula demonstrated that the articles had low readability and were more appropriate for college students and above. Qualitatively, the sampled articles were evaluated by experts using a modified DISCERN survey. According to the modified DISCERN, 13 articles (32.5%), 24 articles (60%), 3 articles (7.5%), were rated as “good,” “moderate,” and “poor,” respectively. There were positive correlations between the DISCERN score and the number of edits (r = 0.537), number of editors (r = 0.560), and article length (r = 0.536). Strengths reported by the panel included completeness and coverage in 11 articles (27.5%), anatomical details in 10 articles (25%), and clinical details in 5 articles (12.5%). The panel also noted areas which could be improved, such as providing missing information in 28 articles (70%), inaccuracies in 10 articles (25%), and lack or poor use of images in 17 articles (42.5%). In conclusion, this study revealed that many Wikipedia anatomy articles were difficult to read. Each article’s quality was dependent on edit frequency and article length. Learners and students should be cautious when using Wikipedia articles for anatomy education due to these limitations.",https://www.semanticscholar.org/paper/b19f3cf7ec13c083f067875a80667c2ad540d6fc,No,,Manual
51,Readability and quality of wikipedia pages on neurosurgical topics,"Objectives: Wikipedia is the largest online encyclopedia with over 40 million articles, and generating 500 million visits per month. The aim of this study is to assess the readability and quality of Wikipedia pages on neurosurgical related topics.   Patients and Methods: We selected the neurosurgical related Wikipedia pages based on the series of online patient information articles that are published by the American Association of Neurological Surgeons (AANS). We assessed readability of Wikipedia pages using five different readability scales (Flesch Reading Ease, Flesch Kincaid Grade Level, Gunning Fog Index, SMOG) Grade level, and Coleman-Liau Index). We used the Center for Disease Control (CDC) Clear Communication Index as well as the DISCERN Instrument to evaluate the quality of each Wikipedia article.   Results: We identified a total of fifty-five Wikipedia articles that corresponded with patient information articles published by the AANS. This constitutes 77.46% of the AANS topics. The mean Flesch Kincaid reading ease score for all of the Wikipedia articles we analyzed is 31.10, which indicates that a college-level education is necessary to understand them. In comparison to the readability analysis for the AANS articles, the Wikipedia articles were more difficult to read across every scale. None of the Wikipedia articles meet the CDC criterion for clear communications.   Conclusion: Our analyses demonstrated that Wikipedia articles related to neurosurgical topics are associated with higher grade levels for reading and also below the expected levels of clear communications for patients. Collaborative efforts from the neurosurgical community are needed to enhance the readability and quality of Wikipedia pages related to neurosurgery.",https://www.semanticscholar.org/paper/4364ec858a65cd9edae04e3fa6672613428c5423,No,,Manual
52,The Quality of Open Source Production: Zealots and Good Samaritans in the Case of Wikipedia,"New forms of production based in electronic technology, such as open-source and opencontent production, convert private commodities (typically software) into essentially public goods. A number of studies find that, like in other collective goods, incentives for reputation and group identity motivate contributions to open source goods, thereby overcoming the social dilemma inherent in producing such goods. In this paper we examine how contributor motivations affect the quality of contributions to the opencontent online encyclopedia Wikipedia. We find that quality is associated with contributor motivations, but in a surprisingly inconsistent way. Registered users’ quality increases with more contributions, consistent with the idea of participants motivated by reputation and commitment to the Wikipedia community. Surprisingly, however, we find the highest quality from the vast numbers of anonymous “Good Samaritans” who contribute only once. Our findings that Good Samaritans as well as committed “zealots” contribute high quality content to Wikipedia suggest that it is the quantity as well as the quality of contributors that positively affects the quality of open source production.",https://www.semanticscholar.org/paper/ecddad562c391d87380de95c2053c3642afdb17c,No,,Not article (report)
53,Wikipedia and Westminster: Quality and Dynamics of Wikipedia Pages about UK Politicians,"Wikipedia is a major source of information providing a large variety of content online, trusted by readers from around the world. Readers go to Wikipedia to get reliable information about different subjects, one of the most popular being living people, and especially politicians. While a lot is known about the general usage and information consumption on Wikipedia, less is known about the life-cycle and quality of Wikipedia articles in the context of politics. The aim of this study is to quantify and qualify content production and consumption for articles about politicians, with a specific focus on UK Members of Parliament (MPs). First, we analyze spatio-temporal patterns of readers' and editors' engagement with MPs' Wikipedia pages, finding huge peaks of attention during election times, related to signs of engagement on other social media (e.g. Twitter). Second, we quantify editors' polarisation and find that most editors specialize in a specific party and choose specific news outlets as references. Finally we observe that the average citation quality is pretty high, with statements on 'Early life and career' missing citations most often (18%).",https://www.semanticscholar.org/paper/8b3292600f9b7e1d5a1fa5ff7ac2edd49380eb59,Maybe,,
54,Network analysis of user generated content quality in Wikipedia,"Purpose – Social media platforms allow near‐unfettered creation and exchange of user generated content (UGC). Drawing from network science, the purpose of this paper is to examine whether high and low quality UGC differ in their connectivity structures in Wikipedia (which consists of interconnected user generated articles). Design/methodology/approach – Using Featured Articles as a proxy for high quality, a network analysis was undertaken of the revision history of six different language Wikipedias, to offer a network‐centric explanation for the emergence of quality in UGC.Findings – The network structure of interactions between articles and contributors plays an important role in the emergence of quality. Specifically the analysis reveals that high‐quality articles cluster in hubs that span structural holes.Research limitations/implications – The analysis does not capture the strength of interactions between articles and contributors. The implication of this limitation is that quality is viewed as a binary variable. Extensions to this research will relate strength of interactions to different levels of quality in UGC. Practical implications - The findings help harness the “wisdom of the crowds” effectively. Organisations should nurture users and articles at the structural hubs from an early stage. This can be done through appropriate design of collaborative knowledge systems and development of organisational policies to empower hubs. Originality/value - The network centric perspective on quality in UGC and the use of a dynamic modelling tool are novel. The paper is of value to researchers in the area of social computing and to practitioners implementing and maintaining such platforms in organisations.",https://www.semanticscholar.org/paper/53013037649e17b1194ae579a9a6e20bd7bf3e0d,Maybe,,
55,Improving the Quality of Consumer Health Information on Wikipedia: Case Series,"Background - Wikipedia is one of the most consulted health resources in the world. Since the public is using health information from Wikipedia to make health care decisions, improving the quality of that health information is in the public interest. The open editable content design of Wikipedia and quality control processes in place provide an opportunity to add high-value, evidence-based information and take an active role in improving the health care information infrastructure. Objective - The aim of this project was to enhance Wikipedia health pages using high-quality, current research findings and track the persistence of those edits and number of page views after the changes to assess the reach of this initiative. Methods - We conducted Wikipedia Editathons with 3 different cohorts of Physical Therapy (PT) students to add high-quality health information to existing Wikipedia pages. Students synthesized best evidence information and updated and/or corrected existing Wikipedia entries on specific health pages. To evaluate the impact of these contributions, we examined two factors: (1) response to our contributions from the Wikipedia editing community, including number and type of subsequent edits as well as persistence of the student contributions and (2) number of page views by the public from the time of the page edits. Results A total of 98 PT students in 3 different cohorts engaged in Editathons, editing 24 health pages. Of the 24 edits, 22 persisted at the end of the observation period (from time of entry to May 31, 2018) and received nearly 8 million page views. Each health page had an average of 354,724 page views. Conclusions The Wikipedia Editathon is an effective way to continuously enhance the quality of health information available on Wikipedia. It is also an excellent way of bridging health technology with best-evidence medical facts and disseminating accurate, useful information to the public.",https://www.semanticscholar.org/paper/967262de8d06ad0a0c182dc826a5a7bda4397d2f,No,,Manual
56,Quality in Internet Collective Goods : Zealots and Good Samaritans in the Case of Wikipedia,"One important innovation in information and communication technology developed over the past decade was organizational rather than merely technological. Open source production is remarkable because it converts a private commodity (typically software) into a public good. A number of studies examine the factors motivating contributions to open source production goods, but we argue it is important to understand the causes of high quality contributions to such goods. In this paper, we analyze quality in the open source online encyclopedia Wikipedia. We find that, for users who create an online persona through a registered user name, the quality of contributions increases as the number of contributions increase, consistent with the idea of experts motivated by reputation and committed to the Wikipedia community. Unexpectedly, however, we find the highest quality contributions come from the vast numbers of anonymous "" Good Samaritans "" who contribute infrequently. Our findings that Good Samaritans as well as committed "" Zealots "" contribute high quality content to Wikipedia suggest that open source production is remarkable as much for its organizational as its technological innovation that enables vast numbers of anonymous one-time contributors to create high quality, essentially public goods.",https://www.semanticscholar.org/paper/da03cd44d8a163a185bd6e9d040c2ce22fc0ad14,No,,
57,Knowledge categorization affects popularity and quality of Wikipedia articles,"The existence of a shared classification system is essential to knowledge production, transfer, and sharing. Studies of knowledge classification, however, rarely consider the fact that knowledge categories exist within hierarchical information systems designed to facilitate knowledge search and discovery. This neglect is problematic whenever information about categorical membership is itself used to evaluate the quality of the items that the category contains. The main objective of this paper is to show that the effects of category membership depend on the position that a category occupies in the hierarchical knowledge classification system of Wikipedia—an open knowledge production and sharing platform taking the form of a freely accessible on-line encyclopedia. Using data on all English-language Wikipedia articles, we examine how the position that a category occupies in the classification hierarchy affects the attention that articles in that category attract from Wikipedia editors, and their evaluation of quality of the Wikipedia articles. Specifically, we show that Wikipedia articles assigned to coarse-grained categories (i. e., categories that occupy higher positions in the hierarchical knowledge classification system) garner more attention from Wikipedia editors (i. e., attract a higher volume of text editing activity), but receive lower evaluations (i. e., they are considered to be of lower quality). The negative relation between attention and quality implied by this result is consistent with current theories of social categorization, but it also goes beyond available results by showing that the effects of categorization on evaluation depend on the position that a category occupies in a hierarchical knowledge classification system.",https://www.semanticscholar.org/paper/fa1db666c361fd03226c8e7bd81c5eb515719ac6,Yes,,
58,On improving wikipedia search using article quality,"Wikipedia is presently the largest free-and-open online encyclopedia collaboratively edited and maintained by volunteers. While Wikipedia offers full-text search to its users, the accuracy of its relevance-based search can be compromised by poor quality articles edited by non-experts and inexperienced contributors. In this paper, we propose a framework that re-ranks Wikipedia search results considering article quality. We develop two quality measurement models, namely Basic and PeerReview, to derive article quality based on co-authoring data gathered from articles' edit history. Compared with Wikipedia's full-text search engine, Google and Wikiseek, our experimental results showed that (i) quality-only ranking produced by PeerReview gives comparable performance to that of Wikipedia and Wikiseek; (ii) PeerReview combined with relevance ranking outperforms Wikipedia's full-text search significantly, delivering search accuracy comparable to Google.",https://www.semanticscholar.org/paper/2b05d442c55216c5819d0924016989f8dec3b23b,Yes,,
59,Computational Trust in Web Content Quality: A Comparative Evalutation on the Wikipedia Project,"The problem of identifying useful and trustworthy information on the World Wide Web is becoming increasingly acute as new tools such as wikis and blogs simplify and democratize publication. It is not hard to predict that in the future the direct reliance on this material will expand and the problem of evaluating the trustworthiness of this kind of content become crucial. The Wikipedia project represents the most successful and discussed example of such online resources. In this paper we present a method to predict Wikipedia articles trustworthiness based on computational trust techniques and a deep domain-specific analysis. Our assumption is that a deeper understanding of what in general defines high-standard and expertise in domains related to Wikipedia – i.e. content quality in a collaborative environment – mapped onto Wikipedia elements would lead to a complete set of mechanisms to sustain trust in Wikipedia context. We present a series of experiment. The first is a study-case over a specific category of articles; the second is an evaluation over 8 000 articles representing 65% of the overall Wikipedia editing activity. We report encouraging results on the automated evaluation of Wikipedia content using our domain-specific expertise method. Finally, in order to appraise the value added by using domain-specific expertise, we compare our results with the ones obtained with a pre-processed cluster analysis, where complex expertise is mostly replaced by training and automatic classification of common features.",https://www.semanticscholar.org/paper/2bcb4e038b95d2ac3e7019580529af9a9be1e9d8,Yes,,
60,Mutual evaluation of editors and texts for assessing quality of Wikipedia articles,"In this paper, we propose a method to identify good quality Wikipedia articles by mutually evaluating editors and texts. A major approach for assessing article quality is a text survival ratio based approach. In this approach, when a text survives beyond multiple edits, the text is assessed as good quality. This approach assumes that poor quality texts are deleted by editors with high possibility. However, many vandals delete good quality texts frequently, then the survival ratios of good quality texts are improperly decreased by vandals. As a result, many good quality texts are unfairly assessed as poor quality. In our method, we consider editor quality for calculating text quality, and decrease the impacts on text qualities by the vandals who has low quality. Using this improvement, the accuracy of the text quality should be improved. However, an inherent problem of this idea is that the editor qualities are calculated by the text qualities. To solve this problem, we mutually calculate the editor and text qualities until they converge. We did our experimental evaluation, and we confirmed that the proposed method could accurately assess the text qualities.",https://www.semanticscholar.org/paper/6e56f240fc876a9ac99b8377108672015142e708,Maybe,,
61,Mining the Factors Affecting the Quality of Wikipedia Articles,"In order to observe the variation of factors affecting the quality of Wikipedia articles during the information quality improvement process, we proposed 28 metrics from four aspects, including lingual, structural, historical and reputational features, and then weighted each metrics indifferent stages by using neural network. We found lingual features weighted more in the lower quality stages, and structural features, along with historical features, became more important while article quality improved. However, reputational features did not act as important as expected. The findings indicate that the information quality is mainly affected by completeness, and well-written is a basic requirement in the initial stage. Reputation of authors or editors is not so important in Wikipedia because of its horizontal structure.",https://www.semanticscholar.org/paper/cc68c28bc765082fa85c23fb71d7cdf212895185,Yes,,
62,Measures for Quality Assessment of Articles and Infoboxes in Multilingual Wikipedia,"One of the most popular collaborative knowledge bases on the Internet is Wikipedia. Articles of this free encyclopaedia are created and edited by users from different countries in about 300 languages. Depending on topic and language version, quality of information there may vary. This study presents and classifies measures that can be extracted from Wikipedia articles for the purpose of automatic quality assessment in different languages. Based on a state of the art analysis and own experiments, specific measures for various aspects of quality have been defined. Additional, in this work they were also defined measures for quality assessment of data contained in the structural parts of Wikipedia articles - infoboxes. This study describes also an extraction methods for various sources of measures, that can be used in quality assessment.",https://www.semanticscholar.org/paper/3eaffa79334d8a91267b800d34c56bb41faa3c5b,Yes,,
63,Measuring the quality of scientific references in Wikipedia: an analysis of more than 115M citations to over 800 000 scientific articles,"Wikipedia is a widely used online reference work which cites hundreds of thousands of scientific articles across its entries. The quality of these citations has not been previously measured, and such measurements have a bearing on the reliability and quality of the scientific portions of this reference work. Using a novel technique, a massive database of qualitatively described citations, and machine learning algorithms, we analyzed 1 923 575 Wikipedia articles which cited a total of 824 298 scientific articles in our database and found that most scientific articles cited by Wikipedia articles are uncited or untested by subsequent studies, and the remainder show a wide variability in contradicting or supporting evidence. Additionally, we analyzed 51 804 643 scientific articles from journals indexed in the Web of Science and found that similarly most were uncited or untested by subsequent studies, while the remainder show a wide variability in contradicting or supporting evidence.",https://www.semanticscholar.org/paper/76e53f6e46b7b8a8ecfe735a23d4531dbd241911,No,,Analyses quality of references instead of articles
64,Classifying Wikipedia Article Quality With Revision History Networks,"We present a novel model for classifying the quality of Wikipedia articles based on structural properties of a network representation of the article's revision history. We create revision history networks (an adaptation of Keegan et. al's article trajectory networks [7]), where nodes correspond to individual editors of an article, and edges join the authors of consecutive revisions. Using descriptive statistics generated from these networks, along with general properties like the number of edits and article size, we predict which of six quality classes (Start, Stub, C-Class, B-Class, Good, Featured) articles belong to, attaining a classification accuracy of 49.35% on a stratified sample of articles. These results suggest that structures of collaboration underlying the creation of articles, and not just the content of the article, should be considered for accurate quality classification.",https://www.semanticscholar.org/paper/8a74f75cb449bff4833be8f77ccf5f2493d75a19,Yes,,
65,WikipediaViz: Conveying article quality for casual Wikipedia readers,"As Wikipedia has become one of the most used knowledge bases worldwide, the problem of the trustworthiness of the information it disseminates becomes central. With WikipediaViz, we introduce five visual indicators integrated to the Wikipedia layout that can keep casual Wikipedia readers aware of important metainformation about the articles they read. The design of WikipediaViz was inspired by two participatory design sessions with expert Wikipedia writers and sociologists who explained the clues they used to quickly assess the trustworthiness of articles. According to these results, we propose five metrics for Maturity and Quality assessment of Wikipedia articles and their accompanying visualizations to provide the readers with important clues about the editing process at a glance. We also report and discuss about the results of the user studies we conducted. Two preliminary pilot studies show that all our subjects trust Wikipedia articles almost blindly. With the third study, we show that WikipediaViz significantly reduces the time required to assess the quality of articles while maintaining a good accuracy.",https://www.semanticscholar.org/paper/07a813d7de9ac072ed970217024a51a835433654,Yes,,Visualization
66,Multilingual Ranking of Wikipedia Articles with Quality and Popularity Assessment in Different Topics,"On Wikipedia, articles about various topics can be created and edited independently in each language version. Therefore, the quality of information about the same topic depends on the language. Any interested user can improve an article and that improvement may depend on the popularity of the article. The goal of this study is to show what topics are best represented in different language versions of Wikipedia using results of quality assessment for over 39 million articles in 55 languages. In this paper, we also analyze how popular selected topics are among readers and authors in various languages. We used two approaches to assign articles to various topics. First, we selected 27 main multilingual categories and analyzed all their connections with sub-categories based on information extracted from over 10 million categories in 55 language versions. To classify the articles to one of the 27 main categories, we took into account over 400 million links from articles to over 10 million categories and over 26 million links between categories. In the second approach, we used data from DBpedia and Wikidata. We also showed how the results of the study can be used to build local and global rankings of the Wikipedia content.",https://www.semanticscholar.org/paper/d5b3f3e9403eda8b45184951b269ee7997452c6b,Yes,,
67,Automatically assessing the quality of Wikipedia contents,"With the development of Web 2.0 technologies, people have gone from being mere content users to content generators. In this context, the evaluation of the quality of (potential) information available online has become a crucial issue. Nowadays, one of the biggest online resources that users rely on as a knowledge base is Wikipedia. The collaborative aspect at the basis of Wikipedia can let to the possible creation of low-quality articles or even misinformation if the process of monitoring the generation and the revision of articles is not performed in a precise and timely way. For this reason, in this paper, the problem of automatically evaluating the quality of Wikipedia contents is considered, by proposing a supervised approach based on Machine Learning to perform the classification of articles on qualitative bases. With respect to prior literature, a wider set of features connected to Wikipedia articles has been taken into account, as well as previously unconsidered aspects connected to the generation of a labeled dataset to train the model, and the use of Gradient Boosting, which produced encouraging results.",https://www.semanticscholar.org/paper/2a9be636902fe22e4d04d4f9721e4e914512e3d7,Yes,,
68,Pharmacy students can improve access to quality medicines information by editing Wikipedia articles,"Background: Pharmacy training programs commonly ask students to develop or edit drug monographs that summarize key information about new medicines as an academic exercise. We sought to expand on this traditional approach by having students improve actual medicines information pages posted on Wikipedia. Methods: We placed students (n = 119) in a required core pharmacy course into groups of four and assigned each group a specific medicines page on Wikipedia to edit. Assigned pages had high hit rates, suggesting that the topics were of interest to the wider public, but were of low quality, suggesting that the topics would benefit from improvement efforts. We provided course trainings about editing Wikipedia. We evaluated the assignment by surveying student knowledge and attitudes and reviewing the edits on Wikipedia. Results:Completing the course trainings increased student knowledge of Wikipedia editing practices. At the end of the assignment, students had a more nuanced understanding of Wikipedia as a resource. Student edits improved substantially the quality of the articles edited, their edits were retained for at least 30 days after course completion, and the average number page views of their edited articles increased. Conclusions:Our results suggest that engaging pharmacy students in a Wikipedia editing assignment is a feasible alternative to writing drug monographs as a classroom assignment. Both tasks provide opportunities for students to demonstrate their skills at researching and explaining drug information but only one serves to improve wider access to quality medicines information. Wikipedia editing assignments are feasible for large groups of pharmacy students and effective in improving publicly available information on one of the most heavily accessed websites globally.",https://www.semanticscholar.org/paper/3ad3f5e0fdc59499a4b7d40df50b61f9e3ade7d2,No,,Manual
69,Wisdom of crowds: the effect of participant composition and contribution behavior on Wikipedia article quality,"This paper aims to explore the effect of participant composition and contribution behavior of the different types of participants on the quality of knowledge generation in online communities.,This study samples all the featured articles in Chinese Wikipedia and performs a Cox regression to reveal how participant composition and contribution behavior affect the quality of articles in different contexts.,The results show that an increase in the number of participants increases the possibility of either enhancing or reducing the article quality. In most cases, the greater the proportion of core members (people who frequently participate in editing), the higher the possibility of enhancing the article quality. Occasional participants’ editorial behavior hinders quality promotion, this negative effect weakens when such editorial behavior becomes more frequent.,The findings help to better leverage the role of online communities in practice and to achieve knowledge collaboration in a more efficient manner. For example, an appropriate centralized organizational form should be established in online communities to improve the efficiency of crowd contributions. And it is worth developing mechanism to encourage participants to frequently participate in editing the article.,This study contributes to the research on the organizational forms of online communities by showing the effect of participant composition and behavior in the new form of organizing on knowledge generation. This study also contributes to the research on wisdom of crowds by revealing who in a group of participants, in what context, and by what means influence knowledge generation.",https://www.semanticscholar.org/paper/da5790edd4224160d1da8107ba083736c9b8683e,Yes,,
70,Quality Evaluation of Wikipedia Articles through Edit History and Editor Groups,"Wikipedia is well known as a free encyclopedia, which is a type of collaborative repository system that allows the viewer to create and edit articles directly in the web browser. The weakness of the Wikipedia system is the possibility of manipulation and vandalism cannot be ruled out, so that the quality of any given Wikipedia article is not guaranteed. It is an important work to establish a quality evaluation method to help users decide how much they should trust an article in Wikipedia. In this paper we investigate the edit history of Wikipedia articles and propose a model of network structure of editors. We propose an algorithm to calculate the network structural indicator restoreratio. We use the proposed indicator combined with existing metrics to predict the quality of Wikipedia articles through support vector machine technology. The experimental results show that the proposed indicator has better performance in quality evaluation than several existing metrics.",https://www.semanticscholar.org/paper/527699259b21d363d57ac38db4986bb433c3ded5,Yes,,
71,Application of SEO Metrics to Determine the Quality of Wikipedia Articles and Their Sources,"The leading online encyclopedia Wikipedia is struggling with inconsistent article quality caused by the collaborative editing model. While one can find many helpful articles with consistent information on Wikipedia, there are also a lot of questionable articles with unclear or unfinished information yet. The quality of each article may vary over time as different users repeatedly re-edit content. One of the most important elements of the Wikipedia articles are references which allow to verify content and to show its source to user. Based on the fact that most of these references are web pages, it is possible to get more information about their quality by using citation analysis tools. For science and practice the empirical proof of the quality of the articles in Wikipedia could have a further signal effect, as the citation of Wikipedia articles, especially in scientific practice, is not yet recognised. This paper presents general results of Wikipedia analysis using metrics from the Toolbox SISTRIX, which is one of the leading providers of indicators for Search Engine Optimization (SEO). In addition to the preliminary analysis of the Wikipedia articles as separate web pages, we extracted data from more than 30 million references in different language versions of Wikipedia and analyzed over 180 thousand most popular hosts. In addition, we compared the same sources from different geographical perspectives using country-specific visibility indices.",https://www.semanticscholar.org/paper/662ee60986d028487b4d06fd970e6645aa03728d,Yes,,
72,Diversity of editors and teams versus quality of cooperative work: experiments on wikipedia,"We study whether and how the diversity of editors and teams affects the quality of work in a virtual cooperative work environment on the Wikipedia example. We propose a measure of interests diversity of an editor and some measures of team diversity in terms of members’ interests and experience. Statistical and machine learning methods are used to investigate the dependency between diversity and work quality. The presented experimental results confirm our hypothesis that interest diversity of a single editors and team diversity are positively related to the quality of their work. Interestingly, some of our experiments also indicate that diversity may be more important than such attributes as productivity of an editor or size or experience of the team. Our experimental results demonstrate that it is possible to predict work quality based on diversity which is an additional statistical signal that diversity is correlated with work quality.",https://www.semanticscholar.org/paper/4e5cdf30ab1fbd13404068c1c7894f8eeb33d0bd,Maybe,,
73,Readability and quality of Wikipedia articles on pelvic floor disorders,"This study is aimed at evaluating the readability and quality of Wikipedia articles on pelvic floor disorders (PFD) and comparing their content with International Urogynecological Association patient education leaflets. Readability was assessed using six different readability scales, including the Simple Measure of Gobbledygook (SMOG) Index, which is considered superior for scoring healthcare information. Quality was assessed by three female pelvic medicine and reconstructive surgery fellows using the modified DISCERN instrument. DISCERN is validated to evaluate the quality of written consumer health information; it was subsequently modified by health education researchers to enable the evaluation of Wikipedia articles. We evaluated 30 Wikipedia articles that correlated with 29 International Urogynecological Association leaflets. The mean SMOG score of the Wikipedia articles was 12.0 ± 2.1 (12th-grade reading level) whereas the mean SMOG score of the International Urological Association (IUGA) leaflets was 3.4 ± 0.3 (third-grade reading level, p < 0.001). The mean modified DISCERN score of the Wikipedia articles was 34.43 ± 5.90 (moderate quality); however, the mean modified DISCERN score of the IUGA literature was 45.02 ± 1.36 (good quality, p < 0.001). Wikipedia articles on PFD are neither readable nor reliable: they require a 12th-grade-level education for comprehension and are merely rated moderate in quality. In comparison, IUGA leaflets require a third-grade education for comprehension and are rated good in quality. Urogynecological providers should provide appropriate health education materials to patients, as Wikipedia is both a popular and sometimes inaccurate resource for patients.",https://www.semanticscholar.org/paper/a6deb56a4f7ffb0859002c508ba9754b45906080,No,,Manual
74,A deep learning-based quality assessment model of collaboratively edited documents: A case study of Wikipedia,"Wikipedia is becoming increasingly critical in helping people obtain information and knowledge. Its leading advantage is that users can not only access information but also modify it. However, this presents a challenging issue: how can we measure the quality of a Wikipedia article? The existing approaches assess Wikipedia quality by statistical models or traditional machine learning algorithms. However, their performance is not satisfactory. Moreover, most existing models fail to extract complete information from articles, which degrades the model’s performance. In this article, we first survey related works and summarise a comprehensive feature framework. Then, state-of-the-art deep learning models are introduced and applied to assess Wikipedia quality. Finally, a comparison among deep learning models and traditional machine learning models is conducted to validate the effectiveness of the proposed model. The models are compared extensively in terms of their training and classification performance. Moreover, the importance of each feature and the importance of different feature sets are analysed separately.",https://www.semanticscholar.org/paper/0abf43df2e9c0fee1262e9d127c05383ffc78251,Yes,,
75,Overview of the 1th International Competition on Quality Flaw Prediction in Wikipedia,"The paper overviews the task ""Quality Flaw Prediction in Wikipedia"" of the PAN'12 competition. An evaluation corpus is introduced which comprises 1 592 226 English Wikipedia articles, of which 208 228 have been tagged to con- tain one of ten important quality flaws. Moreover, the performance of three qual- ity flaw classifiers is evaluated.",https://www.semanticscholar.org/paper/d87a2a5d19419259895a23ff0003092a094518f2,Yes,,
76,Automatically Labeling Low Quality Content on Wikipedia By Leveraging Patterns in Editing Behaviors,"Wikipedia articles aim to be definitive sources of encyclopedic content. Yet, only 0.6% of Wikipedia articles have high quality according to its quality scale due to insufficient number of Wikipedia editors and enormous number of articles. Supervised Machine Learning (ML) quality improvement approaches that can automatically identify and fix content issues rely on manual labels of individual Wikipedia sentence quality. However, current labeling approaches are tedious and produce noisy labels. Here, we propose an automated labeling approach that identifies the semantic category (e.g., adding citations, clarifications) of historic Wikipedia edits and uses the modified sentences prior to the edit as examples that require that semantic improvement. Highest-rated article sentences are examples that no longer need semantic improvements. We show that training existing sentence quality classification algorithms on our labels improves their performance compared to training them on existing labels. Our work shows that editing behaviors of Wikipedia editors provide better labels than labels generated by crowdworkers who lack the context to make judgments that the editors would agree with.",https://www.semanticscholar.org/paper/65063772f09683c581f281ec3aa640549cd9aaee,Yes,,
77,Your process is showing: controversy management and perceived quality in wikipedia,"Large-scale collaboration systems often separate their content from the deliberation around how that content was produced. Surfacing this deliberation may engender trust in the content generation process if the deliberation process appears fair, well-reasoned, and thorough. Alternatively, it could encourage doubts about content quality, especially if the process appears messy or biased. In this paper we report the results of an experiment where we found that surfacing deliberation generally led to decreases in perceptions of quality for the article under consideration, especially - but not only - if the discussion revealed conflict. The effect size depends on the type of editors' interactions. Finally, this decrease in actual article quality rating was accompanied by self-reported improved perceptions of the article and Wikipedia overall.",https://www.semanticscholar.org/paper/d38b8b87051c3597811385018593daa3e491ac14,No,,Manual
78,Article quality classification on Wikipedia: introducing document embeddings and content features,"The quality of articles on the Wikipedia platform is vital for its success. Currently, the assessment of quality is performed manually by the Wikipedia community, where editors classify articles into pre-defined quality classes. However, this approach is hardly scalable and hence, approaches for the automatic classification have been investigated. In this paper, we extend this previous line of research on article quality classification by extending the set of features with novel content and edit features (e.g., document em-beddings of articles). We propose a classification approach utilizing gradient boosted trees based on this novel, extended set of features extracted from Wikipedia articles. Based on an established dataset containing Wikipedia articles and quality classes, we show that our approach is able to substantially outperform previous approaches (also including recent deep learning methods). Furthermore, we shed light on the contribution of individual features and show that the proposed features indeed capture the quality of an article well.",https://www.semanticscholar.org/paper/beb2803e597f83e85c76f6db6834bb88a7aa8f6b,Yes,,
79,Assessing quality score of Wikipedia article using mutual evaluation of editors and texts,"In this paper, we propose a method for assessing quality scores of Wikipedia articles by mutually evaluating editors and texts. Survival ratio based approach is a major approach to assessing article quality. In this approach, when a text survives beyond multiple edits, the text is assessed as good quality, because poor quality texts have a high probability of being deleted by editors. However, many vandals, low quality editors, delete good quality texts frequently, which improperly decreases the survival ratios of good quality texts. As a result, many good quality texts are unfairly assessed as poor quality. In our method, we consider editor quality score for calculating text quality score, and decrease the impact on text quality by vandals. Using this improvement, the accuracy of the text quality score should be improved. However, an inherent problem with this idea is that the editor quality scores are calculated by the text quality scores. To solve this problem, we mutually calculate the editor and text quality scores until they converge. In this paper, we prove that the text quality score converges. We did our experimental evaluation, and confirmed that our proposed method could accurately assess the text quality scores.",https://www.semanticscholar.org/paper/68b167406f708ce39421126bdbb576930c8347a0,Maybe,,
80,Measuring the Quality of Edits to Wikipedia,"Wikipedia is unique among reference works both in its scale and in the openness of its editing interface. The question of how it can achieve and maintain high-quality encyclopedic articles is an area of active research. In order to address this question, researchers need to build consensus around a sensible metric to assess the quality of contributions to articles. This measure must not only reflect an intuitive concept of ""quality,"" but must also be scalable and run efficiently. Building on prior work in this area, this paper uses human raters through Amazon Mechanical Turk to validate an efficient, automated quality metric.",https://www.semanticscholar.org/paper/cd8748fd30092dff2739a2471cbefc78c25e1e53,Yes,,
81,Mining and Predicting Temporal Patterns in the Quality Evolution of Wikipedia Articles,"Online open collaboration systems like Wikipedia are complex adaptive systems within which large numbers of individual agents and artifacts interact and co-evolve over time. A key issue in these systems is the quality of the co-created artifacts and the processes through which high-quality artifacts are produced. In this paper, we took a dynamic approach to uncover common patterns in the temporal evolution of 6,057 Wikipedia articles in the domains of roads, films, and battles. Using Dynamic Time Warping, an advanced time-series clustering method, we identified three distinctive growth patterns, namely, stalled, plateaued, and sustained. Multinomial logistic regressions to predict these different clusters suggest that the path that an article follows is determined by both its inherent attributes, such as topic importance, and the contribution and coordination of editors who collaborated on the article. Our results also suggest that different factors matter at different stages of an article’s life cycle.",https://www.semanticscholar.org/paper/0eed508e65f53dd92fa129523129664879695849,Maybe,,
82,Quality Assessment of Wikipedia Articles Using h-index,"In this paper, we propose a method for assessing quality values of Wikipedia articles from edit history using h-index. One of the major methods for assessing Wikipedia article quality is a peer-review based method. In this method, we assume that if an editor’s texts are left by the other editors, the texts are approved by the editors, then the editor is decided as a good editor. However, if an editor edits multiple articles, and the editor is approved at a small number of articles, the quality value of the editor deeply depends on the quality of the texts. In this paper, we apply h-index, which is a simple but resistant to excessive values, to the peer-review based Wikipedia article assessment method. Although h-index can identify whether an editor is a good quality editor or not, h-index cannot identify whether the editor is a vandal or an inactive editor. To solve this problem, we propose p-ratio for identifying which editors are vandals or inactive editors. From our experiments, we confirmed that by integrating h-index with p-ratio, the accuracy of article quality assessment in our method outperforms the existing peer-review based method.",https://www.semanticscholar.org/paper/18827d99db45b45c9b5c00bd05508fbf524e32b7,Yes,,
83,WikiLyzer: Interactive Information Quality Assessment in Wikipedia,"Digital libraries and services enable users to access large amounts of data on demand. Yet, quality assessment of information encountered on the Internet remains an elusive open issue. For example, Wikipedia, one of the most visited platforms on the Web, hosts thousands of user-generated articles and undergoes 12 million edits/contributions per month. User-generated content is undoubtedly one of the keys to its success, but also a hindrance to good quality: contributions can be of poor quality because anyone, even anonymous users, can participate. Though Wikipedia has defined guidelines as to what makes the perfect article, authors find it difficult to assert whether their contributions comply with them and reviewers cannot cope with the ever growing amount of articles pending review. Great efforts have been invested in algorithmic methods for automatic classification of Wikipedia articles (as featured or non-featured) and for quality flaw detection. However, little has been done to support quality assessment of user-generated content through interactive tools that combine automatic methods and human intelligence. We developed WikiLyzer, a Web toolkit comprising three interactive applications designed to assist (i) knowledge discovery experts in creating and testing metrics for quality measurement, (ii) Wikipedia users searching for good articles, and (iii) Wikipedia authors that need to identify weaknesses to improve a particular article. A design study sheds a light on how experts could create complex quality metrics with our tool, while a user study reports on its usefulness to identify high-quality content.",https://www.semanticscholar.org/paper/452ca7a696f5ea260f0b334c67ef3b71a05c676b,Maybe,,Visualization
84,"‘WP2Cochrane’, a tool linking Wikipedia to the Cochrane Library: Results of a bibliometric analysis evaluating article quality and importance","Medical information on English Wikipedia was accessed over 2 billion times in 2018. Our goal was to develop an automated system to assist Wikipedia volunteers to improve articles with high-quality sources from journals such as The Cochrane Library. We created an automated indexing system by linking available reviews from the Cochrane library with disease-related Wikipedia articles and evaluating the relationship between the quality and importance of these articles with the number of relevant and cited Cochrane reviews. We first conducted a bibliometric analysis, identifying disease-related Wikipedia articles and relevant/cited Cochrane reviews. Citations were thematically coded, and descriptive statistics were calculated. Finally, separate multinomial logistic regression analyses were conducted for article quality and importance. The indexing system identified 4381 disease-related Wikipedia articles, 1193 (27%) of which cited a Cochrane review. Higher quality Wikipedia articles were more likely to cite a Cochrane review (p = 0.002), while lower quality articles were less likely to cite a Cochrane review (p < 0.0005). A greater number of Cochrane reviews are available for more ‘important’ Wikipedia articles (p < 0.005), and these articles were more likely to cite a Cochrane review (p < 0.005). This approach to an indexing system can be leveraged by Wikipedia contributors and editors seeking to update disease-related Wikipedia articles with relevant Cochrane reviews (thus improving their quality), and online information seekers in need of additional information to supplement their Wikipedia search.",https://www.semanticscholar.org/paper/0f57c6a9447ecc5219d4ad2bc3b7ee5a5805d073,Yes,,
85,An Edit-centric Approach for Wikipedia Article Quality Assessment,"We propose an edit-centric approach to assess Wikipedia article quality as a complementary alternative to current full document-based techniques. Our model consists of a main classifier equipped with an auxiliary generative module which, for a given edit, jointly provides an estimation of its quality and generates a description in natural language. We performed an empirical study to assess the feasibility of the proposed model and its cost-effectiveness in terms of data and quality requirements.",https://www.semanticscholar.org/paper/1ec28bdcacf6fd2c72c227ccd74a2ad1ee8d4b1b,Yes,,
87,StRE: Self Attentive Edit Quality Prediction in Wikipedia,"Wikipedia can easily be justified as a behemoth, considering the sheer volume of content that is added or removed every minute to its several projects. This creates an immense scope, in the field of natural language processing toward developing automated tools for content moderation and review. In this paper we propose Self Attentive Revision Encoder (StRE) which leverages orthographic similarity of lexical units toward predicting the quality of new edits. In contrast to existing propositions which primarily employ features like page reputation, editor activity or rule based heuristics, we utilize the textual content of the edits which, we believe contains superior signatures of their quality. More specifically, we deploy deep encoders to generate representations of the edits from its text content, which we then leverage to infer quality. We further contribute a novel dataset containing ∼ 21M revisions across 32K Wikipedia pages and demonstrate that StRE outperforms existing methods by a significant margin – at least 17% and at most 103%. Our pre-trained model achieves such result after retraining on a set as small as 20% of the edits in a wikipage. This, to the best of our knowledge, is also the first attempt towards employing deep language models to the enormous domain of automated content moderation and review in Wikipedia.",https://www.semanticscholar.org/paper/e2988816a10bf5a50bd952601ac06026e5493782,Yes,,
88,The role of conflict in determining consensus on quality in Wikipedia articles,"This paper presents research that investigated the role of conflict in the editorial process of the online encyclopedia, Wikipedia. The study used a grounded approach to analyzing 147 conversations about quality from the archived history of the Wikipedia article Australia. It found that conflict in Wikipedia is a generative friction, regulated by references to policy as part of a coordinated effort within the community to improve the quality of articles.",https://www.semanticscholar.org/paper/d7d82669f167bd609e3d593d0d0cc6743d58c8ef,No,,Manual
89,Quality assessment of wikipedia articles: a deep learning approach,"Wikipedia is indeed a very important knowledge sharing platform. However, since its start in 2001, the quality of Wikipedia is questioned because its content is created potentially by everyone who can access the Internet. Currently, the quality of Wikipedia articles is assessed by human judgement. The method is not scalable up to huge size and fast changing speed of Wikipedia today. An automatic quality classifier for Wikipedia articles is required to support user to choose high quality articles for reading and to notify authors for improving their products. While other existing approaches are based on manually predefined specific feature set, we present our approach of using deep learning to automatically represent Wikipedia articles for quality classification.",https://www.semanticscholar.org/paper/ce000225b8d092b4872ff49081238b73ba8185c5,Yes,,
90,Evaluating the trustworthiness of Wikipedia articles through quality and credibility,"Wikipedia has become a very popular destination for Web surfers seeking knowledge about a wide variety of subjects. While it contains many helpful articles with accurate information, it also consists of unreliable articles with inaccurate or incomplete information. A casual observer might not be able to differentiate between the good and the bad. In this work, we identify the necessity and challenges for trust assessment in Wikipedia, and propose a framework that can help address these challenges by identifying relevant features and providing empirical means to meet the requirements for such an evaluation. We select relevant variables and perform experiments to evaluate our approach. The results demonstrate promising performance that is better than comparable approaches and could possibly be replicated with other social media applications.",https://www.semanticscholar.org/paper/76382864f6d33a8b33296fedadf7dc36be6cbb13,Yes,,
91,The quality of content in open online collaboration platforms: approaches to NLP-supported information quality management in Wikipedia,"Over the past decade, the paradigm of the World Wide Web has shifted from static web pages towards participatory and collaborative content production. The main properties of this user generated content are a low publication threshold and little or no editorial control. While this has improved the variety and timeliness of the available information, it causes an even higher variance in quality than the already heterogeneous quality of traditional web content. Wikipedia is the prime example for a successful, large-scale, collaboratively created resource that reflects the spirit of the open collaborative content creation paradigm. Even though recent studies have confirmed that the overall quality of Wikipedia is high, there is still a wide gap that must be bridged before Wikipedia reaches the state of a reliable, citable source. A key prerequisite to reaching this goal is a quality management strategy that can cope both with the massive scale of Wikipedia and its open and almost anarchic nature. This includes an efficient communication platform for work coordination among the collaborators as well as techniques for monitoring quality problems across the encyclopedia. This dissertation shows how natural language processing approaches can be used to assist information quality management on a massive scale. In the first part of this thesis, we establish the theoretical foundations for our work. We first introduce the relatively new concept of open online collaboration with a particular focus on collaborative writing and proceed with a detailed discussion of Wikipedia and its role as an encyclopedia, a community, an online collaboration platform, and a knowledge resource for language technology applications. We then proceed with the three main contributions of this thesis. Even though there have been previous attempts to adapt existing information quality frameworks to Wikipedia, no quality model has yet incorporated writing quality as a central factor. Since Wikipedia is not only a repository of mere facts but rather consists of full text articles, the writing quality of these articles has to be taken into consideration when judging article quality. As the first main contribution of this thesis, we therefore define a comprehensive article quality model that aims to consolidate both the quality of writing and the quality criteria defined in multiple Wikipedia guidelines and policies into a single model. The model comprises 23 dimensions segmented into the four layers of intrinsic quality, contextual quality, writing quality and organizational quality. As a second main contribution, we present an approach for automatically identifying quality flaws in Wikipedia articles. Even though the general idea of quality detection has been introduced in previous work, we dissect the approach to find that the task is inherently prone to a topic bias which results in unrealistically high cross-validated evaluation results that do not reflect the classifier’s real performance on real world data. We solve this problem with a novel data sampling approach based on the full article revision history that is able to avoid this bias. It furthermore allows us not only to identify flawed articles but also to find reliable counterexamples that do not exhibit the respective quality flaws. For automatically detecting quality flaws in unseen articles, we present FlawFinder, a modular system for supervised text classification. We evaluate the system on a novel corpus of Wikipedia articles with neutrality and style flaws. The results confirm the initial hypothesis that the reliable classifiers tend to exhibit a lower cross-validated performance than the biased ones but the scores more closely resemble their actual performance in the wild. As a third main contribution, we present an approach for automatically segmenting and tagging the user contributions on article Talk pages to improve the work coordination among Wikipedians. These unstructured discussion pages are not easy to navigate and information is likely to get lost over time in the discussion archives. By automatically identifying the quality problems that have been discussed in the past and the solutions that have been proposed, we can help users to make informed decisions in the future. Our contribution in this area is threefold: (i) We describe a novel algorithm for segmenting the unstructured dialog on Wikipedia Talk pages using their revision history. In contrast to related work, which mainly relies on the rudimentary markup, this new algorithm can reliably extract meta data, such as the identity of a user, and is moreover able to handle discontinuous turns. (ii) We introduce a novel scheme for annotating the turns in article discussions with dialog act labels for capturing the coordination efforts of article improvement. The labels reflect the types of criticism discussed in a turn, for example missing information or inappropriate language, as well as any actions proposed for solving the quality problems. (iii) Based on this scheme, we created two automatically segmented and manually annotated discussion corpora extracted from the Simple English Wikipedia (SEWD) and the English Wikipedia (EWD). We evaluate how well text classification approaches can learn to assign the dialog act labels from our scheme to unseen discussion pages and achieve a cross-validated performance of F1 = 0.82 on the SEWD corpus while we obtain an average performance of F1 = 0.78 on the larger and more complex EWD corpus.",https://www.semanticscholar.org/paper/363f84266063148bddf8f76bba519d960e1f6981,No,,Not article (thesis)
92,A matter of words: NLP for quality evaluation of Wikipedia medical articles,"Automatic quality evaluation of Web information is a task with many fields of applications and of great relevance, especially in critical domains, like the medical one. We move from the intuition that the quality of content of medical Web documents is affected by features related with the specific domain. First, the usage of a specific vocabulary (Domain Informativeness); then, the adoption of specific codes (like those used in the infoboxes of Wikipedia articles) and the type of document (e.g., historical and technical ones). In this paper, we propose to leverage specific domain features to improve the results of the evaluation of Wikipedia medical articles, relying on Natural Language Processing (NLP) and dictionaries-based techniques. The results of our experiments confirm that, by considering domain-oriented features, it is possible to improve existing solutions, mainly with those articles that other approaches have less correctly classified.",https://www.semanticscholar.org/paper/4724b3d392a48884c7463a2ba78668c7fddd4f3c,Yes,,NLP
94,Indicator of quality for environmental articles on Wikipedia at the higher education level,"Wikipedia is important in higher education because students and scholars often use it. Nevertheless, the issue of Wikipedia’s quality is an obstacle for its use at the higher education level. In order to contribute to this discussion, we have proposed ‘Verifiability by respected sources’ as an indicator for assessing the quality of Wikipedia articles at the higher education level and conducted an analysis of the most frequently visited articles in the category of Environment on Wikipedia. Results show that these articles contain many unreferenced statements, so their usage at the higher education level is problematic. Therefore, we also propose specific steps for relevant actors that could help to improve the quality of Wikipedia.",https://www.semanticscholar.org/paper/9df731914171c0dfb635f15e4e2ae245353d15fd,Maybe,,
95,Mining team characteristics to predict Wikipedia article quality,"In this study, we were interested in studying which characteristics of virtual teams are good predictors for the quality of their production. The experiment involved obtaining the Spanish Wikipedia database dump and applying different data mining techniques suitable for large data sets to label the whole set of articles according to their quality (comparing them with the Featured/Good Articles, or FA/GA). Then we created the attributes that describe the characteristics of the team who produced the articles and using decision tree methods, we obtained the most relevant characteristics of the teams that produced FA/GA. The team's maximum efficiency and the total length of contribution are the most important predictors. This article contributes to the literature on virtual team organization.",https://www.semanticscholar.org/paper/cdc1216d54d10a3a6f8e742941010f5d83002940,Yes,,
96,Estimating the Quality of Articles in Russian Wikipedia Using the Logical-Linguistic Model of Fact Extraction,We present the method of estimating the quality of articles in Russian Wikipedia that is based on counting the number of facts in the article. For calculating the number of facts we use our logical-linguistic model of fact extraction. Basic mathematical means of the model are logical-algebraic equations of the finite predicates algebra. The model allows extracting of simple and complex types of facts in Russian sentences. We experimentally compare the effect of the density of these types of facts on the quality of articles in Russian Wikipedia. Better articles tend to have a higher density of facts. ,https://www.semanticscholar.org/paper/1aa2271c2a5fe2f7c69556334c4cf8b6e008c0ef,Maybe,,Facts?
97,"Quality Change: Norm or Exception? Measurement, Analysis and Detection of Quality Change in Wikipedia","Wikipedia has been turned into an immensely popular crowd-sourced encyclopedia for information dissemination on numerous versatile topics in the form of subscription free content. It allows anyone to contribute so that the articles remain comprehensive and updated. For enrichment of content without compromising standards, the Wikipedia community enumerates a detailed set of guidelines, which should be followed. Based on these, articles are categorized into several quality classes by the Wikipedia editors with increasing adherence to guidelines. This quality assessment task by editors is laborious as well as demands platform expertise. As a first objective, in this paper, we study evolution of a Wikipedia article with respect to such quality scales. Our results show novel non-intuitive patterns emerging from this exploration. As a second objective we attempt to develop an automated data driven approach for the detection of the early signals influencing the quality change of articles. We posit this as a change point detection problem whereby we represent an article as a time series of consecutive revisions and encode every revision by a set of intuitive features. Finally, various change point detection algorithms are used to efficiently and accurately detect the future change points. We also perform various ablation studies to understand which group of features are most effective in identifying the change points. To the best of our knowledge, this is the first work that rigorously explores English Wikipedia article quality life cycle from the perspective of quality indicators and provides a novel unsupervised page level approach to detect quality switch, which can help in automatic content monitoring in Wikipedia thus contributing significantly to the CSCW community.",https://www.semanticscholar.org/paper/ce03436b8284633ac9e492f1511028583f689a01,Yes,,
99,Assessing the Quality of Wikipedia Editors through Crowdsourcing,"In this paper, we propose a method for assessing the quality of Wikipedia editors. By effectively determining whether the text meaning persists over time, we can determine the actual contribution by editors. This is used in this paper to detect vandal. However, the meaning of text does not always change if a term in the text is added or removed. Therefore, we cannot capture the changes of text meaning automatically, so we cannot detect whether the meaning of text survives or not. To solve this problem, we use crowdsourcing to manually detect changes of text meaning. In our experiment, we confirmed that our proposed method improves the accuracy of detecting vandals by about 5%.",https://www.semanticscholar.org/paper/d7a5280305184070f9b0989de49f39f87a85c985,No,,Manual
100,Quality flaw prediction in Spanish Wikipedia: A case of study with verifiability flaws,"In this work, we present the first quality flaw prediction study for articles containing the two most frequent verifiability flaws in Spanish Wikipedia: articles which do not cite any references or sources at all (denominated Unreferenced) and articles that need additional citations for verification (so-called Refimprove). Based on the underlying characteristics of each flaw, different state-of-the-art approaches were evaluated. For articles not citing any references, a well-established rule-based approach was evaluated and interesting findings show that some of them suffer from Refimprove flaw instead. Likewise, for articles that need additional citations for verification, the well-known PU learning and one-class classification approaches were evaluated. Besides, new methods were compared and a new feature was also proposed to model this latter flaw. The results showed that new methods such as under-bagged decision trees with sum or majority voting rules, biased-SVM, and centroid-based balanced SVM, perform best in comparison with the ones previously published.",https://www.semanticscholar.org/paper/96624b05a21c20da28291207bff2199c68074b57,Yes,,
101,Structural Analysis of Wikigraph to Investigate Quality Grades of Wikipedia Articles,"The quality of Wikipedia articles is manually evaluated which is time inefficient as well as susceptible to human bias. An automated assessment of these articles may help in minimizing the overall time and manual errors. In this paper, we present a novel approach based on the structural analysis of Wikigraph to automate the estimation of the quality of Wikipedia articles. We examine the network built using the complete set of English Wikipedia articles and identify the variation of network signatures of the articles with respect to their quality. Our study shows that these signatures are useful for estimating the quality grades of un-assessed articles with an accuracy surpassing the existing approaches in this direction. The results of the study may help in reducing the need for human involvement for quality assessment tasks.",https://www.semanticscholar.org/paper/8ddfbf5d323e0a94160ffe10a13353793c954f30,Yes,,
102,Utilizing the Wikidata System to Improve the Quality of Medical Content in Wikipedia in Diverse Languages: A Pilot Study,"Background Wikipedia is an important source of medical information for both patients and medical professionals. Given its wide reach, improving the quality, completeness, and accessibility of medical information on Wikipedia could have a positive impact on global health. Objective We created a prototypical implementation of an automated system for keeping drug-drug interaction (DDI) information in Wikipedia up to date with current evidence about clinically significant drug interactions. Our work is based on Wikidata, a novel, graph-based database backend of Wikipedia currently in development. Methods We set up an automated process for integrating data from the Office of the National Coordinator for Health Information Technology (ONC) high priority DDI list into Wikidata. We set up exemplary implementations demonstrating how the DDI data we introduced into Wikidata could be displayed in Wikipedia articles in diverse languages. Finally, we conducted a pilot analysis to explore if adding the ONC high priority data would substantially enhance the information currently available on Wikipedia. Results We derived 1150 unique interactions from the ONC high priority list. Integration of the potential DDI data from Wikidata into Wikipedia articles proved to be straightforward and yielded useful results. We found that even though the majority of current English Wikipedia articles about pharmaceuticals contained sections detailing contraindications, only a small fraction of articles explicitly mentioned interaction partners from the ONC high priority list. For 91.30% (1050/1150) of the interaction pairs we tested, none of the 2 articles corresponding to the interacting substances explicitly mentioned the interaction partner. For 7.21% (83/1150) of the pairs, only 1 of the 2 associated Wikipedia articles mentioned the interaction partner; for only 1.48% (17/1150) of the pairs, both articles contained explicit mentions of the interaction partner. Conclusions Our prototype demonstrated that automated updating of medical content in Wikipedia through Wikidata is a viable option, albeit further refinements and community-wide consensus building are required before integration into public Wikipedia is possible. A long-term endeavor to improve the medical information in Wikipedia through structured data representation and automated workflows might lead to a significant improvement of the quality of medical information in one of the world’s most popular Web resources.",https://www.semanticscholar.org/paper/4f2d62bd9f0225a9358e4e520e6e5289247c3281,Maybe,,
103,The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia,"With the increasing amount of user generated reference texts in the web, automatic quality assessment has become a key challenge. However, only a small amount of annotated data is available for training quality assessment systems. Wikipedia contains a large amount of texts annotated with cleanup templates which identify quality flaws. We show that the distribution of these labels is topically biased, since they cannot be applied freely to any arbitrary article. We argue that it is necessary to consider the topical restrictions of each label in order to avoid a sampling bias that results in a skewed classifier and overly optimistic evaluation results. We factor out the topic bias by extracting reliable training instances from the revision history which have a topic distribution similar to the labeled articles. This approach better reflects the situation a classifier would face in a real-life application.",https://www.semanticscholar.org/paper/51bf2226cb153f0d5c7532b4b71075a7b76b069e,No,,Focus
105,Effects of Contributor Experience on the Quality of Health-Related Wikipedia Articles,"Background Consulting the Internet for health-related information is a common and widespread phenomenon, and Wikipedia is arguably one of the most important resources for health-related information. Therefore, it is relevant to identify factors that have an impact on the quality of health-related Wikipedia articles. Objective In our study we have hypothesized a positive effect of contributor experience on the quality of health-related Wikipedia articles. Methods We mined the edit history of all (as of February 2017) 18,805 articles that were listed in the categories on the portal health & fitness in the English language version of Wikipedia. We identified tags within the articles’ edit histories, which indicated potential issues with regard to the respective article’s quality or neutrality. Of all of the sampled articles, 99 (99/18,805, 0.53%) articles had at some point received at least one such tag. In our analysis we only considered those articles with a minimum of 10 edits (10,265 articles in total; 96 tagged articles, 0.94%). Additionally, to test our hypothesis, we constructed contributor profiles, where a profile consisted of all the articles edited by a contributor and the corresponding number of edits contributed. We did not differentiate between rollbacks and edits with novel content. Results Nonparametric Mann-Whitney U-tests indicated a higher number of previously edited articles for editors of the nontagged articles (mean rank tagged 2348.23, mean rank nontagged 5159.29; U=9.25, P<.001). However, we did not find a significant difference for the contributors’ total number of edits (mean rank tagged 4872.85, mean rank nontagged 5135.48; U=0.87, P=.39). Using logistic regression analysis with the respective article’s number of edits and number of editors as covariates, only the number of edited articles yielded a significant effect on the article’s status as tagged versus nontagged (dummy-coded; Nagelkerke R2 for the full model=.17; B [SE B]=-0.001 [0.00]; Wald c2 [1]=19.70; P<.001), whereas we again found no significant effect for the mere number of edits (Nagelkerke R2 for the full model=.15; B [SE B]=0.000 [0.01]; Wald c2 [1]=0.01; P=.94). Conclusions Our findings indicate an effect of contributor experience on the quality of health-related Wikipedia articles. However, only the number of previously edited articles was a predictor of the articles’ quality but not the mere volume of edits. More research is needed to disentangle the different aspects of contributor experience. We have discussed the implications of our findings with respect to ensuring the quality of health-related information in collaborative knowledge-building platforms.",https://www.semanticscholar.org/paper/71e8bb5240db948593ad47d03b9a703ee7b28020,No,,Focus
106,Relative Quality Assessment of Wikipedia Articles in Different Languages Using Synthetic Measure,"Online encyclopedia Wikipedia is one of the most popular sources of knowledge. It is often criticized for poor information quality. Articles can be created and edited even by anonymous users independently in almost 300 languages. Therefore, a difference in the information quality in various language versions on the same topic is observed. The Wikipedia community has created a system for assessing the quality of articles, which can be helpful in deciding which language version is more complete and correct. There are several issues: each Wikipedia language can use own grading scheme and there is usually a large number of unevaluated articles. In this paper, we propose to use a synthetic measure for automatic quality evaluation of the articles in different languages based on important features",https://www.semanticscholar.org/paper/78a15535dc2d82f2522c3eb20caa65baad1bda44,Yes,,
107,On the Use of Reliable-Negatives Selection Strategies in the PU Learning Approach for Quality Flaws Prediction in Wikipedia,"Learning from positive and unlabeled examples (PU learning) has proven to be an effective method in several Web mining applications. In particular, in the 1st International Competition on Quality Flaw Prediction in Wikipedia in 2012, a tailored PU learning approach performed best amongst the competitors. A key feature of that approach is the introduction of sampling strategies within the original PU learning procedure. The paper in hand revisits the winner approach of 2012 and elaborates on neglected aspects in order to provide evidence for the usefulness of sampling in PU learning. In this regard, we propose a modification to this PU learning approach, and we show how the different sampling strategies affect the flaw prediction effectiveness. Our analysis is based on the original evaluation corpus of the 2012-competition on quality flaw prediction. A main outcome is that under the best sampling strategy, our new modified version of PU learning increases in average the flaw prediction effectiveness by 18.31%, when compared against the winning approach of the competition.",https://www.semanticscholar.org/paper/8df3a3c41c7d00b3f1b1ae7ee7894ccc69b82474,Yes,,
108,Measuring Wikipedia Article Quality in One Dimension by Extending ORES with Ordinal Regression,"Organizing complex peer production projects and advancing scientific knowledge of open collaboration each depend on the ability to measure quality. Wikipedia community members and academic researchers have used article quality ratings for purposes like tracking knowledge gaps and studying how political polarization shapes collaboration. Even so, measuring quality presents many methodological challenges. The most widely used systems use quality assesements on discrete ordinal scales, but such labels can be inconvenient for statistics and machine learning. Prior work handles this by assuming that different levels of quality are “evenly spaced” from one another. This assumption runs counter to intuitions about degrees of effort needed to raise Wikipedia articles to different quality levels. I describe a technique extending the Wikimedia Foundations’ ORES article quality model to address these limitations. My method uses weighted ordinal regression models to construct one-dimensional continuous measures of quality. While scores from my technique and from prior approaches are correlated, my approach improves accuracy for research datasets and provides evidence that the “evenly spaced” assumption is unfounded in practice on English Wikipedia. I conclude with recommendations for using quality scores in future research and include the full code, data, and models.",https://www.semanticscholar.org/paper/2c6de714ad28778eaddb2b886b7168a79cc8b4ab,Yes,,
109,Enrichment of Information in Multilingual Wikipedia Based on Quality Analysis,"Despite the fact that Wikipedia is one of the most popular sources of information in the world, it is often criticized for the poor quality of content. In this online encyclopaedia articles on the same topic can be created and edited independently in different languages. Some of this language versions can provide valuable information on a specific topics. Wikipedia articles may include infobox, which used to collect and present a subset of important information about its subject. This study presents method for quality assessment of Wikipedia articles and information contained in their infoboxes. Choosing the best language versions of a particular article will allow for enrichment of information in less developed version editions of particular articles.",https://www.semanticscholar.org/paper/537cb10b82056ca9e8759c0276e6889e19cce93f,Maybe,,
111,GreenWiki: a tool to support users' assessment of the quality of Wikipedia articles,"In this work, we present GreenWiki, which is a wiki with a panel of quality indicators to assist the reader of a Wikipedia article in assessing its quality.",https://www.semanticscholar.org/paper/ccfcd884d5532e7b993495dcb44fbba1a8774c39,Maybe,,Visualization
112,Social-collaborative determinants of content quality in online knowledge production systems: comparing Wikipedia and Stack Overflow,"Online knowledge production sites, such as Wikipedia and Stack Overflow, are dominated by small groups of contributors. How does this affect knowledge quality and production? Does the persistent presence of some key contributors among the most productive members improve the quality of the knowledge, considered in the aggregate? The paper addresses these issues by correlating week-by-week value changes in contribution unevenness, elite resilience (stickiness), and content quality. The goal is to detect if and how changes in social structural variables may influence the quality of the knowledge produced by two representative online knowledge production sites: Wikipedia and Stack Overflow. Regression analysis shows that on Stack Overflow both unevenness and elite stickiness have a curvilinear effect on quality. Quality is optimized at specific levels of elite stickiness and unevenness. At the same time, on Wikipedia, quality increases linearly with a decline in entropy, overall, and with an increase in stickiness in the maturation phase, after an entropy elite stickiness, quality of content peak is reached.",https://www.semanticscholar.org/paper/1da7fc0b4ed71de73b519726d5f868e9c1af2659,Maybe,,
113,Improving information quality of Wikipedia articles with cooperative principle,"Purpose The purpose of this paper is to investigate the impact of cooperative principle on the information quality (IQ) by making objects more relevant for consumer needs, in particular case Wikipedia articles for students. Design/methodology/approach The authors performed a quantitative study with participants being invited to complete an online survey. Each rater evaluated three selected and re-written articles from Wikipedia by four IQ dimensions (accuracy, completeness, objectivity, and representation). Grice’s maxims and submaxims were used to re-write articles and make them more relevant for student cognitive needs. The results were analyzed with statistical methods of mean, standard deviation, Cronbach’s α, and ICC (two-way random model of single measure). Findings The study demonstrates that Wikipedia articles can be made more relevant for student needs by using cooperative principle with increase in IQ and also achieving higher consistency of students’ scores as recent research. In particular, students in the research perceived the abstract, constructed with cooperative principle, more objective and complete as reported in recent research. Practical implications The work can benefit encyclopedia editors to improve IQ of existing articles as well as consumers that would obtain more relevant information in less reading time. Originality/value This is one of the first attempts to empirically investigate the application of cooperate principle to make objects more relevant for consumer needs and impact of this on IQ. IQ improvement evidence is provided and impacts on IQ dimensions such as objectivity, completeness, accuracy, and representation for research community to validate and compare results.",https://www.semanticscholar.org/paper/e32ec265e99218e301a621486c3363a4a6ae9049,No,,Manual
114,An Empirical Study to Predict the Quality of Wikipedia Articles,"Wikipedia is considered a common way to deliver content in a more effective way as compared to other types of an encyclopedia. However, the quality threat remains an issue regarding the Wikipedia articles. The basic aim of propose research to perform an empirical study to predict the quality of Wikipedia articles. In the proposed methodology, we consider few metrics such as article length (total number of word in an article), number of edits, article age (in the day) and article ranking and perform few statistical tests analyze the quality of Wikipedia articles. Moreover, we observe a significant correlation of proposed metrics with the rating of articles in order to identify their quality. 
",https://www.semanticscholar.org/paper/31e3f37070433dae787aa8228765ccac9272461f,Yes,,
115,On the Use of PU Learning for Quality Flaw Prediction in Wikipedia,"In this article we describe a new approach to assess Quality Flaw Prediction in Wikipedia. The partially supervised method studied, called PU Learning, has been successfully applied in classifications tasks with traditional corpora like Reuters-21578 or 20-Newsgroups. To the best of our knowledge, this is the first time that it is applied in this domain. Throughout this paper, we describe how the original PU Learning approach was evaluated for assessing quality flaws and the modifications introduced to get a quality flaws predictor which obtained the best F1 scores in the task “Quality Flaw Prediction in Wikipedia” of the PAN challenge",https://www.semanticscholar.org/paper/72d5756b021722d8c986fd3764b95c11aa8b0722,Yes,,
116,Thai Wikipedia Quality Measurement using Fuzzy Logic,"Wikipedia is widely known as an online encyclopedia. The open access model is a key success for Wikipedia, however the quality of articles is a problem. Since the articles are collaboratively written and maintained by online volunteers. The flaws are normally detected and removed by Wikipedia users who encounter the problem. They use the cleanup tags to tag the article. Therefore, the quality detection tool can automatically help readers to identify articles that are of good quality. Many current techniques rely on exactly quality feature and identify quality article by classification or clustering method. The aim of this research is to classify the articles of Thai Wikipedia into two classes namely featured article and normal article by using Fuzzy Logic. We believe that the degree of the article’s quality is ambiguous. Our dataset consists of 88 Thai featured articles and 100 normal articles. Our evaluation is based on a corpus comprising of human labeled the degree of each quality this articles. We found that the degree of quality articles obtained from Fuzzy Logic provide the accuracy close to the expert inspection. ",https://www.semanticscholar.org/paper/5c7b2bce626e9f44590fc263a94920d5727e852b,No,,Manual
117,QualityRank: assessing quality of wikipedia articles by mutually evaluating editors and texts,"In this paper, we propose a method to identify high-quality Wikipedia articles by mutually evaluating editors and texts. A major approach for assessing articles using edit history is a text survival ratio based approach. However, the problem is that many high-quality articles are identified as low quality, because many vandals delete high-quality texts, then the survival ratios of high-quality texts are decreased by vandals. Our approach's strongest point is its resistance to vandalism. Using our method, if we calculate text quality values using editor quality values, vandals do not affect any quality values of the other editors, then the accuracy of text quality values should improve. However, the problem is that editor quality values are calculated by text quality values, and text quality values are calculated by editor quality values. To solve this problem, we mutually calculate editor and text quality values until they converge. Using this method, we can calculate a quality value of a text that takes into consideration that of its editors.",https://www.semanticscholar.org/paper/0e24d74a6235462045441c836e504e0b8517e35f,Maybe,,
118,A Framework for Assessing the Quality of Wikipedia Articles: A Meta-synthesis of the Literature,"This study aimed to design and validate an information quality assessment framework based on the systematic review of the literature. A meta-synthesis method was applied to identify features and dimensions for designing the framework for assessing the quality of Wikipedia. Following this, the validity of the obtained framework was evaluated by the Kappa Test of Agreement. The statistical population consists of all scientific documents related to the quality of information and a sample of 39 documents selected based on CASP. MAXQDA 11 was used to analyze data. Nine dimensions were identified and classified across 3 levels. The features and components necessary to evaluate each dimension were identified and explained. The strong level approved the framework of expert agreement (0.654). The proposed framework can be used to assess the Wikipedia quality independently of specialists, improve the quality of articles through identified effective features, and ultimately, build tools and practical guidelines to assess the quality of information.",https://www.magiran.com/paper/2379640,No,,"Só pus «não» para ter a opinião da professora. É uma revisão de literatura, por isso fui ver o pdf. É muito completa, pode ser útil, mas acho que não se enquadra no contexto, não? De qualquer modo, acho que o abstract deve passar"
119,Assessing Information Quality of Wikipedia Articles through Google’s E-A-T Model,"Along with the emergence of Web 2.0, User Generated Content (UGC) is becoming increasingly important for knowledge sharing. Wikipedia being the world’s largest-ever community-based collaborative encyclopedia, is also one of the biggest UGC databases in the world. Wikipedia is dealing with a significant problem of Information Quality (IQ) because of its open-source and collaborative nature. When carrying out attacks such as link spamming, malicious users take advantage of Wikipedia’s popularity on the WWW. As a result, Wikipedia is generally not recommended for academic-related work. There are, however, some articles that are both rich in information and quality. Existing approaches for assessing Wikipedia’s IQ involve statistical models and machine learning algorithms; however, the existing models do not produce satisfactory results. In this study, a novel theoretical model based on Google’s E-A-T framework is introduced to assess Wikipedia’s IQ. The model comprises three IQ constructs Expertise, Authority and Trustworthiness. Based on the empirical findings and study results, a set of IQ dimensions that influence the above three IQ constructs, as well as 45 IQ attributes to measure the IQ dimensions, were identified. The IQ attributes were automatically and inexpensively extracted from the content and meta-data statistics of Wikipedia articles using a Selenium 3.14 web automation script. A sample of 2000 articles comprising 1000 Featured Articles (FA) and 1000 non-FA articles from six WikiProjects was used for the data analysis. The proposed model was compared with three previously published models in terms of classification and clustering accuracy. It received classification and clustering accuracies of 95% and 93% respectively which is a drastic improvement over the existing models. Furthermore, an average inter-rater agreement of 84% was observed. Thus, the proposed model’s effectiveness is fairly validated by this extensive experiment. This study contributes to the related knowledge area by introducing a novel framework to assess Wikipedia articles’ IQ. The study’s limitations include the domain specificity of the chosen dataset and focusing solely on the English language. However, the results can be generalized by improving the dataset by size and replicating the study for the other domains and languages supported by Wikipedia.",https://www.semanticscholar.org/paper/2269aaf18fa7f0168d870e09840d7dcad1e2aa3b,Yes,,
120,How to Inspect and Measure Data Quality about Scientific Publications: Use Case of Wikipedia and CRIS Databases,"The quality assurance of publication data in collaborative knowledge bases and in current research information systems (CRIS) becomes more and more relevant by the use of freely available spatial information in different application scenarios. When integrating this data into CRIS, it is necessary to be able to recognize and assess their quality. Only then is it possible to compile a result from the available data that fulfills its purpose for the user, namely to deliver reliable data and information. This paper discussed the quality problems of source metadata in Wikipedia and CRIS. Based on real data from over 40 million Wikipedia articles in various languages, we performed preliminary quality analysis of the metadata of scientific publications using a data quality tool. So far, no data quality measurements have been programmed with Python to assess the quality of metadata from scientific publications in Wikipedia and CRIS. With this in mind, we programmed the methods and algorithms as code, but presented it in the form of pseudocode in this paper to measure the quality related to objective data quality dimensions such as completeness, correctness, consistency, and timeliness. This was prepared as a macro service so that the users can use the measurement results with the program code to make a statement about their scientific publications metadata so that the management can rely on high-quality data when making decisions.",https://www.semanticscholar.org/paper/56bf3c2223d01ca19aa69e62fba08c5f065422ce,Maybe,,Visualization
121,Using Hyperlink Texts to Improve Quality of Identifying Document Topics Based on Wikipedia,This paper presents a method to identify the topics of documents based on Wikipedia category network. It is to improve the method previously proposed by Schonhofen by taking into account the weights of words in hyperlink texts in Wikipedia articles. The experiments on Computing and Team Sport domains have been carried out and showed that our proposed method outperforms the Schonhofen’s one.,https://www.semanticscholar.org/paper/7adf1412ebcacfd38284dfcbc8962b1b88e6e3ac,No,,Focus
122,Measuring Quality of Wikipedia Articles by Feature Fusion‐based Stack Learning,"Online open‐source knowledge repository such as Wikipedia has become an increasingly important source for users to access knowledge. However, due to its large volume, it is challenging to evaluate Wikipedia article quality manually. To fill this gap, we propose a novel approach named “feature fusion‐based stack learning” to assess the quality of Wikipedia articles. Pre‐trained language models including BERT (Bidirectional Encoder Representations from Transformers) and ELMo (Embeddings from Language Models) are applied to extract semantic information in Wikipedia content. The feature fusion framework consisting of semantic and statistical features is built and fed into an out‐of‐sample (OOS) stacking model, which includes both machine learning and deep learning models. We compare the performance of proposed model with some existing models with different metrics extensively, and conduct ablation studies to prove the effectiveness of our framework and OOS stacking. Generally, the experiment shows that our method is much better than state‐of‐the‐art models.",https://www.semanticscholar.org/paper/38035f89a0531a61e4dfe6c373eb36b0a5f584a4,Yes,,DL
123,An evaluation of the quality of consumer health information on Wikipedia,"Background Wikipedia is a multilingual, open-content, online encyclopedia that exists as a wiki. It is written collaboratively by people with varying degrees of expertise. Indeed, anyone who can access Wikipedia's Web site may alter any of its content. Wikipedia contains a significant number of articles on health-related topics, but the quality of this information is unknown. Purpose ofthe Study The accuracy and completeness of the information on Wikipedia is intuitively circumspect since experts and non-experts alike may contribute to the site's content. Thus, the intent of my research was to systematically evaluate the accuracy and completeness of a sample of health-related articles on Wikipedia. Methodology I selected a previously published methodology for use in my study. The advantage of this choice is that I could use the results from that study as control data for my evaluation of Wikipedia. As an additional control, I evaluated the content of the Healthwise® knowledge base (a collection of consumer health articles that is esteemed by many health professionals for its high-quality). The articles reviewed concerned the following four health topics: breast cancer, childhood asthma, depression, and obesity. Evaluation criteria were defined a priori. For each health topic, several key elements (totaling 100 across all four topics) were identified as desirable components of the content. A panel of health professionals and consumer advocates developed this list of elements. For each element, a rater scored the",https://www.semanticscholar.org/paper/6daa230a4e22031754e43685e5bd36b8395fbe61,No,,Manual
125,"How do metrics of link analysis correlate to quality, relevance and popularity in wikipedia?","Many links between Web pages can be viewed as indicative of the quality and importance of the pages they pointed to. Accordingly, several studies have proposed metrics based on links to infer web page content quality. However, as far as we know, the only work that has examined the correlation between such metrics and content quality consisted of a limited study that left many open questions. In spite of these metrics having been shown successful in the task of ranking pages which were provided as answers to queries submitted to search engines, it is not possible to determine the specific contribution of factors such as quality, popularity, and importance to the results. This difficulty is partially due to the fact that such information is hard to obtain for Web pages in general. Unlike ordinary Web pages, the quality, importance and popularity of Wikipedia articles are evaluated by human experts or might be easily estimated. Thus, it is feasible to verify the relation between link analysis metrics and such factors in Wikipedia articles, our goal in this work. To accomplish that, we implemented several link analysis algorithms and compared their resulting rankings with the ones created by human evaluators regarding factors such as quality, popularity and importance. We found that the metrics are more correlated to quality and popularity than to importance, and the correlation is moderate.",https://www.semanticscholar.org/paper/2578b4d957fc7fbcc7a5be2dbcea5c8f8cc4c48c,Yes,,
127,"On the Relation of Edit Behavior, Link Structure, and Article Quality on Wikipedia","When editing articles on Wikipedia, arguments between editors frequently occur. These conflicts occasionally lead to destructive behavior and diminish article quality. Currently, the relation between editing behavior, link structure, and article quality is not well-understood in our community, notwithstanding that this relation may facilitate editing processes and article quality on Wikipedia. To shed light on this complex relation, we classify edits for 13,045 articles and perform an in-depth analysis of a 4,800 article subsample. Additionally, we build a network of wikilinks (internal Wikipedia hyperlinks) between articles. Using this data, we compute parsimonious metrics to quantify editing and linking behavior. Our analysis unveils that controversial articles differ considerably from others for almost all metrics, while slight trends are also detectable for higher-quality articles. With our work, we assist online collaboration communities, especially Wikipedia, in long-term improvement of article quality by identifying deviant behavior via simple sequence-based edit and network-based article metrics. ",https://www.semanticscholar.org/paper/176c68a518d81215e585f09ce7db2415c8fdde2f,Maybe,,
128,A Psycho-Lexical Approach to the Assessment of Information Quality on Wikipedia,"The great popularity of Wikipedia makes it one of the dominant knowledge source around the World. However, since one of the core principles of Wikipedia is being open for anyone to maintain it, Wikipedia cannot fully ensure the reliability of its articles, and thus sometimes suffered criticism for containing low-quality information. It is therefore essential to assess the quality of Wikipedia articles automatically. In this paper we describe how we approach that problem by using a psycho-lexical resource, i.e., the Language Inquiry and Word Count (LIWC) dictionary. By training a classifier on different LIWC categories, we discuss the implications of each category for Wikipedia quality assessment.",https://www.semanticscholar.org/paper/f5a926ec49dc153326a0aca080377dc0be12e75d,Yes,,
129,On the Evolution of Quality Flaws and the Effectiveness of Cleanup Tags in the English Wikipedia,"The improvement of information quality is a major task for the free online encyclopedia Wikipedia. Recent studies targeted the analysis and detection of specific quality flaws in Wikipedia articles. To date, quality flaws have been exclusively investigated in current Wikipedia articles, based on a snapshot representing the state of Wikipedia at a certain time. This paper goes further, and provides the first comprehensive breakdown of the evolution of quality flaws in Wikipedia. We utilize cleanup tags to analyze the quality flaws that have been tagged by the Wikipedia community in the English Wikipedia, from its launch in 2001 until 2011. This leads to interesting findings regarding (1) the development of Wikipedia’s quality flaw structure and (2) the usage and the effectiveness of cleanup tags. Specifically, we show that inline tags are more effective than tag boxes, and provide statistics about the considerable volume of rare and non-specific cleanup tags. We expect that this work will support the Wikipedia community in making quality assurance activities more efficient.",https://www.semanticscholar.org/paper/b6aa3e619142d395c01857ec6e7b3b14da8fd0f7,Maybe,,
130,Thai Wikipedia article quality filtering algorithm,"Wikipedia is a content creation system that uses open collaboration as a strategy to drive the variety of topic coverage. There are approximately 100,000 Thai articles in Wikipedia. We found that the quality of content is the big issue since there are only 240 articles that has been labeled as featured articles whereas the rest of Thai articles are unlabeled. The website is ranked number 12 in term of user access in Thailand. That infers the use of their content in many academic documents and it would affect Thai educational quality in the long term. This paper present a good quality article filtering framework using decision tree algorithm. We propose new feature set obtained from a variety of references found in Wikipedia articles. The feature sets are applied in the machine learning algorithm in order to get the classifier with the knowledge concept of high and low quality articles. The performance of filtering algorithm on unlabeled articles is evaluated by real users to validate the performance of the system. ",https://www.iaeng.org/publication/IMECS2017/IMECS2017_pp299-305.pdf,Yes,,
131,Assessing the Quality of Wikipedia Pages Using Edit Longevity and Contributor Centrality,"In this paper we address the challenge of assessing the quality of Wikipedia pages using scores derived from edit contribution and con- tributor authoritativeness measures. The hypothesis is that pages with significant contributions from authoritative contributors are likely to be high-quality pages. Contributions are quantified using edit longevity measures and contributor authoritativeness is scored using centrality metrics in either the Wikipedia talk or co-author networks. The results suggest that it is useful to take into account the contributor authori- tativeness when assessing the information quality of Wikipedia content. The percentile visualization of the quality scores provides some insights about the anomalous articles, and can be used to help Wikipedia editors to identify Start and Stub articles that are of relatively good quality.",https://www.semanticscholar.org/paper/f621c58c765a3d2a6501a9fbce21681b4d2beb28,Yes,,
132,Towards Information Quality Assurance in Spanish: Wikipedia,"Featured Articles (FA) are considered to be the best articles that Wikipedia has to offer and in the last years, researchers have found interesting to analyze whether and how they can be distinguished from “ordinary” articles. Likewise, identifying what issues have to be enhanced or fixed in ordinary articles in order to improve their quality is a recent key research trend. Most of the approaches developed to face these information quality problems have been proposed for the English Wikipedia. However, few efforts have been accomplished in Spanish Wikipedia, despite being Spanish, one of the most spoken languages in the world by native speakers. In this respect, we present a breakdown of Spanish Wikipedia’s quality flaw structure. Besides, we carry out studies with three different corpora to automatically assess information quality in Spanish Wikipedia, where FA identification is evaluated as a binary classification task. Our evaluation on a unified setting allows to compare with the English version, the performance achieved by our approach on the Spanish version. The best results obtained show that FA identification in Spanish, can be performed with an F1 score of 0.88 using a document model consisting of only twenty six features and Support Vector Machine as classification algorithm.",https://www.semanticscholar.org/paper/8cba1878de84959de7a5401c9181819ee9bdf205,Yes,,
133,Effects of Implicit Positive Ratings for Quality Assessment of Wikipedia Articles,"In this paper, we propose a method to identify high-quality Wikipedia articles by using implicit positive ratings. One of the major approaches for assessing Wikipedia articles is a text survival ratio based approach. In this approach, when a text survives beyond multiple edits, the text is assessed as high quality. However, the problem is that many low quality articles are misjudged as high quality, because every editor does not always read the whole article. If there is a low quality text at the bottom of a long article, and the text has not seen by the other editors, then the text survives beyond many edits, and the text is assessed as high quality. To solve this problem, we use a section and a paragraph as a unit instead of a whole page. In our method, if an editor edits an article, the system considers that the editor gives positive ratings to the section or the paragraph that the editor edits. From experimental evaluation, we confirmed that the proposed method could improve the accuracy of quality values for articles.",https://www.semanticscholar.org/paper/ba661edb080125d879bbfe004ae8dc4f1c6b7499,Maybe,,
134,Learning with Wikipedia in Higher Education: Academic Performance and Students' Quality Perception,"Despite some quality concerns, the success of Wikipedia as a resource of information is influencing education. New technology-enhanced learning strategies are recently developed to include this open educational resource in courses’ design. Although research about the use of Wikipedia in higher education and its quality perception is scarce, there are some empirical evidences showing its positive effect on the students’ academic performance and its positive quality perception. In line with this, the main aim of this paper is to prove statistically that Wikipedia’s usage improves the final marks of the students enrolled in topics from different knowledge areas. Additionally, we study the Wikipedia’s quality perception among students. Based on an experimental research design with 2330 students, and using data from a questionnaire and from their course marks, we prove through different statistical tests that (1) the academic active use of Wikipedia has a positive influence on the student’s academic performance, which is moderated by knowledge areas, and that (2) Wikipedia’s quality perception is positive and it does not depend on the student’s performance.",https://www.semanticscholar.org/paper/a1f353d97743fce8b8ca5ace3b80f1bc9f820f96,No,,Manual
135,Self-Regulation: How Wikipedia Leverages User-Generated Quality Control Under Section 230,"As Virginia Woolf once wrote, “[T]o enjoy freedom, we have…to control ourselves.” In the market for online information services, Wikipedia has done just that. Wikipedia has achieved astounding success via self-regulation. Wikipedia promotes user-generated quality control not as a legal obligation, but as a commitment to its educational purpose and values of its fact-checking community. In doing so, Wikipedia has leveraged the purpose of Section 230 of the Communications Decency Act into consumer welfare. Section 230 protects sites that engage in ""Good Samaritan"" policing of harmful material, with no requirement on the quality or quantity of such monitoring. Interactive sites should treat the statute an opportunity, rather than mere permission to thrive in the world of Web 2.0: those who can productively self-regulate, should.",https://www.semanticscholar.org/paper/f22da6c8f5aea5e111eee39956fea749eae91785,No,,Focus
136,Assessing the Quality of Wikipedia Articles,"Wikipedia is a very important information reference source for the Internet users. Due to the fact that the content of Wikipedia is the collaborative result from a massive number of participants all over the world, the quality of Wikipedia might be questionable. Over the last decade, many research works are dedicated to solve the issue of Wikipedia quality. In this paper, we present our latest research in determining the quality of Wikipedia articles. The evaluation on the real-world dataset shows that our method outperforms other baseline methods proposed recently.",https://www.semanticscholar.org/paper/f47fe9e54ef4444ea46e910d9429245bea9d0dfd,Yes,,
137,Letter to the Editor: Quality of mental health information on Wikipedia,"We read with interest the analysis of the quality of mental health information available online by Reavley et al. (2011) and, as both mental health researchers and Wikipedia editors who have contributed to the articles included in the study, we were encouraged to see this important online resource discussed in Psychological Medicine. As the authors point out, Google searches often yield Wikipedia articles in the first few results ; the ‘schizophrenia ’ article (http://en.wikipedia.org/ wiki/Schizophrenia) had been viewed 369 372 times in the 30 days prior to 20 December 2011 (http://stats. grok.se/). Across all domains except for readability, Wikipedia outperformed Encyclopaedia Britannica, a psychiatry textbook and a number of static information websites from professional bodies. The finding is notable because it contradicts the common stereotype of Wikipedia as being written by lay people, and therefore inaccurate. However, we believe further research is warranted to explore who is editing these articles. While serving as academic researchers, we have made thousands of edits to articles on schizophrenia, neuropsychology, neurodegenerative diseases, conversion disorder and mood-rating scales. This has been done in our spare time out of enjoyment, but we suspect there are many other ‘ lurkers ’ who edit articles using anonymous Internet protocol (IP) addresses to make improvements. It is also worth noting that these are ‘ featured articles ’ which represent the best of Wikipedia, and that not all articles are of this standard. Reavley et al. (2011) suggest professional associations could create Wikipedia task forces and even include statements of approval on articles. Because the ethos of Wikipedia states that a layperson has just as much right to make an edit as a committee of experts, we believe this is probably not the right path to take. We would propose instead that professional bodies support members as individual contributors, rather than ‘ task force members’, to write and maintain informed, current, readable articles in their areas of expertise. This would mean a wider range of professionals could be involved and any perceived conflicts of interest in article content would be avoided. There are three pillars to support such an approach: leadership, education, and incentive. Leadership from the professional bodies of mental health (andmedicine broadly) should recognize that Wikipedia is a go-to source of information that rivals any resource in human history for patients, and that letting poor-quality articles go unimproved is harmful. Education should be provided to train professionals to make appropriate and useful contributions within the context of Wikipedia as a community, rather than just their own academic field. Finally, there must be incentive to motivate experts to contribute ; editing articles should be seen as a positive contribution by promotion boards. Asking experts to donate their time to peerreview and improve the work of others is not new, and peer-reviewing for journals is already something that receives both formal and informal support. There may be an opportunity to improve understanding, reduce stigma and educate patients on a scale not seen before, and although it is a strange new world, we believe it is one with which we all must engage. If readers are interested in participating, searching for ‘WikiProject Medicine ’ (http://en. wikipedia.org/wiki/Wikipedia:WikiProject_Medicine) would be a good place to start.",https://www.semanticscholar.org/paper/db0b7c773018080e0401d15272100a2a57272dd5,No,,Not article (letter to editor)
139,Using Morphological and Semantic Features for the Quality Assessment of Russian Wikipedia,"Nowadays, the assessment of the quality and credibility of Wikipedia articles becomes increasingly important. We propose to use morphological and semantic features to estimate the quality of Wikipedia articles in Russian language. We distinguished over 150 linguistic features and divided them into four groups. In these groups, we considered the features of encyclopedic style, readability and subjectivism of the article’s text. Based on Random Forest as a classification algorithm, we show the most importance linguistic features that affect the quality of Russian Wikipedia articles. We compare the classification results of our four linguistic features groups separately. We have achieved the F-measure of 89,75%.",https://www.semanticscholar.org/paper/9b06d30b1eafc37b2b90c1f3562d879f7fa2a8aa,Yes,,
140,The Adoption of Wikipedia: A Community- and Information Quality-Based View,"The Web 2.0 model has aroused vast attention as it alters the traditional role of Internet users as pure information receivers. Wikipedia, as one of the most successful case of the Web 2.0 model, creates an online encyclopedia through the collective efforts of volunteers. Shared freely by all Internet users, it forms an online community platform on which users can seek and share knowledge. This study investigates the factors that affect the adoption of Wikipedia. Based on Davis' (1989) TAM, perceived critical mass, community identification, and information quality were incorporated into the research model for explaining the intentions and usage behavior of Wikipedia users. This research is a work-in-progress and a questionnaire survey will be executed, targeting at Internet users who had prior experiences with Wikipedia. The survey is expected to be conducted over the Internet in March, 2008.",https://www.semanticscholar.org/paper/ea0c5b9d37cb168cc418102e7a934fe7ee2d0e77,No,,Manual
141,Cross-lingual Data Quality for Knowledge Base Acceleration Across Wikipedia Editions,"Knowledge-sharing communities like Wikipedia and knowledge bases like Freebase are expected to capture the latest facts about the real world. However, neither of these can keep pace with the rate at which events happen and new knowledge is reported in news and social media. To narrow this gap, we propose an approach to accelerate the online maintenance of knowledge bases. Our method, called LAIKA, is based on link prediction. Wikipedia editions in dierent languages, Wikinews, and other news media come with extensive but noisy interlinkage at the entity level. We utilize this input for recommending, for a given Wikipedia article or knowledge-base entry, new categories, related entities, and cross-lingual interwiki links. LAIKA constructs a large graph from the available input and uses link-overlap measures and random-walk techniques to generate missing links and rank them for recommendations. Experiments with a very large graph from multilingual Wikipedia editions demonstrate the accuracy of our link predictions.",https://www.semanticscholar.org/paper/e4dda236a443a596dd710ba6dddaba59648caf22,No,,Focus
142,Quality Assessment of Peer-Produced Content in Knowledge Repositories Using Big Data and Social Networks: The Case of Implicit Collaboration in Wikipedia,"This research provides a method for quality assessment of peer-produced content in knowledge repositories using a complementary view of collaboration. Using the definition of collaboration as the action of working with someone to produce something, we identify the aspects of collaboration that the present research on online communities does not consider. To this end, we introduce and define the concept of implicit collaboration and then identify two dimensions and four possible areas of collaboration. In each area, we identify the relevant social network that captures collaboration. Using customized measures on each of the networks that capture various aspects of collaboration, we quantify the utility of implicit collaboration in assessing article quality. Experiments conducted on the complete population of graded English language Wikipedia articles show that all the identified measures improve the predictive accuracy of the existing models by 11.89 percent while improving the class-wise precision by 9-18 percent and the class-wise recall by 5-26 percent. We also find that our method complements the existing quality assessment approaches well. Our research has implications for developing automated quality assessment methods for peer-produced content using big data and social networks.",https://www.semanticscholar.org/paper/3739f4702b05af3586dd9ad8d5d37cd924c8c110,Yes,,
143,Knowledge Quality of Collaborative Editing in Wikipedia: an Integrative Perspective of Social Capital and Team Conflict,"Collaborative editing has become one of the most popular forms of knowledge contribution in virtual communities. Wikipedia— the largest online encyclopaedia— is a representative example of collaborative work. Despite the abundant researches on Wikipedia, to the best of our knowledge, no one has considered the integration of social capital and conflict. Besides, extant literatures on knowledge quality just pay attention to task conflict, while relational conflict is rarely mentioned. Meanwhile, our study proposes the nonlinear relationship between task conflict and knowledge quality instead of linear relationships in prior studies. We also postulate the moderating effect of task complexity. Furthermore, there is little empirical research on the influence of social capital on conflict, especially the distinct effects of cognitive and relational capital. This paper aims at proposing a theoretical model to examine the effect of social capital and conflict, meanwhile taking the task complexity into account. We will make our efforts to verify our research model in the following phases, and we believe that the present work can make some contributions to both research and practice.",https://www.semanticscholar.org/paper/6ca33772623202bcbcd57f0c996e31518078ff43,Maybe,,
144,Content and Quality of Information about Stroke in Wikipedia across Multiple Languages,"Background: Given the high contribution of stroke to the global burden of disease, there is a need for good-quality information on Web platforms such as Wikipedia. Aims: This study aimed to describe the quality of the Wikipedia articles on stroke written in different languages. Methods: We studied the world’s 30 most spoken languages. With the DISCERN score, we evaluated the quality of the information within the Wikipedia articles. Three investigators assessed each of the texts translated to English. We also registered the word count, the number of references, and if the text referred to the emergency status of stroke, cues to suspect a stroke, and allusions to endovascular treatment. Results: There is a Wikipedia article for stroke in 23 out of the 30 languages. The mean DISCERN score was 35 29.9 ± 9.2. Overall quality ranged from 3/5 in 26.1% to 1/5 in 17.4%. Word count had a mean of 36 3,145.8 ± 3,048.9 words, and the texts included a mean of 43.1 ± 57.3 references; 69.6% of the articles referred to stroke as a medical emergency, 52.2% included awareness symptoms, and 34.8% included endovascular management among the stroke treatments. Three pages included steroids as part of the stroke treatment. The DISCERN score was not correlated with the number of speakers, but it was positively correlated with the number of references (r = 0.90, p < 0.001) and the number of words (r = 0.78, p < 0.001) in the articles. Conclusion: The analyzed Wikipedia articles do not contain relevant and up-to-date information to the general population. Further, the content varies widely across the different languages and is missing for some of them. The missing versions disproportionally affect millions of potential information seekers in undeveloped countries.",https://www.semanticscholar.org/paper/796882706963fe29d6febcd63d5f6b6166e76813,No,,"Manual (DISCERN tool means manual, no?)"
145,Equal opportunities in the access to quality online health information? A multi-lingual study on Wikipedia,"Wikipedia is a free, multilingual, and collaborative online encyclopedia. Nowadays, it is one of the largest sources of online knowledge, often appearing at the top of the results of the major search engines, being one of the most sought-after resources by the public searching for health information. The collaborative nature of Wikipedia raises security concerns since this information is used for decision-making, especially in the health area. Despite being available in hundreds of idioms, there are asymmetries between idioms, namely regarding their quality. In this work, we compare the quality of health information on Wikipedia between idioms with 100 million native speakers or more, and also in Greek, Italian, Korean, Turkish, Persian, Catalan and Hebrew, for historical tradition. Quality metrics are applied to health and medical articles in English, maintained by WikiProject Medicine, and their versions in the above idioms. With this, we contribute to a clarification of the role of Wikipedia in the access to health information. We demonstrate differences in both the quantity and quality of information available between idioms. English is the idiom with the highest quality in general. Urdu, Greek, Indonesian, and Hindi achieved lower values of quality.",https://www.semanticscholar.org/paper/a5b7aa53662a843c411555aae6508e24e9adcc1c,Maybe,,probably manual
146,Effects of Stigmergic and Explicit Coordination on Wikipedia Article Quality,"Prior research on Wikipedia has noted the importance of both explicit coordination of edits (i.e., through the article Talk page) and stigmergic coordination (i.e., through the article itself). Using a panel data set of article quality and edits for 23 articles over time, we examine the impact of different kinds of edits on article quality. We find that stigmergically-coordinated edits seem to have the biggest effect on quality, but that explicit coordination of major edits also predicts article quality. The findings have implications for both research on coordination in Wikipedia and for supporting editors.",https://www.semanticscholar.org/paper/15ef01000cda62428cdfc3d27cfe21738bd33f2b,No,,Manual
147,Predicting Information Quality Flaws in Wikipedia by Using Classical and Deep Learning Approaches,"Quality flaws prediction in Wikipedia is an ongoing research trend. In particular, in this work we tackle the problem of automatically predicting five out of the ten most frequent quality flaws; namely: No footnotes, Notability, Primary Sources, Refimprove and Wikify. Different classical and deep learning state-of-the-art approaches were studied. From among the evaluated approaches, some of them always reach or improve the existing benchmarks on the test corpus from the 1st International Competition on Quality Flaw Prediction in Wikipedia; a well-known uniform evaluation corpus from this research field. Particularly, the results showed that under-bagged decision trees with different aggregation rules perform best improving the existing benchmarks for four out the five flaws.",https://www.semanticscholar.org/paper/dbf3799d79675e9d4cd8365511fa42556e4967a3,Yes,,DL
148,Quality assessment of Arabic web content: The case of the Arabic Wikipedia,"With the huge size and large diversity of Arabic web content, machine assessment of document quality acquires added importance. Users are in dire need for quality rating of the material returned in response to their queries. The Wikipedia, with its large metadata, has been a topic of extensive research on document quality assessment. Criteria used include text properties and style parameters, contributor and edit characteristics and multimedia components. In this paper we report on our ongoing work to adapt existing document assessment approaches to Arabic content with concentration on the Arabic Wikipedia and present some of the results. We also try to augment that with features specific to Arabic as well as parameters like author expertise and social media presence. One of our goals is an aggregate measure integrating many of the features into a single document quality index. We plan to use Wikipedia article quality assessment results to train general content assessment methods that can be applied to general content that lacks major Wikipedia features.",https://www.semanticscholar.org/paper/b7821e99b90522f4b8c68f91cbf230fcc6447d13,Yes,,
149,Determining Quality of Articles in Polish Wikipedia Based on Linguistic Features,"Wikipedia is the most popular and the largest user-generated source of knowledge on the Web. Quality of the information in this encyclopedia is often questioned. Therefore, Wikipedians have developed an award system for high quality articles, which follows the specific style guidelines. Nevertheless, more than 1.2 million articles in Polish Wikipedia are unassessed. This paper considers over 100 linguistic features to determine the quality of Wikipedia articles in Polish language. We evaluate our models on 500 000 articles of Polish Wikipedia. Additionally, we discuss the importance of linguistic features for quality prediction.",https://www.semanticscholar.org/paper/2d12a9dae283284e3eaf6e2bd4057e5d335e8389,Yes,,
150,Quality Classification of ASEAN Wikipedia Articles using Statistical Features,"The quality of Wikipedia articles is still the main concerned in all languages. Wikipedia relies mostly on human editors and administrators to provide the quality of content. But the magnitude of Wikipedia content makes locating all instances of article very time consuming. Therefore, we need the automatic quality detection that can help users to evaluate the quality of articles. In this paper, we propose the feature set to applied for the ASEAN language Wikipedia articles. We investigate the statistical features such as # of link, # of infobox, length of article, # of headings, # of files, # of contributors, # of viewer, # of written articles found in other languages, and # of templates applied in the article. The experiments are perform using Naïve Bayes and Decision tree algorithm. We found that the accuracy of Decision tree (96.34%) outperform Naïve Bayes (86.47%). Moreover, we found that the statistical features play an important role in quality classification of Vietnamese, Indonesian, Malaysian, Thai, and Tagalog/Philippines Wikipedia articles.",https://www.semanticscholar.org/paper/f47f2ea88b5412b6a44a264aec67b998a6e3e9ce,Yes,,
151,Feature Analysis for Assessing the Quality of Wikipedia Articles through Supervised Classification,"Nowadays, thanks to Web 2.0 technologies, people have the possibility to generate and spread contents on different social media in a very easy way. In this context, the evaluation of the quality of the information that is available online is becoming more and more a crucial issue. In fact, a constant flow of contents is generated every day by often unknown sources, which are not certified by traditional authoritative entities. This requires the development of appropriate methodologies that can evaluate in a systematic way these contents, based on `objective' aspects connected with them. This would help individuals, who nowadays tend to increasingly form their opinions based on what they read online and on social media, to come into contact with information that is actually useful and verified. Wikipedia is nowadays one of the biggest online resources on which users rely as a source of information. The amount of collaboratively generated content that is sent to the online encyclopedia every day can let to the possible creation of low-quality articles (and, consequently, misinformation) if not properly monitored and revised. For this reason, in this paper, the problem of automatically assessing the quality of Wikipedia articles is considered. In particular, the focus is on the analysis of hand-crafted features that can be employed by supervised machine learning techniques to perform the classification of Wikipedia articles on qualitative bases. With respect to prior literature, a wider set of characteristics connected to Wikipedia articles are taken into account and illustrated in detail. Evaluations are performed by considering a labeled dataset provided in a prior work, and different supervised machine learning algorithms, which produced encouraging results with respect to the considered features.",https://www.semanticscholar.org/paper/dcc5a5d1c7bc3fc9de1080bdf0be71ac2d833385,No,,Not article (preprint)
152,Good Quality Complementary Information for Multilingual Wikipedia,"Many Wikipedia articles lack information, because not all users submit truly complete information to Wikipedia. However, Wikipedia has many language versions that have been developed independently. Therefore, if we supply these complementary information from many language versions, the users must satisfy the amount of information of Wikipedia articles with the complementary information, instead of only one language version of Wikipedia articles. In this study, we specifically examine multilingual Wikipedia and propose a method of extracting good quality complementary information from Wikipedia of other languages. Specifically, we compare Wikipedia articles with less information to those with more information. From Wikipedia articles, which can have the same theme and different languages, we extract different information as complementary information. As described herein, we extract comparison target articles of Wikipedia based on a link graph, because cases exist in which information included in an articles is written in multiple pages of different languages. Furthermore, some low-quality information is extracted as complementary information because Wikipedia articles are written by not only good editors but also bad editors such as vandals. We propose a method to calculate the quality of information based on the editors, and we extract good quality complementary information",https://www.semanticscholar.org/paper/632e144609040754fb03476054db3e5c52807764,No,,Focus
154,Use of linguistic criteria for estimating of wikipedia articles quality,"As far as the question of texts and articles quality is urgent today, in process of a research, the concept of quality for Wikipedia articles was analysed. There were marked out linguistic criteria of quality for technical documentation and scientific articles. Nowadays everyone knows about such informational resource as Wikipedia. Since that day when Wikipedia was just an offshoot of Nupedia (project to produce a free encyclopedia), it has become the most well-known and popular internet encyclopedia with 282 active language editions such as German, French, Russian and Polish and of course the biggest one is English edition, that has more than 5 million articles. It is multilingual, web-based, free content encyclopedia project. It takes the 5 place according to the list of the most popular websites [6]. Wikipedia is written collaboratively by largely anonymous volunteers who write without pay. Anyone, with Internet access, can write and make changes to Wikipedia articles, except in limited cases where editing is restricted to prevent disruption or vandalism. Users can contribute anonymously, under a pseudonym, or, if they choose to, with their real identity. Some users visit Wikipedia to share their knowledge, others to get (acquire) [6]. Every day, hundreds of thousands of visitors from the various parts of the world collectively make tens of thousands of edits and create thousands of new articles to augment the knowledge held by the Wikipedia encyclopedia. All users, old or young, with different backgrounds and people of all cultures can make changes in articles or add their own one. Wikipedia's greatest strengths, weaknesses, and differences all arise because it is open to anyone, it has a large contributor base, and its articles are written according to editorial guidelines and policies. According to the Nature (the first to use peer review that compares Wikipedia and Britannica‘s coverage of science), Wikipedia‘s strongest suit is the speed at which it can updated, a factor not considered by Nature‘s reviewers. Of course it has large amount of uncovered flaws, different kinds of factual errors, omissions or misleading statements. Quality issues, however, concern the creators of Wikipedia. That‘s why, in 2006 during the Opening plenary at Wikimania, Jimmy Wales suggested to concentrate on quality of the articles instead of their number [2]. They created assessment system WP: ASSESS. It uses a letter scheme which estimates how complete the article is, assigning to the definite article its grade. According to this system, Wikipedia has 9 grades: FA (Featured Article) [4], A, GA (Good Article), B, C, Start, Stub, FL (Featured List), List. Each of these grades has special criteria. Featured articles are",https://www.semanticscholar.org/paper/6faf93fd7284ef2ae6a6c0e6937b938c95223350,No,,Not article (report? http://repository.kpi.kharkov.ua/handle/KhPI-Press/49839?locale=en) 
155,Cumulative Experience and Recent Behavior and their Relation to Content Quality on Wikipedia,"Cumulative experience is often seen as a major factor for influencing content quality in collaborative projects such as Wikipedia. However, past studies often utilize cumulative experience based on the quantity of work rather than quality and context. Moreover, the perspective on cumulative experience assumes a final destination for user behavior, whereas much of the literature indicates that user behavior changes over time. This paper aims to address these two factors by providing better descriptions and context to determine their effect on content quality. The study rematerialized these factors based on 1 Communications Facility 495, 516 High Street, Bellingham, WA 98225. U.S.A. Telephone: 360-6502401, Email: michael.tsikerdekis@wwu.edu",https://www.semanticscholar.org/paper/1962d8ff1a78b207088110cda37fb99461e56b73,Maybe,,
156,Quality of monkeypox information in Wikipedia across multiple languages,,https://www.semanticscholar.org/paper/f9b483de972a99113231ac512f4fc04081f3ac22,No,,Not article (letter to editor)
157,On Quality Assesement in Wikipedia Articles Based on Markov Random Fields,"This article investigates the possibility of accurate quality prediction of resources generated by communities based on the crowd-generated content. We use data from Wikipedia, the prime example of community-run site, as our object of study. We define the quality as a distribution of user-assigned grades across a predefined range of possible scores and present a measure of distribution similarity to quantify the accuracy of a prediction. The proposed method of quality prediction is based on Markov Random Field and its Loopy Belief Propagation implementation. Based on our results, we highlight key problems in the approach as presented, as well as trade-offs caused by relying solely on network structure and characteristics, excluding metadata. The overall results of content quality prediction are promising in homophilic networks.",https://www.semanticscholar.org/paper/8bc3af08dc9d06747a6ca83c941a6e2cc03b29fc,Yes,,
158,Quality of Articles in Wikipedia,"The recent research of wikipediais is firs briefly analyzed,especially on the statistics of quality of articles in Wikipedia.Then the automatic evaluating methods of article quality are discussed.The methods mainly include two kinds: the correlation-based analysis and cooperation modeling.Furthermore,we present the open problems of automatic quality evaluation and the possiblepromotions of collective intelligence.",https://www.semanticscholar.org/paper/4afc868db9e2f8b71e1476e02cd786c42767b560,Maybe,,Não encontro o artigo em lado nenhum
159,Assessing Quality Values of Wikipedia Articles Using Implicit Positive and Negative Ratings,"In this paper, we propose a method to identify high-quality Wikipedia articles by mutually evaluating editors and text using implicit positive and negative ratings. One of major approaches for assessing Wikipedia articles is a text survival ratio based approach. However, the problem of this approach is that many low quality articles are misjudged as high quality, because of two issues. This is because, every editor does not always read the whole articles. Therefore, if there is a low quality text at the bottom of a long article, and the text have not seen by the other editors, then the text survives beyond many edits, and the survival ratio of the text is high. To solve this problem, we use a section or a paragraph as a unit of remaining instead of a whole page. This means that if an editor edits an article, the system treats that the editor gives positive ratings to the section or the paragraph that the editor edits. This is because, we believe that if editors edit articles, the editors may not read the whole page, but the editors should read the whole sections or paragraphs, and delete low-quality texts. From experimental evaluation, we confirmed that the proposed method could improve the accuracy of quality values for articles. ",https://www.semanticscholar.org/paper/7e0f730ebf1ef8d1fdcfac69cf66eef0f44c4813,Maybe,,"Bem me parecia que isto estava sempre a aparecer: 60, 79, 117 são bastante semelhantes."
160,On the Assessment of Information Quality in Spanish Wikipedia,"Featured Articles (FA) are considered to be the best articles that Wikipedia has to offer and in the last years, researchers have found interesting to analyze whether and how they can be distinguished from “ordinary” articles. Likewise, identifying what issues have to be enhanced or fixed in ordinary articles in order to improve their quality is a recent key research trend. Most of the approaches developed in these research trends have been proposed for the English Wikipedia. However, few efforts have been accomplished in Spanish Wikipedia, despite being Spanish, one of the most spoken languages in the world by native speakers. In this respect, we present a first breakdown of Spanish Wikipedia’s quality flaw structure. Besides, we carry out a study to automatically assess information quality in Spanish Wikipedia, where FA identification is evaluated as a binary classification task. The results obtained show that FA identification can be performed with an F1 score of 0.81, using a document model consisting of only twenty six features and AdaBoosted C4.5 decision trees as classification algorithm.",https://www.semanticscholar.org/paper/cc4ec9eee2bea21b2ec974d0dc3e47be58f9b3d0,Yes,,"Este Ferretti costuma aparecer muito, nas quality flaws e na spanish wikipedia"
161,Assessing the quality of health-related Wikipedia articles with generic and specific metrics,"Wikipedia is an online, free, multi-language, and collaborative encyclopedia, currently one of the most significant information sources on the web. The open nature of Wikipedia contributions raises concerns about the quality of its information. Previous studies have addressed this issue using manual evaluations and proposing generic measures for quality assessment. In this work, we focus on the quality of health-related content. For this purpose, we use general and health-specific features from Wikipedia articles to propose health-specific metrics. We evaluate these metrics using a set of Wikipedia articles previously assessed by WikiProject Medicine. We conclude that it is possible to combine generic and specific metrics to determine health-related content’s information quality. These metrics are computed automatically and can be used by curators to identify quality issues. Along with the explored features, these metrics can also be used in approaches that automatically classify the quality of Wikipedia health-related articles.",https://www.semanticscholar.org/paper/9a1c237dd3692545d024968b8831c5c00fb29f79,Yes,,
162,Ontology-Based Classifiers for Wikipedia Article Quality Classification,Quality of Wikipedia article is the main issues that need to be solved. This research proposes the ontology-based classification framework that considers the quality of article in term of its comprehensive content which is one of the properties for featured and good articles in Thai Wikipedia. We create concepts or main ideas of articles in three domains using ontology as a knowledge representation. Knowledge based are created using OAM tool that do data mapping and classify the quality of articles via set of rules. We have investigated the ontology approach which combined Naive Bayes classifier and found that the precision of our proposed method outperform traditional Naive Bayes for two times.,https://www.semanticscholar.org/paper/261b08ea302a506eb47bee6a548e39b7e96db899,Maybe,,
163,Ranking Wikipedia article's data quality by learning dimension distributions,"As the largest free user-generated knowledge repository, data quality of Wikipedia has attracted great attention these years. Automatic assessment of Wikipedia article’s data quality is a pressing concern. We observe that every Wikipedia quality class exhibits its specific characteristic along different first-class quality dimensions including accuracy, completeness, consistency and minimality. We propose to extract quality dimension values from article’s content and editing history using dynamic Bayesian network (DBN) and information extraction techniques. Next, we employ multivariate Gaussian distributions to model quality dimension distributions for each quality class, and combine multiple trained classifiers to predict an article’s quality class, which can distinguish different quality classes effectively and robustly. Experiments demonstrate that our approach generates a good performance.",https://www.semanticscholar.org/paper/7e32e541b6bca921823093a51e273d3adc0294d0,Yes,,
164,"Accuracy and quality in historical representation: Wikipedia, textbooks and the Investiture Controversy","Wikipedia’s popularity is unquestioned, but a perceived lack of accuracy and reliability in articles on historical topics prevents historians from embracing it more fully. This article argues that accuracy may be only one component of overall quality. While Wikipedia may have demonstrable shortcomings, it also has strengths in areas such as completeness and accessibility. These strengths appear when historical narratives in Wikipedia are compared to other sources of historical information readily available to American undergraduates. The article compares Wikipedia’s entry on the Investiture Controversy to current scholarship and textbook treatments of the theme. On a broader view of quality, Wikipedia appears in a more favorable light than it does when we employ a narrow focus on accuracy about specific dates and events.",https://www.semanticscholar.org/paper/404b03abab9f704831922a20533c8d3354646592,No,,Focus
165,Automatically Assessing the Need of Additional Citations for Information Quality Verification in Wikipedia Articles,"Quality flaws prediction in Wikipedia is an ongoing research trend. In particular, in this work we tackle the problem of automatically assessing the need of including additional citations for contributing to verify the articles’ content; the so-called Refimprove quality flaw. This information quality flaw, ranks among the five most frequent flaws and represents 12.4% of the flawed articles in the English Wikipedia. Underbagged decision trees, biased-SVM, and centroid-based balanced SVM –three different state-of-the-art approaches– were evaluated, with the aim of handling the existing imbalances between the number of articles’ tagged as flawed content, and the remaining untagged documents that exist in Wikipedia, which can help in the learning stage of the algorithms. Also, a uniformly sampled balanced SVM classifier was evaluated as a baseline. The results showed that under-bagged decision trees with the min rule as aggregation method, perform best achieving an F1 score of 0.96 on the test corpus from the 1st International Competition on Quality Flaw Prediction in Wikipedia; a well-known uniform evaluation corpus from this research field. Likewise, biased-SVM also achieved an F1 score that outperform previously published results.",https://www.semanticscholar.org/paper/8c0b0a019de5e5a59e44ca48fa05df66d48d7f6f,Yes,,
166,Real-Time Prediction of Wikipedia Articles' Quality,,https://repositorio-aberto.up.pt/handle/10216/142752,No,,Not article (thesis)
167,Comparative assessment of three quality frameworks for statistics derived from big data : the cases of Wikipedia page views and Automatic Identification Systems,"National and international statistical agencies are currently experimenting with the production of statistics derived partly or entirely from big data sources. At the same time there have been initiatives in the official statistics community and elsewhere to extend existing quality frameworks to statistics whose production involves the use of this type of data sources. UNECE's suggested framework for the quality of big data and Eurostat's accreditation of big data sources as input data for official statistics are two examples in this regard. The framework proposed in the report on big data of AAPOR (American Association for Public Opinion Research) is an example coming from outside official statistics. These frameworks have been developed based mostly on theoretical considerations, even if early experiments have provided some input. In this paper, we propose to enrich the experience in the application of these frameworks to particular use cases of statistical products based on big data sources in order to assess their suitability, feasibility and completeness. We apply these three quality frameworks in the context of ""experimental"" cultural statistics based on Wikipedia page views and to data from Automatic Identification Systems (AIS) for the production of transport statistics.",https://www.semanticscholar.org/paper/fb77e3053150d7c4e42ca164b3d7b92140cfaf16,No,,Focus
168,Contraception in the German-language Wikipedia: a content and quality analysis.,"BACKGROUND: Adolescents and adults today often obtain information about contraception from the Internet, especially from the online encyclopedia Wikipedia. This is because Google searches usually return Wikipedia entries as top hits. RESEARCH AIM: Against this background, the aim of the current study is to systematically analyze for the first time the content and quality of Wikipedia articles on contraceptive methods. Five central quality dimensions are examined: the degree of correctness (research question RQ1), completeness (RQ2), neutrality (RQ3), comprehensibility (RQ4), and currency (RQ5) of the contraceptive information - and on this basis the overall quality of the articles (RQ6). MATERIALS AND METHODS: A sample of all German-language Wikipedia articles on all contraceptive methods was formed (N = 25). These articles were analyzed by three independent, trained coders using a codebook that was developed based on the current state of the research and tested for reliability. Data analysis was performed using SPSS. The study is preregistered and all data, materials, and analysis scripts are publicly available. RESULTS: The 25 Wikipedia articles on contraceptive methods were found to vary widely in content quality. While they showed good quality on average in terms of correctness (RQ1) and neutrality (RQ3), they scored mediocre in terms of completeness (RQ2), comprehensibility (RQ4), and currency (RQ5), resulting in moderate overall quality (RQ6). DISCUSSION: More research as well as practice measures are needed to further assess and improve the quality of contraceptive information on Wikipedia and in other social media in a more targeted way.",https://www.semanticscholar.org/paper/48cf2c6b9df60f7fb1e7ee71732ff60c8a734ff3,No,,"Manual (por acaso não percebi bem se é mesmo manual ou não. podemos passar, mas vai ser excluído pq o artigo é em alemão)."
169,Assessing the Quality of Thai Wikipedia Articles Using Concept and Statistical Features,"The quality evaluation of Thai Wikipedia articles relies on user consideration. There are increasing numbers of articles every day therefore the automatic evaluation method is needed for user. Components of Wikipedia articles such as headers, pictures, references, and links are useful to indicate the quality of articles. However readers need complete content to cover all of concepts in that article. The concept features are investigated in this work. The aim of this research is to classify Thai Wikipedia articles into two classes namely high-quality and low-quality class. Three article domains (Biography, Animal, and Place) are testes with decision tree and Naive Bayes. We found that Naive Bayes gets high TP Rate compared to decision tree in every domain. Moreover, we found that the concept feature plays an important role in quality classification of Thai Wikipedia articles. ",https://www.semanticscholar.org/paper/9ae7b7133f62363a386922631bf4637e182815a4,Yes,,
170,Interactive Quality Analytics of User-generated Content,"Digital libraries and services enable users to access large amounts of data on demand. Yet, quality assessment of information encountered on the Internet remains an elusive open issue. For example, Wikipedia, one of the most visited platforms on the Web, hosts thousands of user-generated articles and undergoes 12 million edits/contributions per month. User-generated content is undoubtedly one of the keys to its success but also a hindrance to good quality. Although Wikipedia has established guidelines for the “perfect article,” authors find it difficult to assert whether their contributions comply with them and reviewers cannot cope with the ever-growing amount of articles pending review. Great efforts have been invested in algorithmic methods for automatic classification of Wikipedia articles (as featured or non-featured) and for quality flaw detection. Instead, our contribution is an interactive tool that combines automatic classification methods and human interaction in a toolkit, whereby experts can experiment with new quality metrics and share them with authors that need to identify weaknesses to improve a particular article. A design study shows that experts are able to effectively create complex quality metrics in a visual analytics environment. In turn, a user study evidences that regular users can identify flaws, as well as high-quality content based on the inspection of automatic quality scores.",https://www.semanticscholar.org/paper/b8f287f6f3ed28b4e494cfc05a39b4df1e36a87e,Yes,,Visualization
171,Anfis based models for accessing quality of wikipedia articles,"Wikipedia is a free, web-based, collaborative, multilingual encyclopedia project supported by the non-profit Wikimedia Foundation. Due to the free nature of Wikipedia and allowing open access to ev ...",https://www.semanticscholar.org/paper/c5dbe7d3f750a60ec8cfe56fbb8d3834b7ef0dee,No,,No article
172,Structure-Based Features for Predicting the Quality of Articles in Wikipedia,"Success of Wikipedia is decidedly due to the free availability of high quality articles across many different expertise areas. If most of these resolute collaborations between authoritative users might constitute referenceable sources, Wikipedia is not sheltered from well-identified problems regarding articles quality, e.g., reputability of third-party sources and vandalism. Because of the huge number of articles and the intensive edit rate, it is not reasonable to even consider the manual evaluation of the content quality of each article. In this paper, we tackle the problem of modeling and predicting the quality of articles in collaborative platforms. We propose a quality model integrating both temporal and structural features captured from the implicit peer review process enabled by Wikipedia. A generic HITS-like framework is developed and able to capture both the quality of the content and the authority of the associated authors. Notably, a mutual reinforcement principle held between articles quality and author’s authority is exploited in order to take advantage of the collaborative graph generated by the users. Experiments conducted on a set of representative data from Wikipedia show the effectiveness of the computed indicators both in an unsupervised and supervised scenario.",https://www.semanticscholar.org/paper/83fe1663a16d7e8e3d1988799bd9dc4cdbf2e16a,Yes,,
173,Evaluating Article Quality and Editor Reputation in Wikipedia,"We study a novel problem of quality and reputation evaluation for Wikipedia articles. We propose a difficult and interesting question: How to generate reasonable article quality score and editor reputation in a framework at the same time? In this paper, We propose a dual wing factor graph(DWFG) model, which utilizes the mutual reinforcement between articles and editors to generate article quality and editor reputation. To learn the proposed factor graph model, we further design an efficient algorithm. We conduct experiments to validate the effectiveness of the proposed model. By leveraging the belief propagation between articles and editors, our approach obtains significant improvement over several alternative methods(SVM, LR, PR, CRF). ",https://www.semanticscholar.org/paper/59ef2fa73fa249be1f5043da108464b7c3f7427c,Maybe,,
174,Revision and Quality Analysis of Library and Information Science Concepts in Wikipedia,Wikipedia is public encyclopedia contains articles on different subjects and different topics. The objective of the study is to evaluate the standard and status of revision of the library related articles. History page of Wikipedia article contains statistics of page from where the status and standard of the page has been derived. It has been concluded that Wikipedia covers 67 concepts of DDC 23rd edition arrived in 020 class (Library and Information Science Class). The content of Library and Information Science (LIS) related articles in Wikipedia was not enough informative as per Wikiproject. The concepts related to information science i.e. World Wide Web has been edited more times and it has been edited by more editors.,https://www.proquest.com/openview/b72aa5a15431b3df4a0bd5c2d0a454b4/1?pq-origsite=gscholar&cbl=54903,No,,Focus
175,Citation Detective : a Public Dataset to Improve and Quantify Wikipedia Citation Quality at Scale,"Machine learning models designed to improve citation quality in Wikipedia, such as text-based classifiers detecting sentences needing citations (“Citation Need” models), have received a lot of attention from both the scientific and the Wikimedia communities. However, due to their highly technical nature, the accessibility of such models is limited, and their usage generally restricted to machine learning researchers and practitioners. To fill this gap, we present Citation Detective , a system designed to periodically run Citation Need models on a large number of articles in English Wikipedia, and release public, usable, monthly data dumps exposing sentences classified as missing citations. By making Citation Need models usable to the broader public, Citation Detective opens up new opportunities for research and applications. We provide an example of a research direction enabled by Citation Detective , by conducting a large-scale analysis of citation quality in Wikipedia, showing that article citation quality is positively correlated with article quality, and that articles in Medicine and Biology are the most well sourced in English Wikipedia. The Citation Detective data and source code will be made publicly available and are being integrated with community tools for citation improvement such as Citation Hunt .",https://www.semanticscholar.org/paper/2fa03900c85676a9e75dd6ab6bce7009f4cd875c,Yes,,
176,"A Strategy Oriented, Machine Learning Approach to Automatic Quality Assessment of Wikipedia Articles","A Strategy Oriented, Machine Learning Approach to Automatic Quality Assessment of Wikipedia Articles Gabriel De La Calzada This work discusses an approach to modeling and measuring information quality of Wikipedia articles. The approach is based on the idea that the quality of Wikipedia articles with distinctly different profiles needs to be measured using different information quality models. To implement this approach, a software framework written in the Java language was developed to collect and analyze information of Wikipedia articles. We report on our initial study, which involved two categories of Wikipedia articles: ”stabilized” (those, whose content has not undergone major changes for a significant period of time) and ”controversial” (articles that have undergone vandalism, revert wars, or whose content is subject to internal discussions between Wikipedia editors). In addition, we present simple information quality models and compare their performance on a subset of Wikipedia articles with the information quality evaluations provided by human users. Our experiment shows that using special-purpose models for information quality captures user sentiment about Wikipedia articles better than using a single model for both categories of articles.",https://www.semanticscholar.org/paper/ca0385429020b68b3c44e2dfb1550eca74a20b21,No,,Not article (thesis)
180,"Wikipedia-based Corpora for Analyzing Revisions, Discussions and Text Quality in Collaborative Writing","Wikipedia is both a valuable resource and a remarkable example of a non-standard data source for computational linguistics research. It is a unique source of authentic corpus material to study collaborative writing in the Web and offers terabytes of collaboratively constructed content in numerous domains. Due to the magnitude, diversity and the collaborative construction of its content, creating task-specific corpora for computational linguistics from Wikipedia is a non-trivial task. For example, the semiand sometimes unstructured data makes its segmentation difficult. In this presentation, we report about our ongoing work on several Wikipedia-based corpora tailored to support computational linguistics research with respect to:",https://www.semanticscholar.org/paper/56e0e88e19504c7c21478616ed5c1d41a81c5045,No,,Not article (report)
181,Understanding the 'Quality Motion' of Wikipedia Articles Through Semantic Convergence Analysis,"To better inform the users of the articles quality, Wikipedia assigns quality labels to the articles. While most of the existing studies of the Wikipedia phenomenon took the quality ratings provided by Wikipedia as the outcome variable of their research, a few yet growing number of studies ask expert raters to rate the quality of selected Wikipedia articles because of their doubts in Wikipedia’s ratings. This study aims to check if Wikipedia’s ratings really reflect its stated criteria. According to Wikipedia criteria, having abundant and stable content is the key to article’s quality promotion; we therefore examine the content change in terms of quantity change and content stability by showing the semantic convergence. We found out that the quantity of content change is significant in the promoted articles, which complies with Wikipedia’s stated criteria. ",https://www.semanticscholar.org/paper/00c6bd451cdedd9e4ad551978c41f0acd3f751ca,Maybe,,
182,Quality contents creation in a commons-based peer production on line environment: the it.wikipedia experience,"The increasing growth of Wikipedia poses many questions about its organizational model and its development as a freeopen knowledge repository. Yochai Benkler describes Wikipedia as a CBPP (commons-based peer production) system: a platform which enables users to easily generate knowledge contents and to manage them collaboratively and on freevoluntary basis. The quality of its output is one of the main concerns related to Wikipedia. How would a CBPP environment guarantee at the same time the openness of its organization and a good level of accreditation? Which aspects of the project have more influence on quality? The paper offers an overview of one of the quality processes in it.wiki (Italian Wikipedia): the Vetrina section (Featured Articles). It also suggests an explanation to quality accreditation issue which questions Benkler's hypothesis. Thanks to a qualitative analysis carried out through in-depth interviews to Wikipedia users and through a period of ethnographic observation, the paper outlines Vetrina's organization and the social factors related to quality definition. The goal of the analysis is to give a better understanding of co-generation of contents processes and at the same time it tries to investigate quality assessment in one of the best known open knowledge on line project.",https://www.semanticscholar.org/paper/4e8583c37086c87ca9d1f2e9fc8fb0b8e5b0f8d5,No,,Focus
183,The Wikipedia Medical Student: Comparing the Quality of Vascular Surgery Topics Across Two Commonly Used Educational Resources,"Objectives: Medical students commonly refer to Wikipedia.org as their preferred online resource for medical education. To date, the quality and readability of common vascular disorders on Wikipedia has not been evaluated or compared against a standard textbook of surgery. The aims of this study were to (1) assess the quality of Wikipedia.org articles against the equivalent chapters in a standard medical school textbook of surgery; (2) identify any errors of omission; and (3) compare the readability of both resources using ease-of-reading and grade-level tools. Methods: Eight fundamental topics in vascular surgery were analyzed. The articles were accessed from Wikipedia.org through its native search engine, and equivalent chapters from Schwartz Principles of Surgery, 9th-edition, (SPOS) were compared. Quality was evaluated using the DISCERN tool, errors of omission were evaluated using a proprietary scoring system designed by author(s), and readability was evaluated using the Flesch-Reading-Ease test, Gunning-fog score, Coleman-Liau Index, SMOG Index, Automated-Readability-Index, and the Flesch-Kincaid grade level. Results: SPOS scored highest in quality, with perfect DISCERN scores of 5 and had the lowest errors of omission, whereas Wikipedia.org scored best for readability, being, on average, understandable by most grade 12 educated students. Interobserver concordances validated these results. Conclusions: SPOS is superior to Wikipedia.org when critiquing quality and errors of omission, whereas Wikipedia.org is superior to the textbook when considering the ease of reading. This study recommends the use of surgical textbooks as a primary learning resource for medical students, and cautions the use of Wikipedia.org due to its significant rate of omissions.",https://www.semanticscholar.org/paper/1d6c8c496b89d888fec2d178673a9a6514f0e353,No,,Manual
185,The Category Structure in Wikipedia: To Analyze and Know Its Quality Using K-Core Decomposition,"Wikipedia is a famous and free encyclopedia. A network based on its category structure is built and then analyzed from various aspects, such as the connectivity distribution, evolution of the overall topology. As an innovative point of our paper, the model that is on the base of the k-core decomposition is used to analyze evolution of the overall topology and test the quality (that is, the error and attack tolerance) of the structure when nodes are removed. The model based on removal of edges is compared. Our results offer useful insights for the growth and the quality of the category structure, and the methods how to better organize the category structure.",https://www.semanticscholar.org/paper/7a1851c877117001252cf90fc2ab26599695ed40,No,,"Focus (sinceramente não percebi nada do abstract, mas acho que não tem a ver, certo? diferente definição de qualidade)"
186,Building Quality in Wikipedia: A Theoretical Approach,,https://www.semanticscholar.org/paper/210306511daff2d670b8a3b9e3047deae363ec37,No,,Not article (thesis)
187,Quality assessment of Wikipedia content using topic models,"The web has become a large knowledge provider for society, allowing people to not just consume information but also produce it. Collaborative documents bring some significant advantages and decentralization, but they also raise questions concerning its quality. In this work, we explore the quality assessment on collaborative documents using these documents' topics. The proposed approach improved in 3.2% the accuracy of quality assesment of Wikipedia content. Then, the main contribution in this paper is an analysis of how we can use topic modelling in order to improve quality prediction performance.",https://www.semanticscholar.org/paper/5548a6d03196d6f9375cec984921336050baf485,Maybe,,
189,Promoting Quality in Wikipedia through Enculturation,,https://www.cs.ubc.ca/~bestchai/papers/group09_workshop.pdf,No,,Not article (report)
193,Quality of References Supporting Urologic Articles on Wikipedia,"Background: Wikipedia is an easily available and commonly used source of medical information for patients and practitioners. Its editing guidelines state that information added to the site should be accompanied by in-line citations to reliable (preferably secondary) sources to meet the website’s standards for verifiability. We hypothesized that a large fraction of these citations would be to popular media and to scientific works of relatively low academic impact. Objective: To characterize the works cited in the most frequently accessed Wikipedia articles on urologic diseases and interventions. Methods: Of the 1500 most-viewed articles in the Medicine WikiProject, 24 were relevant to urology. From these 24, the top 10 most-viewed urology-related articles were selected for citation analysis. Sources in in-line citations, defined as “ref” elements with unique or absent “name” attributes, were categorized by publisher and assigned one of 6 publication types. These types were: original scientific research, systematic reviews, scholarly non-systematic reviews (including books, editorials, and guidelines), medical references directed at the lay person (e.g. WebMD), popular media, and other. Finally, citation counts were retrieved using the Scopus database for references with digital object identifiers (DOI) or PubMed identification numbers (PMID). Results: The 24 articles included in the study were accessed by Wikipedia users an average of 2019 times per article per day (range 987-4,699) and contained an average of 40.5 citations (range 5-189). Of the 567 references cited in the 10 most viewed articles, 25.9% (147) were original research, 8.8% (50) were systematic reviews, 47.1% (267) were other scholarly reviews, 6.0% (34) were lay-directed medical resources, and only 7.9% (45) were popular media. 245 separate publishers were cited (208 scientific), of which the Journal of Urology was the most common (21 citations) followed by the Cochrane Database of Systematic Reviews (20). Of the 262 references with citation counts the Scopus database, 74.8% (196) were cited by at least 10 other scholarly articles. The median reference was cited by 26 other scientific articles (mean 151). Conclusions: The vast majority of sources backing information in urologic articles on Wikipedia are scientific books and peer-reviewed journals targeted to a professional audience. The plurality of these are textbooks and reviews of the literature. The vast majority of scientific articles cited have achieved at least a moderate level of recognition within their field. These findings suggest that the Wikipedia editing process for these articles does favor sources of relatively high scientific quality. Further research is required to determine whether the relatively high quality and quantity of sources cited implies high quality of the Wikipedia articles themselves. ",https://www.semanticscholar.org/paper/75739ccfb7d74b3a870b1f78f50973e64b8e3f74,No,,Manual
194,Information Quality dimensions of Information Science articles in English Wikipedia: exploring the relationship between ranking of the Wikipedia articles in search …,"Purpose: Investigating the Information Quality dimensions of Information Science articles in English Wikipedia and exploring the relationship between ranking of the Wikipedia articles in search engines and the quality of the articles. Methodology: Using a descriptive- survey method and based on the identified framework, different indices related to the quality dimensions including credibility, completeness, complexity, awareness, stability, timeliness, and volatility are computed for the selected sample. Findings: The examined article showed a relatively favorable situation in terms of complexity and volatility but they did not have a very proper situation in terms of completeness, complexity and awareness. It was shown that although the Bing performed a better job in terms of retrieving Wikipedia articles in the top ten results compared to the other examined search engines there was no significant difference in the ranking of the articles among the evaluated search engines. There is no significant relation between the rank of a retrieved article in a search engine and the information quality dimensions except for the timeliness. Conclusion: Wikipedia authors should try to improve the other dimensions of the information quality including completeness, complexity and awareness by enhancing their writing skills. The lack of a relationship between some components of the Information Quality and the rank of Wikipedia articles in the search engines may be related to the financial and commercial actions behind the scene of the information retrieval systems which have restricted them not to reveal their ranking algorithms.",http://nastinfo.nlai.ir/article_2263.html?lang=en,No,,Manual
196,Quality of open content in Wikipedia: towards a broader view of expertise?,,https://www.semanticscholar.org/paper/2152fe52e54c568e2bcde938508de1913f3947a4,No,,No access
197,Predicting Low-Quality Wikipedia Articles Using User’s Judgements,"Wikipedia has become the most popular on-line encyclopedia. Millions of users rely on it to obtain desired knowledge and thus it becomes important and practical to model the quality of Wikipedia articles and to have inferior contents which bother readers or even mislead readers to be predicted. While identifying low-quality articles with manual efforts is a possible solution, it costs too much manpower and is too time-consuming. In this paper, we utilize article ratings from Wikipedia users for the first time to assess article quality. We define “low-quality” based on those ratings and design automatic methods to identify potential low-quality articles. More specifically, we formulate the problem as a set of binary classification problems and label articles according to whether they are “low-quality”. We compare two baseline algorithms and Logistic Regression algorithm, and the results indicate that it is promising to design effective and efficient automatic solutions for the task. We believe that our work is important for ensuring the quality of Wikipedia, as well as other knowledge markets. ",https://www.semanticscholar.org/paper/f9ac07db4730ee15b657cab2e20c6ca6ed366336,Yes,,
199,An investigation of the relationship between the amount of extra-textual data and the quality of Wikipedia articles,"Wikipedia, a web-based collaboratively maintained free encyclopedia, is emerging as one of the most important websites on the internet. However, its openness raises many concerns about the quality of the articles and how to assess it automatically. In the Portuguese-speaking Wikipedia, articles can be rated by bots and by the community. In this paper, we investigate the correlation between these ratings and the count of media items (namely images and sounds) through a series of experiments. Our results show that article ratings and the count of media items are correlated.",https://www.semanticscholar.org/paper/48f6cd91cf9257329719bc69cb3457a3a35edde5,Yes,,
200,Is Wikipedia a High Quality Evidence-Based Resource?,,https://www.semanticscholar.org/paper/44150e8873341b7930d117f98d77cc2333146eb8,No,,Not article (blog post? )
202,Monitoring network structure and content quality of signal processing articles on wikipedia,"Wikipedia has become a widely-used resource on signal processing. However, the freelance-editing model of Wikipedia makes it challenging to maintain a high content quality. We develop techniques to monitor the network structure and content quality of Signal Processing (SP) articles on Wikipedia. Using metrics to quantify the importance and quality of articles, we generate a list of SP articles on Wikipedia arranged in the order of their need for improvement. The tools we use include the HITS and PageRank algorithms for network structure, crowdsourcing for quantifying article importance and known heuristics for article quality.",https://www.semanticscholar.org/paper/ef732209c1b9fdcd014f9639533c5658593f9ac5,Yes,,
203,Quick fix or citable source?: Carol Haigh investigates the quality and rigour of using Wikipedia in support of academic research,,https://www.semanticscholar.org/paper/afe5f366486d11b6086897219128cc4b7e7a7c3e,No,,No access
204,"Internal Bonding, External Bridging and Functional Diversity: Impact of Social Capital on the Quality of Wikipedia Articles","This research focuses on the question of why Wikipedia articles are different in quality. Since Wikipedia articles are developed in an open and social environment, our work investigates if social capital of Wikipedia contributors plays a role in determining the quality of Wikipedia articles. In this study, we focus on three major factors of social capital with respect to teams of contributors working on different Wikipedia articles: internal bonding, external bridging and functional diversity. An empirical analysis of Wikipedia articles using quantitative techniques suggests that all these three major factors have a significant impact on the quality of Wikipedia articles. These results have implications for developing automated techniques for quality assessment of Wikipedia and also provide insights into improving quality of these articles.",https://www.semanticscholar.org/paper/127e19e615fb2f90b8b123de16396c878ae95c4d,No,,"Focus. (Seems that automatic assessment is more on the future work aspect. Makes sense, 29 is more recent and from the same authors and appears to be more related to automatic methods)"
207,Towards the automatic evaluation of stylistic quality of natural texts: constructing a special-­purpose corpus of stylistic edits from the Wikipedia revision history,"This thesis proposes an approach to automatic evaluation of the stylistic quality of natural texts through data-driven methods of Natural Language Processing. Advantages of data driven methods and their dependency on the size of training data are discussed. Also the advantages of using Wikipedia as a source for textual data mining are presented. The method in this project crucially involves a program for quick automatic extraction of sentences edited by users from the Wikipedia Revision History. The resulting edits have been compiled in a large-scale corpus of examples of stylistic editing. The complete modular structure of the extraction program is described and its performance is analyzed. Furthermore, the need to separate stylistic edits stylistic edits from factual ones is discussed and a number of Machine Learning classification algorithms for this task are proposed and tested. The program developed in this project was able to process approximately 10% of the whole Russian Wikipedia Revision history (200 gigabytes of textual data) in one month, resulting in the extraction of more than two millions of user edits. The best algorithm for the classification of edits into factual and stylistic ones achieved 86.2% cross-validation accuracy, which is comparable with state-of-the-art performance of similar models described in published papers.",https://www.semanticscholar.org/paper/995830ab9b296b17c05fd40d3ec67965bf1d060e,No,,Not article (thesis)
208,CMPSCI 645 Final Report Quality Evaluation of Wikipedia Articles,"An increasing number of people rely on the World Wide Web to access information. That has led to the development of many encyclopedia-like online repositories for specialized information. Arguably the most popular such site is Wikipedia, an online encyclopedia, which relies on collaborative content development. In Wikipedia, anyone (including anonymous authors) can create new articles, or edit and contribute to existing articles. Wikipedia relies on the idea that when millions of users share their knowledge and expertise, the end result is highquality, easily accessible information. In practice, however, malicious or simply ill-informed users can remove good content and add bad content.",https://www.semanticscholar.org/paper/37e299d93f15ddfca764eb1b9632bab3e4ba67d4,No,,Not article (report)
209,Dark matter at 5800 An investigation of the quality of user-contributed entries on the topic of dark matter in Wikipedia and other types of texts,"Statistics have shown that Wikipedia is very frequently used by the general public and that its articles rank high in online search engines. However, the accuracy and general quality of Wikipedia have been debated over the years. This study aims to investigate the quality of Wikipedia by expert reviewers pertaining to the accuracy, currency, breadth, readability, im-ages, structure, neutrality and relevance of a Wikipedia entry on dark matter. The entry has over 5800 edits. A comparison to two other centrally controlled sources, edited by acclaimed experts was also made. Data was collected by asking a number of qualified experts to review and rate three different texts, one published by NASA, one by Encyclopaedia Britannica and one from the English language version of Wikipedia. An interview with one of the experts was also carried out. The results showed that Wikipedia scored better than the other texts in all examined variables except for readability. Wikipedia was the preferred source by all but one panel members and its credibility was considered high. This review indicates that both NASA’s and Encyclopaedia Britannica’s articles on dark matter had a lower degree of quality than expected considering their brands’ high level of credibility. This report encourages the use of Wikipedia both for reference and as a platform to communicate, revise and correct re-search.",https://www.semanticscholar.org/paper/9f02b963b54e92d3d32a58295a29e84b47fcb406,No,,Manual
210,Wiki Evolution dataset: English Wikipedia revision articles represented by quality attributes,"Este artigo descreve a criação e disponibilização da base de dados de evolução de artigos da Wikipédia. A base é caracterizada por atributos de qualidades e a classe de qualidade dos artigos em determinada data, sendo cada instância entendida como revisão. Esta base pode ser utilizada para estudos relacionados com classificação automática de qualidade que considerem o histórico de revisão do artigo e entendimento de como o conteúdo e qualidade dos artigos evoluem ao longo do tempo nessa plataforma colaborativa.",https://www.semanticscholar.org/paper/9ded22c4dad8607fd8298ad21735e38027eb41b0,No,,Focus
211,Polish Wikipedia Articles Quality Assessment,,,No,,No access
262,Cultural diversity of quality of information on Wikipedias,"This article explores the relationship between linguistic culture and the preferred standards of presenting information based on article representation in major Wikipedias. Using primary research analysis of the number of images, references, internal links, external links, words, and characters, as well as their proportions in Good and Featured articles on the eight largest Wikipedias, we discover a high diversity of approaches and format preferences, correlating with culture. We demonstrate that high‐quality standards in information presentation are not globally shared and that in many aspects, the language culture's influence determines what is perceived to be proper, desirable, and exemplary for encyclopedic entries. As a result, we demonstrate that standards for encyclopedic knowledge are not globally agreed‐upon and “objective” but local and very subjective.",https://www.semanticscholar.org/paper/ef210d43c0ce301c41e2a7a81ee724874a7e7181,Yes,,
278,"When the levee breaks: without bots, what happens to Wikipedia's quality control processes?","In the first half of 2011, ClueBot NG -- one of the most prolific counter-vandalism bots in the English-language Wikipedia -- went down for four distinct periods, each period of downtime lasting from days to weeks. In this paper, we use these periods of breakdown as naturalistic experiments to study Wikipedia's heterogeneous quality control network, which we analyze as a multi-tiered system in which distinct classes of reviewers use various reviewing technologies to patrol for different kinds of damage at staggered time periods. Our analysis showed that the overall time-to-revert edits was almost doubled when this software agent was down. Yet while a significantly fewer proportion of edits made during the bot's downtime were reverted, we found that those edits were later eventually reverted. This suggests that other agents in Wikipedia took over this quality control work, but performed it at a far slower rate.",https://www.semanticscholar.org/paper/ec2642fb3e3183ecc238ca6c21f59b51d0191c2a,No,,Focus
338,On the Feasibility of External Factual Support as Wikipedia's Quality Metric,"Developing metrics to estimate the information quality of Wikipedia articles is an interesting and important research area. In this article, we propose and analyse the feasibility, of a new quality metric based on the “external factual support” of an article. The rationale behind this metric is identified, a formal definition of the metric is presented and some implementation aspects are introduced. Preliminary results show the feasibility of our proposal and its potential to discriminate high quality versus low quality Wikipedia’s articles.",https://www.semanticscholar.org/paper/e1f3c58d3b101ae9452a210297dd1150f056a01d,Yes,,
359,Quality of Wikipedia Articles: Analyzing Features and Building a Ground Truth for Supervised Classification,"Wikipedia is nowadays one of the biggest online resources on which users rely as a source of information. The amount of collaboratively generated content that is sent to the online encyclopedia every day can let to the possible creation of low-quality articles (and, consequently, misinformation) if not properly monitored and revised. For this reason, in this paper, the problem of automatically assessing the quality of Wikipedia articles is considered. In particular, the focus is (i) on the analysis of groups of hand-crafted features that can be employed by supervised machine learning techniques to classify Wikipedia articles on qualitative bases, and (ii) on the analysis of some issues behind the construction of a suitable ground truth. Evaluations are performed, on the analyzed features and on a specifically built labeled dataset, by implementing different supervised classifiers based on distinct machine learning algorithms, which produced promising results.",https://www.semanticscholar.org/paper/e89231c16e56289654b0bcd62ae0b8d9a2af6e11,Yes,,
369,Contributing to question of the quality of Wikipedia and the possibilities of its use in the classroom,,,No,,No access
376,M1042 The Quality of Open Access and Open Source Internet Material in Gastroenterology: Is Wikipedia Appropriate for Knowledge Transfer to Patients?,,https://www.semanticscholar.org/paper/cc18ef19798c727052ea28968940b18cc5874492,No,,No access
379,Wikipedia as a Source of Health Information for the Public: Systematic Analysis of Quality and Readability of 8 Common Rheumatic Diseases Articles,,,No,,No access