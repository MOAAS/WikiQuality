@article{Dang2017,
   abstract = {Wikipedia is considered as the largest knowledge repository in the history of humanity and plays a crucial role in modern daily life. Assigning the correct quality class to Wikipedia articles is an important task in order to provide guidance for both authors and readers of Wikipedia. The manual review cannot cope with the editing speed of Wikipedia. An automatic classification is required to classify the quality of Wikipedia articles. Most existing approaches rely on traditional machine learning with manual feature engineering, which requires a lot of expertise and effort. Furthermore, it is known that there is no general perfect feature set because information leak always occurs in feature extraction phase. Also, for each language of Wikipedia, a new feature set is required. In this paper, we present an approach relying on deep learning for quality classification of Wikipedia articles. Our solution relies on Recurrent Neural Networks (RNN) which is an endto-end learning technique that eliminates disadvantages of feature engineering. Our approach learns directly from raw data without human intervention and is language-neutral. Experimental results on English, French and Russian Wikipedia datasets show that our approach outperforms state-of-the-art solutions.},
   author = {Quang Vinh Dang and Claudia Lavinia Ignat},
   doi = {10.1145/3125433.3125448},
   isbn = {9781450351874},
   journal = {Proceedings of the 13th International Symposium on Open Collaboration, OpenSym 2017},
   keywords = {Deep learning,Document quality,End-to-end learning,Long-Short Term Memory (LSTM),Recurrent Neural Network (RNN),Wikipedia},
   month = {8},
   publisher = {Association for Computing Machinery, Inc},
   title = {An end-to-end learning solution for assessing the quality of Wikipedia articles},
   url = {https://doi.org/10.1145/3125433.3125448},
   year = {2017},
}
@article{Dalip2009,
   abstract = {The old dream of a universal repository containing all the human knowledge and culture is becoming possible through the Internet and the Web. Moreover, this is happening with the direct collaborative, participation of people. Wikipedia is a great example. It is an enormous repository of information with free access and edition, created by the community in a collaborative manner. However, this large amount of information, made available democratically and virtually without any control, raises questions about its relative quality. In this work we explore a significant number of quality indicators, some of them proposed by us and used here for the first time, and study their capability to assess the quality of Wikipedia articles. Furthermore, we explore machine learning techniques to combine these quality indicators into one single assessment judgment. Through experiments, we show that the most important quality indicators are the easiest ones to extract, namely, textual features related to length, structure and style. We were also able to determine which indicators did not contribute significantly to the quality assessment. These were, coincidentally, the most complex features, such as those based on link analysis. Finally, we compare our combination method with state-of-the-art solution and show significant improvements in terms of effective quality prediction. Copyright 2009 ACM.},
   author = {Daniel Hasan Dalip and Marcos André Gonçalves and Marco Cristo and Pável Calado},
   doi = {10.1145/1555400.1555449},
   isbn = {9781605586977},
   issn = {15525996},
   journal = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries},
   keywords = {Machine learning,Quality assessment,SVM,User Issues] General Terms Human Factors,Wikipedia},
   pages = {295-304},
   title = {Automatic quality assessment of content created collaboratively by web communities: A case study of wikipedia},
   url = {http://en.wikipedia.org/wiki/Wikipedia},
   year = {2009},
}
@article{Laurent2009,
   abstract = {Objective: To determine the significance of the English Wikipedia as a source of online health information. Design: The authors measured Wikipedia's ranking on general Internet search engines by entering keywords from MedlinePlus, NHS Direct Online, and the National Organization of Rare Diseases as queries into search engine optimization software. We assessed whether article quality influenced this ranking. The authors tested whether traffic to Wikipedia coincided with epidemiological trends and news of emerging health concerns, and how it compares to MedlinePlus. Measurements: Cumulative incidence and average position of Wikipedia® compared to other Web sites among the first 20 results on general Internet search engines (Google®, Google UK®, Yahoo®, and MSN®), and page view statistics for selected Wikipedia articles and MedlinePlus pages. Results: Wikipedia ranked among the first ten results in 71-85% of search engines and keywords tested. Wikipedia surpassed MedlinePlus and NHS Direct Online (except for queries from the latter on Google UK), and ranked higher with quality articles. Wikipedia ranked highest for rare diseases, although its incidence in several categories decreased. Page views increased parallel to the occurrence of 20 seasonal disorders and news of three emerging health concerns. Wikipedia articles were viewed more often than MedlinePlus Topic (p = 0.001) but for MedlinePlus Encyclopedia pages, the trend was not significant (p = 0.07-0.10). Conclusions: Based on its search engine ranking and page view statistics, the English Wikipedia is a prominent source of online health information compared to the other online health information providers studied. © 2009 J Am Med Inform Assoc.},
   author = {Michaël R. Laurent and Tim J. Vickers},
   doi = {10.1197/JAMIA.M3059},
   issn = {10675027},
   issue = {4},
   journal = {Journal of the American Medical Informatics Association},
   month = {7},
   pages = {471-479},
   pmid = {19390105},
   title = {Seeking Health Information Online: Does Wikipedia Matter?},
   volume = {16},
   year = {2009},
}
@article{Chevalier2010,
   abstract = {As Wikipedia has become one of the most used knowledge bases worldwide, the problem of the trustworthiness of the information it disseminates becomes central. With WikipediaViz, we introduce five visual indicators integrated to the Wikipedia layout that can keep casual Wikipedia readers aware of important meta-information about the articles they read. The design of WikipediaViz was inspired by two participatory design sessions with expert Wikipedia writers and sociologists who explained the clues they used to quickly assess the trustworthiness of articles. According to these results, we propose five metrics for Maturity and Quality assessment ofWikipedia articles and their accompanying visualizations to provide the readers with important clues about the editing process at a glance. We also report and discuss about the results of the user studies we conducted. Two preliminary pilot studies show that all our subjects trust Wikipedia articles almost blindly. With the third study, we show that WikipediaViz significantly reduces the time required to assess the quality of articles while maintaining a good accuracy.},
   author = {Fanny Chevalier and Stéphane Huot and Jean Daniel Fekete},
   doi = {10.1109/PACIFICVIS.2010.5429611},
   isbn = {9781424466849},
   journal = {IEEE Pacific Visualization Symposium 2010, PacificVis 2010 - Proceedings},
   keywords = {Collaborative knowledge,Encyclopedia,Information visualization,Participatory design,Wikipedia},
   pages = {49-56},
   title = {WikipediaViz: Conveying article quality for casual wikipedia readers},
   url = {https://www.researchgate.net/publication/221536224_WikipediaViz_Conveying_Article_Quality_for_Casual_Wikipedia_Readers},
   year = {2010},
}
@article{Saengthongpattana2014,
   abstract = {The quality evaluation of Thai Wikipedia articles relies on user consideration. There are increasing numbers of articles every day therefore the automatic evaluation method is needed for user. Components of Wikipedia articles such as headers, pictures, references, and links are useful to indicate the quality of articles. However readers need complete content to cover all of concepts in that article. The concept features are investigated in this work. The aim of this research is to classify Thai Wikipedia articles into two classes namely high-quality and low-quality class. Three article domains (Biography, Animal, and Place) are testes with decision tree and Naïve Bayes. We found that Naïve Bayes gets high TP Rate compared to decision tree in every domain. Moreover, we found that the concept feature plays an important role in quality classification of Thai Wikipedia articles. © Springer International Publishing Switzerland 2014.},
   author = {Kanchana Saengthongpattana and Nuanwan Soonthornphisaj},
   doi = {10.1007/978-3-319-05951-8_49},
   isbn = {9783319059501},
   issn = {21945357},
   issue = {VOLUME 1},
   journal = {Advances in Intelligent Systems and Computing},
   keywords = {Concept feature,Decision tree,Naïve Bayes,Quality of Thai Wikipedia articles,Statistical feature},
   pages = {513-523},
   publisher = {Springer Verlag},
   title = {Assessing the quality of Thai Wikipedia articles using concept and statistical features},
   volume = {275 AISC},
   url = {https://www.researchgate.net/publication/290603893_Assessing_the_Quality_of_Thai_Wikipedia_Articles_Using_Concept_and_Statistical_Features},
   year = {2014},
}
@article{Su2016,
   abstract = {The great popularity of Wikipedia makes it one of the dominant knowledge source around the World. However, since one of the core principles of Wikipedia is being open for anyone to maintain it, Wikipedia cannot fully ensure the reliability of its articles, and thus sometimes suffered criticism for containing low-quality information. It is therefore essential to assess the quality of Wikipedia articles automatically. In this paper we describe how we approach that problem by using a psycho-lexical resource, i.e., the Language Inquiry and Word Count (LIWC) dictionary. By training a classifier on different LIWC categories, we discuss the implications of each category for Wikipedia quality assessment.},
   author = {Qi Su and Pengyuan Liu},
   doi = {10.1109/WI-IAT.2015.23},
   isbn = {9781467396172},
   journal = {Proceedings - 2015 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, WI-IAT 2015},
   keywords = {Information Quality,LIWC,User-generated Content,Wikipedia},
   month = {2},
   pages = {184-187},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A psycho-lexical approach to the assessment of information quality on wikipedia},
   url = {https://www.researchgate.net/publication/221023929_On_Measuring_the_Quality_of_Wikipedia_Articles},
   year = {2016},
}
@article{,
   abstract = {Collaboratively edited articles such as in Wikipedia suffer from well-identified problems regarding their quality, e.g., information accuracy, reputability of third-party sources, vandalism. Due to the huge number of articles and the intensive edit rate, the manual evaluation of article content quality is inconceivable. In this paper, we tackle the problem of automatically establishing the quality of Wikipedia articles. Evidences are shown to consider the interactions between authors and articles to assess the quality score. Collaborations between authors and reviewers are also considered to reinforce the discriminative process. This work gives a generic formulation of the Mutual Reinforcement principle held between articles quality and authors authority and take explicitly advantage of the co-edits graph generated by individuals. Experiments conducted on a set of representative data from Wikipedia show the effectiveness of our approach.},
   author = {Baptiste De La Robertie and Yoann Pitarch and Olivier Teste},
   doi = {10.1145/2808797.2808895},
   isbn = {9781450338547},
   journal = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2015},
   month = {8},
   pages = {464-471},
   publisher = {Association for Computing Machinery, Inc},
   title = {Measuring article quality in Wikipedia using the collaboration network},
   url = {https://dl.acm.org/doi/10.1145/2808797.2808895},
   year = {2015},
}
@article{Azer2015,
   abstract = {Objective: To evaluate accuracy of content and readability level of English Wikipedia articles on cardiovascular diseases, using quality and readability tools. Methods: Wikipedia was searched on the 6 October 2013 for articles on cardiovascular diseases. Using a modified DISCERN (DISCERN is an instrument widely used in aßeßing online resources), articles were independently scored by three aßeßors. The readability was calculated using Flesch-Kincaid Grade Level. The inter-rater agreement between evaluators was calculated using the Fleiß κ scale. Results: This study was based on 47 English Wikipedia entries on cardiovascular diseases. The DISCERN scores had a median=33 (IQR=6). Four articles (8.5%) were of good quality (DISCERN score 40-50), 39 (83%) moderate (DISCERN 30-39) and 4 (8.5%) were poor (DISCERN 10-29). Although the entries covered the aetiology and the clinical picture, there were deficiencies in the pathophysiology of diseases, signs and symptoms, diagnostic approaches and treatment. The number of references varied from 1 to 127 references; 25.9±29.4 (mean±SD). Several problems were identified in the list of references and citations made in the articles. The readability of articles was 14.3±1.7 (mean±SD); consistent with the readability level for college students. In comparison, Harrison's Principles of Internal Medicine 18th edition had more tables, leß references and no significant difference in number of graphs, images, illustrations or readability level. The overall agreement between the evaluators was good (Fleiß κ 0.718 (95% CI 0.57 to 0.83). Conclusions: The Wikipedia entries are not aimed at a medical audience and should not be used as a substitute to recommended medical resources. Course designers and students should be aware that Wikipedia entries on cardiovascular diseases lack accuracy, predominantly due to errors of omißion. Further improvement of the Wikipedia content of cardiovascular entries would be needed before they could be considered a supplementary resource.},
   author = {Samy A. Azer and Nourah M. AlSwaidan and Lama A. Alshwairikh and Jumana M. AlShammari},
   doi = {10.1136/BMJOPEN-2015-008187},
   issn = {20446055},
   issue = {10},
   journal = {BMJ Open},
   pmid = {26443650},
   publisher = {BMJ Publishing Group},
   title = {Accuracy and readability of cardiovascular entries on Wikipedia: Are they reliable learning resources for medical students?},
   volume = {5},
   url = {https://www.researchgate.net/publication/282904833_Accuracy_and_readability_of_cardiovascular_entries_on_Wikipedia_Are_they_reliable_learning_resources_for_medical_students},
   year = {2015},
}
@article{Dang2017,
   abstract = {Wikipedia is a great example of large scale collaboration, where people from all over the world together build the largest and maybe the most important human knowledge repository in the history. However, a number of studies showed that the quality of Wikipedia articles is not equally distributed. While many articles are of good quality, many others need to be improved. Assessing the quality of Wikipedia articles is very important for guiding readers towards articles of high quality and suggesting authors and reviewers which articles need to be improved. Due to the huge size of Wikipedia, an effective automatic assessment method to measure Wikipedia articles quality is needed. In this paper, we present an automatic assessment method of Wikipedia articles quality by analyzing their content in terms of their format features and readability scores. Our results show improvements both in terms of accuracy and information gain compared with other existing approaches.},
   author = {Quang Vinh Dang and Claudia Lavinia Ignat},
   doi = {10.1109/CIC.2016.42},
   isbn = {9781509046072},
   journal = {Proceedings - 2016 IEEE 2nd International Conference on Collaboration and Internet Computing, IEEE CIC 2016},
   month = {1},
   pages = {266-275},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Measuring quality of collaboratively edited documents: The case of Wikipedia},
   url = {https://www.semanticscholar.org/paper/Measuring-Quality-of-Collaboratively-Edited-The-of-Dang-Ignat/31f226174f2da4ceca288b93769371c388d6c4fb},
   year = {2017},
}
@article{Lewoniewski2017,
   abstract = {Despite the fact that Wikipedia is often criticized for its poor quality, it continues to be one of the most popular knowledge bases in the world. Articles in this free encyclopedia on various topics can be created and edited in about 300 different language versions independently. Our research has showed that in language sensitive topics, the quality of information can be relatively better in the relevant language versions. However, in most cases, it is difficult for the Wikipedia readers to determine the language affiliation of the described subject. Additionally, each language edition of Wikipedia can have own rules in the manual assessing of the content’s quality. There are also differences in grading schemes between language versions: some use a 6–8 grade system to assess articles, and some are limited to 2–3. This makes automatic quality comparison of articles between various languages a challenging task, particularly if we take into account a large number of unassessed articles; some of the Wikipedia language editions have over 99% of articles without a quality grade. The paper presents the results of a relative quality and popularity assessment of over 28 million articles in 44 selected language versions. Comparative analysis of the quality and the popularity of articles in popular topics was also conducted. Additionally, the correlation between quality and popularity of Wikipedia articles of selected topics in various languages was investigated. The proposed method allows us to find articles with information of better quality that can be used to automatically enrich other language editions of Wikipedia.},
   author = {Włodzimierz Lewoniewski and Krzysztof Węcel and Witold Abramowicz},
   doi = {10.3390/INFORMATICS4040043},
   issn = {22279709},
   issue = {4},
   journal = {Informatics},
   keywords = {DBpedia,Information quality,WikiRank,Wikipedia},
   month = {12},
   publisher = {MDPI AG},
   title = {Relative quality and popularity evaluation of multilingual wikipedia articles},
   volume = {4},
   url = {https://www.researchgate.net/publication/345698101_Relative_Quality_and_Popularity_Evaluation_of_Multilingual_Wikipedia_Articles},
   year = {2017},
}
@article{Modiri2018,
   abstract = {Objectives: Wikipedia is the largest online encyclopedia with over 40 million articles, and generating 500 million visits per month. The aim of this study is to assess the readability and quality of Wikipedia pages on neurosurgical related topics. Patients and Methods: We selected the neurosurgical related Wikipedia pages based on the series of online patient information articles that are published by the American Association of Neurological Surgeons (AANS). We assessed readability of Wikipedia pages using five different readability scales (Flesch Reading Ease, Flesch Kincaid Grade Level, Gunning Fog Index, SMOG) Grade level, and Coleman-Liau Index). We used the Center for Disease Control (CDC) Clear Communication Index as well as the DISCERN Instrument to evaluate the quality of each Wikipedia article. Results: We identified a total of fifty-five Wikipedia articles that corresponded with patient information articles published by the AANS. This constitutes 77.46% of the AANS topics. The mean Flesch Kincaid reading ease score for all of the Wikipedia articles we analyzed is 31.10, which indicates that a college-level education is necessary to understand them. In comparison to the readability analysis for the AANS articles, the Wikipedia articles were more difficult to read across every scale. None of the Wikipedia articles meet the CDC criterion for clear communications. Conclusion: Our analyses demonstrated that Wikipedia articles related to neurosurgical topics are associated with higher grade levels for reading and also below the expected levels of clear communications for patients. Collaborative efforts from the neurosurgical community are needed to enhance the readability and quality of Wikipedia pages related to neurosurgery.},
   author = {Omeed Modiri and Daipayan Guha and Naif M. Alotaibi and George M. Ibrahim and Nir Lipsman and Aria Fallah},
   doi = {10.1016/J.CLINEURO.2018.01.021},
   issn = {18726968},
   journal = {Clinical Neurology and Neurosurgery},
   keywords = {Online,Patient education,Web 2.0,Wikipedia},
   month = {3},
   pages = {66-70},
   pmid = {29408776},
   publisher = {Elsevier B.V.},
   title = {Readability and quality of wikipedia pages on neurosurgical topics},
   volume = {166},
   url = {https://www.researchgate.net/publication/322847031_Readability_and_Quality_of_Wikipedia_Pages_on_Neurosurgical_Topics},
   year = {2018},
}
@article{Bassani2019,
   abstract = {With the development of Web 2.0 technologies, people have gone from being mere content users to content generators. In this context, the evaluation of the quality of (potential) information available online has become a crucial issue. Nowadays, one of the biggest online resources that users rely on as a knowledge base is Wikipedia. The collaborative aspect at the basis of Wikipedia can let to the possible creation of low-quality articles or even misinformation if the process of monitoring the generation and the revision of articles is not performed in a precise and timely way. For this reason, in this paper, the problem of automatically evaluating the quality of Wikipedia contents is considered, by proposing a supervised approach based on Machine Learning to perform the classification of articles on qualitative bases. With respect to prior literature, a wider set of features connected to Wikipedia articles has been taken into account, as well as previously unconsidered aspects connected to the generation of a labeled dataset to train the model, and the use of Gradient Boosting, which produced encouraging results.},
   author = {Elias Bassani and Marco Viviani},
   doi = {10.1145/3297280.3297357},
   isbn = {9781450359337},
   journal = {Proceedings of the ACM Symposium on Applied Computing},
   keywords = {Information Quality,Machine Learning,Social Media,Wikipedia},
   pages = {804-807},
   publisher = {Association for Computing Machinery},
   title = {Automatically assessing the quality of Wikipedia contents},
   volume = {Part F147772},
   year = {2019},
}
@article{Wang2021,
   abstract = {Wikipedia is becoming increasingly critical in helping people obtain information and knowledge. Its leading advantage is that users can not only access information but also modify it. However, this presents a challenging issue: how can we measure the quality of a Wikipedia article? The existing approaches assess Wikipedia quality by statistical models or traditional machine learning algorithms. However, their performance is not satisfactory. Moreover, most existing models fail to extract complete information from articles, which degrades the model’s performance. In this article, we first survey related works and summarise a comprehensive feature framework. Then, state-of-the-art deep learning models are introduced and applied to assess Wikipedia quality. Finally, a comparison among deep learning models and traditional machine learning models is conducted to validate the effectiveness of the proposed model. The models are compared extensively in terms of their training and classification performance. Moreover, the importance of each feature and the importance of different feature sets are analysed separately.},
   author = {Ping Wang and Xiaodan Li and Renli Wu},
   doi = {10.1177/0165551519877646},
   issn = {17416485},
   issue = {2},
   journal = {Journal of Information Science},
   keywords = {Deep learning,Wikipedia,feature framework,information quality assessment},
   month = {4},
   pages = {176-191},
   publisher = {SAGE Publications Ltd},
   title = {A deep learning-based quality assessment model of collaboratively edited documents: A case study of Wikipedia},
   volume = {47},
   url = {https://www.researchgate.net/publication/336145185_A_deep_learning-based_quality_assessment_model_of_collaboratively_edited_documents_A_case_study_of_Wikipedia},
   year = {2021},
}
@article{Velichety2019,
   abstract = {This research provides a method for quality assessment of peer-produced content in knowledge repositories using a complementary view of collaboration. Using the definition of collaboration as the action of working with someone to produce something, we identify the aspects of collaboration that the present research on online communities does not consider. To this end, we introduce and define the concept of implicit collaboration and then identify two dimensions and four possible areas of collaboration. In each area, we identify the relevant social network that captures collaboration. Using customized measures on each of the networks that capture various aspects of collaboration, we quantify the utility of implicit collaboration in assessing article quality. Experiments conducted on the complete population of graded English language Wikipedia articles show that all the identified measures improve the predictive accuracy of the existing models by 11.89 percent while improving the class-wise precision by 9-18 percent and the class-wise recall by 5-26 percent. We also find that our method complements the existing quality assessment approaches well. Our research has implications for developing automated quality assessment methods for peer-produced content using big data and social networks.},
   author = {Srikar Velichety},
   doi = {10.1145/3371041.3371045},
   issn = {00950033},
   issue = {4},
   journal = {Data Base for Advances in Information Systems},
   keywords = {Discussions,Edits,Implicit Collaboration,Social Networks,Wikipedia},
   month = {11},
   pages = {28-51},
   publisher = {Association for Computing Machinery},
   title = {Quality assessment of peer-produced content in knowledge repositories using big data and social networks: The case of implicit collaboration in wikipedia},
   volume = {50},
   year = {2019},
}
@article{Wang2020,
   abstract = {Currently, web document repositories have been collaboratively created and edited. One of these repositories, Wikipedia, is facing an important problem: assessing the quality of Wikipedia. Existing approaches exploit techniques such as statistical models or machine leaning algorithms to assess Wikipedia article quality. However, existing models do not provide satisfactory results. Furthermore, these models fail to adopt a comprehensive feature framework. In this article, we conduct an extensive survey of previous studies and summarize a comprehensive feature framework, including text statistics, writing style, readability, article structure, network, and editing history. Selected state-of-the-art deep-learning models, including the convolutional neural network (CNN), deep neural network (DNN), long short-term memory (LSTMs) network, CNN-LSTMs, bidirectional LSTMs, and stacked LSTMs, are applied to assess the quality of Wikipedia. A detailed comparison of deep-learning models is conducted with regard to different aspects: classification performance and training performance. We include an importance analysis of different features and feature sets to determine which features or feature sets are most effective in distinguishing Wikipedia article quality. This extensive experiment validates the effectiveness of the proposed model.},
   author = {Ping Wang and Xiaodan Li},
   doi = {10.1002/ASI.24210},
   issn = {23301643},
   issue = {1},
   journal = {Journal of the Association for Information Science and Technology},
   month = {1},
   pages = {16-28},
   publisher = {John Wiley and Sons Inc.},
   title = {Assessing the quality of information on wikipedia: A deep-learning approach},
   volume = {71},
   url = {https://www.researchgate.net/publication/332294515_Assessing_the_quality_of_information_on_wikipedia_A_deep-learning_approach},
   year = {2020},
}
@article{Couto2021,
   abstract = {Wikipedia is an online, free, multi-language, and collaborative encyclopedia, currently one of the most significant information sources on the web the open nature of Wikipedia contributions raises concerns about the quality of its information. Previous studies have addressed this issue using manual evaluations and proposing generic measures for quality assessment. In this work, we focus on the quality of health-related content. For this purpose, we use general and health-specific features from Wikipedia articles to propose health-specific metrics. We evaluate these metrics using a set of Wikipedia articles previously assessed by WikiProject Medicine. We conclude that it is possible to combine generic and specific metrics to determine health-related content's information quality these metrics are computed automatically and can be used by curators to identify quality issues. Along with the explored features, these metrics can also be used in approaches that automatically classify the quality of Wikipedia health-related articles.},
   author = {Luís Couto and Carla Teixeira Lopes},
   doi = {10.1145/3442442.3452355},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Health-related Content,Information Quality,Wikipedia},
   month = {4},
   pages = {640-647},
   publisher = {Association for Computing Machinery, Inc},
   title = {Assessing the quality of health-related Wikipedia articles with generic and specific metrics},
   url = {https://doi.org/10.1145/3442442.3452355},
   year = {2021},
}
@article{Teblunthuis2021,
   abstract = {Organizing complex peer production projects and advancing scientific knowledge of open collaboration each depend on the ability to measure quality. Wikipedia community members and academic researchers have used article quality ratings for purposes like tracking knowledge gaps and studying how political polarization shapes collaboration. Even so, measuring quality presents many methodological challenges. The most widely used systems use quality assesements on discrete ordinal scales, but such labels can be inconvenient for statistics and machine learning. Prior work handles this by assuming that different levels of quality are "evenly spaced"from one another. This assumption runs counter to intuitions about degrees of effort needed to raise Wikipedia articles to different quality levels. I describe a technique extending the Wikimedia Foundations' ORES article quality model to address these limitations. My method uses weighted ordinal regression models to construct one-dimensional continuous measures of quality. While scores from my technique and from prior approaches are correlated, my approach improves accuracy for research datasets and provides evidence that the "evenly spaced"assumption is unfounded in practice on English Wikipedia. I conclude with recommendations for using quality scores in future research and include the full code, data, and models.},
   author = {Nathan Teblunthuis},
   doi = {10.1145/3479986.3479991},
   isbn = {9781450385008},
   keywords = {Wikipedia,datasets,machine learning,measurement,methods,online communities,peer production,quality,sociotechnical systems,statistics},
   month = {9},
   pages = {1-10},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Measuring Wikipedia Article Quality in One Dimension by Extending ORES with Ordinal Regression},
   url = {https://doi.org/10.1145/3479986.3479991},
   year = {2021},
}
@article{Hu2016,
   abstract = {This study attempts to investigate to what extent indicators of academic writing and cognitive thinking can help measure the writing quality of group collaborative writings on Wikis. Particularly, comparisons were made on Wiki content in different stages of the projects. Preliminary results from a multiple linear regression analysis reveal that linguistic indicators such as engagement markers and self-mention were significant predictors in earlier stages to the projects, whereas verbs indicating cognitive thinking in the evaluation level were significant in later project stages.},
   author = {Xiao Hu and Tzi Dong Jeremy Ng and Lu Tian and Chi Un Lei},
   doi = {10.1145/2883851.2883963},
   isbn = {9781450341905},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Automated assessment,Metadiscourse,Wiki},
   month = {4},
   pages = {518-519},
   publisher = {Association for Computing Machinery},
   title = {Automating assessment of collaborative writing quality in multiple stages: The case of wiki},
   volume = {25-29-April-2016},
   url = {http://dx.doi.org/10.1145/2883851.2883963},
   year = {2016},
}
@article{Dang2016,
   abstract = {As Wikipedia became the largest human knowledge repository, quality measurement of its articles received a lot of attention during the last decade. Most research efforts focused on classification of Wikipedia articles quality by using a different feature set. However, so far, no 'golden feature set' was proposed. In this paper, we present a novel approach for classifying Wikipedia articles by analysing their content rather than by considering a feature set. Our approach uses recent techniques in natural language processing and deep learning, and achieved a comparable result with the state-of-the-art.},
   author = {Quang Vinh Dang and Claudia Lavinia Ignat},
   doi = {10.1145/2910896.2910917},
   isbn = {9781450342292},
   issn = {15525996},
   journal = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries},
   keywords = {Wikipedia,deep learning,document representation,feature engineering,quality assessment},
   month = {9},
   pages = {27-30},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Quality assessment of Wikipedia articles without feature engineering},
   volume = {2016-September},
   url = {https://dl.acm.org/doi/10.1145/2910896.2910917},
   year = {2016},
}
@article{Gelman2008,
   abstract = {This short paper outlines a research study in progress, which is motivated by the perception that the spelling error rate of a document can serve as a rudimentary proxy for the degree of quality control exercised in its creation, and, subsequently, indicate its quality. One objective of this research is to validate this understanding. Ultimately, the goal of this research is to take advantage of such an association. In particular, we propose a simple, "quick and dirty" metric for assisting in the evaluation of the quality of websites. This metric utilizes the reported hit counts of search engine queries on a pre-determined set of commonly misspelled words. Copyright 2008 ACM.},
   author = {Irit Askira Gelman and Anthony L. Barletta},
   doi = {10.1145/1458527.1458538},
   isbn = {9781605582597},
   journal = {International Conference on Information and Knowledge Management, Proceedings},
   keywords = {Data quality,Data quality indicator,Indicator,Information credibility,Information quality,Quick and dirty,Web Data,Web data},
   pages = {43-46},
   title = {A "quick and dirty" website data quality indicator},
   url = {https://dl.acm.org/doi/10.1145/1458527.1458538},
   year = {2008},
}
@article{Lex2012,
   abstract = {Nowadays, many decisions are based on information found in the Web. For the most part, the disseminating sources are not certified, and hence an assessment of the quality and credibility of Web content became more important than ever. With factual density we present a simple statistical quality measure that is based on facts extracted from Web content using Open Information Extraction. In a first case study, we use this measure to identify featured/good articles in Wikipedia. We compare the factual density measure with word count, a measure that has successfully been applied to this task in the past. Our evaluation corroborates the good performance of word count in Wikipedia since featured/good articles are often longer than non-featured. However, for articles of similar lengths the word count measure fails while factual density can separate between them with an F-measure of 90.4%. We also investigate the use of relational features for categorizing Wikipedia articles into featured/good versus non-featured ones. If articles have similar lengths, we achieve an F-measure of 86.7% and 84% otherwise. © 2012 ACM.},
   author = {Elisabeth Lex and Michael Voelske and Marcelo Errecalde and Edgardo Ferretti and Leticia Cagnina and Christopher Horn and Benno Stein and Michael Granitzer},
   doi = {10.1145/2184305.2184308},
   isbn = {9781450312370},
   journal = {ACM International Conference Proceeding Series},
   pages = {7-10},
   title = {Measuring the quality of web content using factual information},
   url = {https://dl.acm.org/doi/10.1145/2184305.2184308},
   year = {2012},
}
@article{Rafalak2016,
   abstract = {Machine learning algorithms and recommender systems trained on human ratings are widely in use today. However, human ratings may be associated with a high level of uncertainty and are subjective, influenced by demographic or psychological factors. We propose a new approach to the design of object classes from human ratings: the use of entire distributions to construct classes. By avoiding aggregation for class definition, our approach loses no information and can deal with highly volatile or conflicting ratings. The approach is based the concept of the Earth Mover's Distance (EMD), a measure of distance for distributions. We evaluate the proposed approach based on four datasets obtained from diverse Web content or movie quality evaluation services or experiments. We show that clusters discovered in these datasets using the EMD measure are characterized by a consistent and simple interpretation. Quality classes defined using entire rating distributions can be fitted to clusters of distributions in the four datasets using two parameters, resulting in a good overall fit. We also consider the impact of the composition of small samples on the distributions that are the basis of our classification approach. We show that using distributions based on small samples of 10 evaluations is still robust to several demographic and psychological variables. This observation suggests that the proposed approach can be used in practice for quality evaluation, even for highly uncertain and subjective ratings.},
   author = {Maria Rafalak and Dominik Deja and Adam Wierzbicki and Radosław Nielek and Michał Kakol},
   doi = {10.1145/2994132},
   issn = {1559114X},
   issue = {4},
   journal = {ACM Transactions on the Web},
   keywords = {Classification design,Earth mover's distance,Rating distribution,Robustness,Sample composition,Web content quality},
   month = {11},
   publisher = {Association for Computing Machinery},
   title = {Web content classification using distributions of subjective quality evaluations},
   volume = {10},
   url = {http://dx.doi.org/10.1145/2994132},
   year = {2016},
}
@article{,
   abstract = {In this paper we propose a measure for estimating the lexical quality of the Web, that is, the representational aspect of the textual web content. Our lexical quality measure is based in a small corpus of spelling errors and we apply it to English and Spanish. We first compute the correlation of our measure with web popularity measures to show that gives independent information and then we apply it to different web segments, including social media. Our results shed a light on the lexical quality of the Web and show that authoritative websites have several orders of magnitude less misspellings than the overall Web. We also present an analysis of the geographical distribution of lexical quality throughout English and Spanish speaking countries as well as how this measure changes in about one year. © 2012 ACM.},
   author = {Ricardo Baeza-Yates and Luz Rello},
   doi = {10.1145/2184305.2184307},
   isbn = {9781450312370},
   journal = {ACM International Conference Proceeding Series},
   keywords = {English and Spanish domains,Geographical distribution,Lexical quality,Social media,Web quality},
   pages = {1-6},
   title = {On measuring the lexical quality of the web},
   url = {https://dl.acm.org/doi/10.1145/2184305.2184307},
   year = {2012},
}
@article{,
   abstract = {Digital libraries and services enable users to access large amounts of data on demand. Yet, quality assessment of information encountered on the Internet remains an elusive open issue. For example, Wikipedia, one of the most visited platforms on the Web, hosts thousands of user-generated articles and undergoes 12 million edits/contributions per month. User-generated content is undoubtedly one of the keys to its success but also a hindrance to good quality. Although Wikipedia has established guidelines for the “perfect article,” authors find it difficult to assert whether their contributions comply with them and reviewers cannot cope with the ever-growing amount of articles pending review. Great efforts have been invested in algorithmic methods for automatic classification of Wikipedia articles (as featured or non-featured) and for quality flaw detection. Instead, our contribution is an interactive tool that combines automatic classification methods and human interaction in a toolkit, whereby experts can experiment with new quality metrics and share them with authors that need to identify weaknesses to improve a particular article. A design study shows that experts are able to effectively create complex quality metrics in a visual analytics environment. In turn, a user study evidences that regular users can identify flaws, as well as high-quality content based on the inspection of automatic quality scores.},
   author = {Cecilia Di Sciascio and David Strohmaier and Marcelo Errecalde and Eduardo Veas},
   doi = {10.1145/3150973},
   issn = {21606463},
   issue = {2-3},
   journal = {ACM Transactions on Interactive Intelligent Systems},
   keywords = {Information quality assessment,Text analytics,User-generated content,Visual analytics,Wikipedia},
   month = {4},
   publisher = {Association for Computing Machinery},
   title = {Interactive quality analytics of user-generated content: An integrated toolkit for the case of Wikipedia},
   volume = {9},
   url = {https://dl.acm.org/doi/10.1145/3150973},
   year = {2019},
}
@article{Biancani2014,
   abstract = {Wikipedia is unique among reference works both in its scale and in the openness of its editing interface. The question of how it can achieve and maintain high-quality encyclopedic articles is an area of active research. In order to address this question, researchers need to build consensus around a sensible metric to assess the quality of contributions to articles. This measure must not only reflect an intuitive concept of "quality, " but must also be scalable and run efficiently. Building on prior work in this area, this paper uses human raters through Amazon Mechanical Turk to validate an efficient, automated quality metric.},
   author = {Susan Biancani},
   doi = {10.1145/2641580.2641621},
   isbn = {9781450330169},
   journal = {Proceedings of the 10th International Symposium on Open Collaboration, OpenSym 2014},
   keywords = {Collaboration,Experience,Ownership,Peer,Peer Review,Quality,WikiWork,Wikipedia,web-based interaction H12 [Models and Principles]: User/Machine Systems General Terms Measurement},
   pages = {G4},
   publisher = {Association for Computing Machinery, Inc},
   title = {Measuring the quality of edits to Wikipedia},
   url = {http://dx.doi.org/10.1145/2641580.2641621},
   year = {2014},
}
@article{Hu2007,
   abstract = {Wikipedia has grown to be the world largest and busiest free encyclopedia, in which articles are collaboratively written and maintained by volunteers online. Despite its success as a means of knowledge sharing and collaboration, the public has never stopped criticizing the quality of Wikipedia articles edited by non-experts and inexperienced contributors. In this paper, we investigate the problem of assessing the quality of articles in collaborative authoring of Wikipedia. We propose three article quality measurement models that make use of the interaction data between articles and their contributors derived from the article edit history. Our basic model is designed based on the mutual dependency between article quality and their author authority. The PeerReview model introduces the review behavior into measuring article quality. Finally, our ProbReview models extend PeerReview with partial reviewership of contributors as they edit various portions of the articles. We conduct experiments on a set of well-labeled Wikipedia articles to evaluate the effectiveness of our quality measurement models in resembling human judgement. Copyright 2007 ACM.},
   author = {Meiqun Hu and Ee Peng Lim and Aixin Sun and Hady W. Lauw and Ba Quy Vuong},
   doi = {10.1145/1321440.1321476},
   isbn = {9781595938039},
   journal = {International Conference on Information and Knowledge Management, Proceedings},
   keywords = {Article quality,Authority,Collaborative authoring,Peer review,Wikipedia,quality assessment,transient contribution,web-based interaction; K43 [Computers and Society]: Organizational Impacts-Computer-supported collaborative work General Terms Measurement},
   pages = {243-252},
   title = {Measuring article quality in wikipedia: Models and evaluation},
   url = {https://dl.acm.org/doi/10.1145/1321440.1321476},
   year = {2007},
}
@article{Nikolenko2016,
   abstract = {Automated evaluation of topic quality remains an important unsolved problem in topic modeling and represents a major obstacle for development and evaluation of new topic models. Previous attempts at the problem have been formulated as variations on the coherence and/or mutual information of top words in a topic. In this work, we propose several new metrics for evaluating topic quality with the help of distributed word representations; our experiments suggest that the new metrics are a better match for human judgement, which is the gold standard in this case, than previously developed approaches.},
   author = {Sergey I. Nikolenko},
   doi = {10.1145/2911451.2914720},
   isbn = {9781450342902},
   journal = {SIGIR 2016 - Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {Text mining,Topic modeling,Topic quality},
   month = {7},
   pages = {1029-1032},
   publisher = {Association for Computing Machinery, Inc},
   title = {Topic quality metrics based on distributed word representations},
   url = {http://dx.doi.org/10.1145/2911451.2914720},
   year = {2016},
}
@article{Antunes2020,
   abstract = {Looking for health information is one of the most popular activities online. However, the specificity of language on this domain is frequently an obstacle to comprehension, especially for the ones with lower levels of health literacy. For this reason, search engines should consider the readability of health content and, if possible, adapt it to the user behind the search. In this work, we explore methods to assess the readability of health content automatically. We propose features capable of measuring the specificity of a medical text and estimate the knowledge necessary to comprehend it. The features are based on information retrieval metrics and the log-likelihood of a text with lay and medico-scientific language models. To evaluate our methods, we built and used a dataset composed of health articles of Simple English Wikipedia and the respective documents in ordinary Wikipedia. We achieved a maximum accuracy of 88% in binary classifications (easy versus hard-to-read). We found out that the machine learning algorithm does not significantly interfere with performance. We also experimented and compared different features combinations. The features using the values of the log-likelihood of a text with lay and medico-scientific language models perform better than all the others.},
   author = {Hélder Antunes and Carla Teixeira Lopes},
   doi = {10.1145/3397271.3401187},
   isbn = {9781450380164},
   journal = {SIGIR 2020 - Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {consumer health search,machine learning,natural language processing,readability},
   month = {7},
   pages = {1973-1976},
   publisher = {Association for Computing Machinery, Inc},
   title = {Proposal and Comparison of Health Specific Features for the Automatic Assessment of Readability},
   url = {https://doi.org/10.1145/3397271.3401187},
   year = {2020},
}
@article{Banerjee2011,
   abstract = {This paper proposes a novel bio-inspired model that quantifies the quality aspect of Wiki content. Unlike the statistical measures, the proposed system automates the quality dispersion mechanism using ant colony's pheromone artifacts over Wiki content. The inclusion of artificial ant agents is relevant to the heuristic behavior of Wiki content management and reputation paradigm of Wiki. The proposed model generates substantial empirical and graphical evidences, which could be timely boosted to enhance Wiki culture and editing process of Wiki in order to be theoretically trusted and validated across the users. Observed results present evidences that despite of users' attribution, registered or anonymous, the proposed agent based model provides quantifying editing in content of Wiki and accordingly both commercial viability, in terms of quality, and vandalism can be ensured. Copyright © 2011 ACM.},
   author = {Soumya Banerjee and Nashwa El-Bendary and Hameed Al-Qaheri},
   doi = {10.1145/2077489.2077545},
   isbn = {9781450310475},
   journal = {Proceedings of the International Conference on Management of Emergent Digital EcoSystems, MEDES'11},
   keywords = {Ant colony optimization,Bio-inspired,Content management,Quality,Quality measurement,Reputation paradigm,Wiki,WikiWork,Wikipedia,web-based interaction H12 [Models and Principles]: User/Machine Systems General Terms Measurement},
   pages = {305-312},
   title = {Exploring wiki: Measuring the quality of social media using ant colony metaphor},
   url = {http://dx.doi.org/10.1145/2641580.2641621},
   year = {2011},
}
@article{Pirolli2009,
   abstract = {An experiment was conducted to study how credibility judgments about Wikipedia are affected by providing users with an interactive visualization (WikiDashboard) of article and author editing history. Overall, users who self-reported higher use of Internet information and higher rates of Wikipedia usage tended to produce lower credibility judgments about Wikipedia articles and authors. However, use of WikiDashboard significantly increased article and author credibility judgments, with effect sizes larger than any other measured effects of background media usage and attitudes on Wikiepedia credibility. The results suggest that increased exposure to the editing/authoring histories of Wikipedia increases credibility judgments. Copyright 2009 ACM.},
   author = {Peter Pirolli and Evelin Wollny and Bongwon Suh},
   doi = {10.1145/1518701.1518929},
   isbn = {9781605582474},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {Credibility,Wikidashboard,Wikipedia,credibility},
   pages = {1505-1508},
   title = {So you know you're getting the best possible information: A tool that increases wikipedia credibility},
   url = {https://dl.acm.org/doi/10.1145/1518701.1518929},
   year = {2009},
}
@article{Keeton2010,
   abstract = {Information quality (IQ) is a measure of how fit information is for a purpose. Sometimes called Quality of Information (QoI) by analogy with Quality of Service (QoS), it quantifies whether the correct information is being used to make a decision or take an action. Not understanding when information is of adequate quality can lead to bad decisions and catastrophic effects, including system outages, increased costs, lost revenue - and worse. Quantifying information quality can help improve decision making, but the ultimate goal should be to select or construct information producers that have the appropriate balance between information quality and the cost of providing it. In this paper, we provide a brief introduction to the field, argue the case for applying information quality metrics in the systems domain, and propose a research agenda to explore this space.},
   author = {Kimberly Keeton and Pankaj Mehra and John Wilkes},
   doi = {10.1145/1710115.1710121},
   issn = {01635999},
   issue = {3},
   journal = {Performance Evaluation Review},
   keywords = {Data quality,Goal-directed design,IQ,Information processing pipeline,Information quality,Modeling,Prediction,QoI,Uncertainty,goal-directed design,information processing pipeline,modeling,prediction,uncertainty},
   pages = {26-31},
   title = {Do you know your IQ? A research agenda for information quality in systems},
   volume = {37},
   year = {2010},
}
@article{Viseur2014,
   abstract = {Wikipedia is a collaborative multilingual encyclopedia launched in 2001. We already conducted a first research on the extraction of biographical data about personalities from Belgium in order to build a large database with biographical data. However, the question of the reliability of the data arises. In particular, in the case of Wikipedia, the data are generated by users and could be subject to errors. In consequence, we wanted to answer to the following question: are the data introduced in Wikipedia articles reliable? Our research is organized in three sections. The first section provides a brief state of the art about the reliability of the user-generated data. A second section presents the methodology of our research. A third section will present the results. The error rates that were measured for the birthdate is low (0.75%), although it is higher than the 0.21% score that we observed for the baseline (reference sources). In a fourth section, the results are discussed.},
   author = {Robert Viseur},
   doi = {10.1145/2641580.2641618},
   isbn = {9781450330169},
   journal = {Proceedings of the 10th International Symposium on Open Collaboration, OpenSym 2014},
   keywords = {Biography,Data extraction,Data quality,Information retrieval,Open data,Reliability,Wikipedia,reliability,standards,systems issues,user issues General Terms Reliability Keywords wikipedia},
   pages = {G2},
   publisher = {Association for Computing Machinery, Inc},
   title = {Reliability of user-generated data: The case of biographical data in Wikipedia},
   url = {http://dx.doi.org/10.1145/2641580.2641618},
   year = {2014},
}
@article{Blumenstock2008,
   abstract = {Wikipedia, "the free encyclopedia", now contains over two million English articles, and is widely regarded as a high-quality, authoritative encyclopedia. Some Wikipedia articles, however, are of questionable quality, and it is not always apparent to the visitor which articles are good and which are bad. We propose a simple metric - word count - for measuring article quality. In spite of its striking simplicity, we show that this metric significantly outperforms the more complex methods described in related work.},
   author = {Joshua E. Blumenstock},
   doi = {10.1145/1367497.1367673},
   isbn = {9781605580852},
   journal = {Proceeding of the 17th International Conference on World Wide Web 2008, WWW'08},
   keywords = {Information quality,Wikipedia,Word count,word count},
   pages = {1095-1096},
   title = {Size matters: Word count as a measure of quality on Wikipedia},
   url = {https://www.researchgate.net/publication/221023915_Size_matters_Word_count_as_a_measure_of_quality_on_Wikipedia},
   year = {2008},
}
@article{,
   abstract = {Wikipedia, an international project that uses Wiki software to collaboratively create an encyclopaedia, is becoming more and more popular. Everyone can directly edit articles and every edit is recorded. The version history of all articles is freely available and allows a multitude of examinations. This paper gives an overview on Wikipedia research. Wikipedia's fundamental components, i.e. articles, authors, edits, and links, as well as content and quality are analysed. Possibilities of research are explored including examples and first results. Several characteristics that are found in Wikipedia, such as exponential growth and scale-free networks are already known in other context. However the Wiki architecture also possesses some intrinsic specialities. General trends are measured that are typical for all Wikipedias but vary between languages in detail.},
   author = {Jakob Voß},
   title = {Measuring wikipedia},
   url = {http://eprints.rclis.org/6207},
   year = {2005},
}
@article{Javanmardi2010,
   abstract = {Wikipedia is commonly viewed as the main online encyclopedia. Its content quality, however, has often been questioned due to the open nature of its editing model. A high-quality contribution by an expert may be followed by a low-quality contribution made by an amateur or a vandal; therefore the quality of each article may uctuate over time as it goes through iterations of edits by different users. With the increasing use of Wikipedia, the need for a reliable assessment of the quality of the content is also rising. In this study, we model the evolution of content quality in Wikipedia articles in order to estimate the fraction of time during which articles retain high-quality status. To evaluate the model, we assess the quality of Wikipedia's featured and non-featured articles. We show how the model reproduces consistent results with what is expected. As a case study, we use the model in a CalSWIM mashup the content of which is taken from both highly reliable sources and Wikipedia, which may be less so. Integrating CalSWIM with a trust management system enables it to use not only recency but also quality as its criteria, and thus filter out vandalized or poor-quality content. Copyright 2010 ACM.},
   author = {Sara Javanmardi and Cristina Lopes},
   doi = {10.1145/1964858.1964876},
   isbn = {9781450302173},
   journal = {SOMA 2010 - Proceedings of the 1st Workshop on Social Media Analytics},
   keywords = {Crowdsourcing,Web 2.0,Wiki,Wikipedia,web-based interaction General Terms Collaborative Authoring},
   pages = {132-138},
   title = {Statistical measure of quality in Wikipedia},
   url = {https://dl.acm.org/doi/10.1145/1964858.1964876},
   year = {2010},
}
@article{Wilkinson2007,
   abstract = {The rise of the Internet has enabled collaboration and cooperation on anunprecedentedly large scale. The online encyclopedia Wikipedia, which presently comprises 7.2 million articles created by 7.04 million distinct editors, provides a consummate example. We examined all 50 million edits made to the 1.5 million English-language Wikipedia articles and found that the high-quality articles are distinguished by a marked increase in number of edits, number of editors, and intensity of cooperative behavior, as compared to other articles of similar visibility and age. This is significant because in other domains, fruitful cooperation has proven to be difficult to sustain as the size of the collaboration increases. Furthermore, in spite of the vagaries of human behavior, we show that Wikipedia articles accrete edits according to a simple stochastic mechanism in which edits beget edits. Topics of high interest or relevance are thus naturally brought to the forefront of quality. Copyright © 2007 ACM.},
   author = {Dennis M. Wilkinson and Bernardo A. Huberman},
   doi = {10.1145/1296951.1296968},
   isbn = {9781595938619},
   journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
   keywords = {Cooperation,Web-based interaction; K43 [Comput-ers and Society]: Organizational Impacts-Computer-supported collaborative work General Terms Collaborative authoring,Wikipedia,groupware Keywords Cooperation},
   pages = {157-164},
   title = {Cooperation and quality in Wikipedia},
   url = {https://www.researchgate.net/publication/200772402_Cooperation_and_quality_in_Wikipedia},
   year = {2007},
}
@article{Liu2011,
   abstract = {The quality of Wikipedia articles is debatable. On the one hand, existing research indicates that not only are people willing to contribute articles but the quality of these articles is close to that found in conventional encyclopedias. On the other hand, the public has never stopped criticizing the quality of Wikipedia articles, and critics never have trouble finding low-quality Wikipedia articles. Why do Wikipedia articles vary widely in quality? We investigate the relationship between collaboration and Wikipedia article quality. We show that the quality of Wikipedia articles is not only dependent on the different types of contributors but also on how they collaborate. Based on an empirical study, we classify contributors based on their roles in editing individual Wikipedia articles. We identify various patterns of collaboration based on the provenance or, more specifically, who does what to Wikipedia articles. Our research helps identify collaboration patterns that are preferable or detrimental for article quality, thus providing insights for designing tools and mechanisms to improve the quality of Wikipedia articles.},
   author = {J Liu},
   doi = {10.1145/1985347.1985352},
   issue = {2},
   journal = {dl.acm.org},
   keywords = {Hm [Information Systems]: Miscellaneous General Terms: Design,Management Additional Key Words and Phrases: Wikipedia,article quality,collaboration pattern},
   month = {9},
   pages = {72},
   title = {Who does what: Collaboration patterns in the Wikipedia and their impact on article quality},
   volume = {2},
   url = {https://dl.acm.org/doi/abs/10.1145/1985347.1985352},
   year = {2011},
}
@article{Halfaker2009,
   abstract = {Wikipedia is a highly successful example of what mass collaboration in an informal peer review system can accomplish. In this paper, we examine the role that the quality of the contributions, the experience of the contributors and the ownership of the content play in the decisions over which contributions become part of Wikipedia and which ones are rejected by the community. We introduce and justify a versatile metric for automatically measuring the quality of a contribution. We find little evidence that experience helps contributors avoid rejection. In fact, as they gain experience , contributors are even more likely to have their work rejected. We also find strong evidence of ownership behaviors in practice despite the fact that ownership of content is discouraged within Wikipedia.},
   author = {A Halfaker and A Kittur and R Kraut and J Riedl - Proceedings of the 5th and undefined 2009},
   isbn = {9781605587301},
   journal = {dl.acm.org},
   keywords = {Experience,H12 [Models and Principles]: User/Machine Systems Keywords Wikipedia,Own-ership,Peer,Peer Review,Quality,WikiWork},
   title = {A jury of your peers: quality, experience and ownership in Wikipedia},
   url = {https://dl.acm.org/doi/abs/10.1145/1641309.1641332},
   year = {2009},
}
@article{Stein2007,
   abstract = {The considerable high quality of Wikipedia articles is often accredited to the large number of users who contribute to Wikipedia's encyclopedia articles, who watch articles and correct errors immediately. In this paper, we are in particular interested in a certain type of Wikipedia articles, namely, the featured articles - articles marked by a community's vote as being of outstanding quality. The German Wikipedia has the nice property that it has two types of featured articles: excellent and worth reading. We explore on the German Wikipedia whether only the mere number of contributors makes the difference or whether the high quality of featured articles results from having experienced authors contributing with a reputation for high quality contributions. Our results indicate that it does matter who contributes. Copyright 2007 ACM.},
   author = {Klaus Stein and Claudia Hess},
   doi = {10.1145/1286240.1286290},
   isbn = {1595938206},
   journal = {Hypertext 2007: Proceedings of the Eighteenth ACM Conference on Hypertext and Hypermedia, HT'07},
   keywords = {Collaborative working,Measures of quality and reputation,Statistical analysis of Wikipedia,Wiki,Wikipedia},
   pages = {171-174},
   title = {Does it matter who contributes - A study on featured articles in the german wikipedia},
   url = {https://dl.acm.org/doi/10.1145/1286240.1286290},
   year = {2007},
}
@article{,
   abstract = {In this paper we address the problem of developing actionable quality models for Wikipedia, models whose features directly suggest strategies for improving the quality of a given article. We rst survey the literature in order to understand the notion of article quality in the context of Wikipedia and existing approaches to automatically assess article quality. We then develop classication models with varying combinations of more or less actionable features, and nd that a model that only contains clearly actionable features delivers solid performance. Lastly we discuss the implications of these results in terms of how they can help improve the quality of articles across Wikipedia. Categories and Subject Descriptors H.5 [Information Interfaces and Presentation]: Group and Organization InterfacesCollaborative computing, Computer-supported cooperative work, Web-based interac- Tion. Copyright 2010 ACM.},
   author = {Morten Warncke-Wang and Dan Cosley and John Riedl},
   doi = {10.1145/2491055.2491063},
   isbn = {9781450318525},
   journal = {Proceedings of the 9th International Symposium on Open Collaboration, WikiSym + OpenSym 2013},
   keywords = {Classication,Flaw detection,Information quality,Machine learning,Modelling,Web-based interac-tion Keywords Wikipedia,Wikipedia},
   title = {Tell me more: An actionable quality model for wikipedia},
   url = {https://dl.acm.org/doi/10.1145/2491055.2491063},
   year = {2013},
}
@article{,
   abstract = {The main feature of the free online-encyclopedia Wikipedia is the wiki-tool, which allows viewers to edit the articles directly in the web browser. As a weakness of this openness for example the possibility of manipulation and vandalism cannot be ruled out, so that the quality of any given Wikipedia article is not guaranteed. Hence the automatic quality assessment has been becoming a high active research field. In this paper we offer new metrics for an efficient quality measurement. The metrics are based on the lifecycles of low and high quality articles, which refer to the changes of the persistent and transient contributions throughout the entire life span. Copyright © 2009 ACM.},
   author = {Thomas Wöhner and Ralf Peters},
   doi = {10.1145/1641309.1641333},
   isbn = {9781605587301},
   journal = {Proceedings of the 5th International Symposium on Wikis and Open Collaboration, WiKiSym 2009},
   keywords = {Persistent contribution,Quality assessment,Transient contribution,Wikipedia,Wikipedia lifecycle,quality assessment,transient contribution,web-based interaction; K43 [Computers and Society]: Organizational Impacts-Computer-supported collaborative work General Terms Measurement},
   title = {Assessing the quality of Wikipedia articles with lifecycle based metrics},
   url = {https://dl.acm.org/doi/10.1145/1641309.1641333},
   year = {2009},
}
@article{Adler2008,
   abstract = {We consider the problem of measuring user contributions to ver-sioned, collaborative bodies of information, such as wikis. Measuring the contributions of individual authors can be used to divide revenue, to recognize merit, to award status promotions, and to choose the order of authors when citing the content. In the context of the Wikipedia, previous works on author contribution estimation have focused on two criteria: the total text created, and the total number of edits performed. We show that neither of these criteria work well: both techniques are vulnerable to manipulation, and the total-text criterion fails to reward people who polish or re-arrange the content. We consider and compare various alternative criteria that take into account the quality of a contribution, in addition to the quantity, and we analyze how the criteria differ in the way they rank authors according to their contributions. As an outcome of this study, we propose to adopt total edit longevity as a measure of author contribution. Edit longevity is resistant to simple attacks, since edits are counted towards an author's contribution only if other authors accept the contribution. Edit longevity equally rewards people who create content, and people who rearrange or polish the content. Finally, edit longevity distinguishes the people who contribute little (who have contribution close to zero) from spammers or vandals, whose contribution quickly grows negative. © 2008 ACM.},
   author = {B. Thomas Adler and Luca De Alfaro and Ian Pye and Vishwanath Raman},
   doi = {10.1145/1822258.1822279},
   journal = {WikiSym 2008 - The 4th International Symposium on Wikis, Proceedings},
   keywords = {H53 [Information Interfaces and Presentation]: Group and Organization Interfaces-Computer-supported cooperative work, Web-based interaction,J4 [Social and Behavioral Sciences]: Miscellaneous *,K43 [Computers and Society]: Orga-nizational Impacts-Computer-supported collaborative work},
   title = {Measuring author contributions to the Wikipedia},
   year = {2008},
}
@article{Vuong2008,
   abstract = {Wikipedia 1 is a very large and successful Web 2.0 example. As the number of Wikipedia articles and contributors grows at a very fast pace, there are also increasing disputes occurring among the contributors. Disputes often happen in articles with controversial content. They also occur frequently among contributors who are "aggressive" or controversial in their personalities. In this paper, we aim to identify controversial articles in Wikipedia. We propose three models, namely the Basic model and two Controversy Rank (CR) models. These models draw clues from collaboration and edit history instead of interpreting the actual articles or edited content. While the Basic model only considers the amount of disputes within an article, the two Controversy Rank models extend the former by considering the relationships between articles and contributors. We also derived enhanced versions of these models by considering the age of articles. Our experiments on a collection of 19,456 Wikipedia articles shows that the Controversy Rank models can more effectively determine controversial articles compared to the Basic and other baseline models. © 2008 ACM.},
   author = {Ba Quy Vuong and Ee Peng Lim and Aixin Sun and Minh Tam Le and Hady Wirawan Lauw and Kuiyu Chang},
   doi = {10.1145/1341531.1341556},
   isbn = {9781605587998},
   journal = {WSDM'08 - Proceedings of the 2008 International Conference on Web Search and Data Mining},
   keywords = {Controversy rank,Information Quality,Online dispute,Wikipedia},
   pages = {171-182},
   title = {On ranking controversies in wikipedia: Models and evaluation},
   url = {http://en.wikipedia.org/wiki/Wikipedia:Featured_article_criteria.},
   year = {2008},
}
@article{Halfaker2011,
   abstract = {Reverts are important to maintaining the quality of Wikipedia. They fix mistakes, repair vandalism, and help enforce policy. However, reverts can also be damaging, especially to the aspiring editor whose work they destroy. In this research we analyze 400,000 Wikipedia revisions to understand the effect that reverts had on editors. We seek to understand the extent to which they demotivate users, reducing the workforce of contributors, versus the extent to which they help users improve as encyclopedia editors. Overall we find that reverts are powerfully demotivating, but that their net influence is that more quality work is done in Wikipedia as a result of reverts than is lost by chasing editors away. However, we identify key conditions - most specifically new editors being reverted by much more experienced editors - under which reverts are particularly damaging. We propose that reducing the damage from reverts might be one effective path for Wikipedia to solve the newcomer retention problem. © 2011 ACM.},
   author = {Aaron Halfaker and Aniket Kittur and John Riedl},
   doi = {10.1145/2038558.2038585},
   isbn = {9781450309097},
   journal = {WikiSym 2011 Conference Proceedings - 7th Annual International Symposium on Wikis and Open Collaboration},
   keywords = {WikiWork,Wikipedia,experience,motivation,productivity,quality,revert},
   pages = {163-172},
   title = {Don't bite the newbies: How reverts affect the quantity and quality of Wikipedia work},
   url = {https://dl.acm.org/doi/10.1145/2038558.2038585},
   year = {2011},
}
@article{Lipka2010,
   abstract = {Wikipedia provides an information quality assessment model with criteria for human peer reviewers to identify featured articles. For this classification task "Is an article featured or not?" we present a machine learning approach that exploits an article's character trigram distribution. Our approach differs from existing research in that it aims to writing style rather than evaluating meta features like the edit history. The approach is robust, straightforward to implement, and outperforms existing solutions. We underpin these claims by an experiment design where, among others, the domain transferability is analyzed. The achieved performances in terms of the F-measure for featured articles are 0.964 within a single Wikipedia domain and 0.880 in a domain transfer situation. © 2010 Copyright is held by the author/owner(s).},
   author = {Nedim Lipka and Benno Stein},
   doi = {10.1145/1772690.1772847},
   isbn = {9781605587998},
   journal = {Proceedings of the 19th International Conference on World Wide Web, WWW '10},
   keywords = {Information Quality,domain transfer,information quality,wikipedia},
   pages = {1147-1148},
   title = {Identifying featured articles in Wikipedia: Writing style matters},
   url = {https://dl.acm.org/doi/10.1145/1772690.1772847},
   year = {2010},
}
@article{Arazy2010,
   abstract = {The success of Wikipedia and the relative high quality of its articles seem to contradict conventional wisdom. Recent studies have begun shedding light on the processes contributing to Wikipedia's success, highlighting the role of coordination and contribution inequality. In this study, we expand on these works in two ways. First, we make a distinction between global (Wikipedia-wide) and local (article-specific) inequality and investigate both constructs. Second, we explore both direct and indirect effects of these inequalities, exposing the intricate relationships between global inequality, local inequality, coordination, and article quality. We tested our hypotheses on a sample of a Wikipedia articles using structural equation modeling and found that global inequality exerts significant positive impact on article quality, while the effect of local inequality is indirect and is mediated by coordination. Copyright 2010 ACM.},
   author = {Ofer Arazy and Oded Nov},
   doi = {10.1145/1718918.1718963},
   isbn = {9781605589879},
   journal = {Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW},
   keywords = {Contribution inequality,Coordination,Global inequality,Information quality,Local inequality,Wikipedia},
   pages = {233-236},
   title = {Determinants of wikipedia quality: The roles of global and local contribution inequality},
   url = {https://dl.acm.org/doi/10.1145/1718918.1718963},
   year = {2010},
}
@article{Yaari2011,
   abstract = {This study examines the ways in which information consumers evaluate the quality of content in a collaborative-writing environment, in this case Wikipedia. Sixty-four users were asked to assess the quality of five articles from the Hebrew Wikipedia, to indicate the highest and lowest-quality article of the five and explain their choices. Participants viewed both the article page, and the article's history page, so that their decision was based both on the article's current content and on its development. The analysis shows that the attributes that most frequently assisted the users in deciding about the quality of the items were not unique to Wikipedia: attributes such as amount of information, satisfaction with content and external links were mentioned frequently, as with other information quality studies on the web. The findings also support the claim that quality is a subjective concept which depends on the user's unique point of view. Attributes such as number of edits and number of unique editors received two contradictory meanings-both few edits/editors and many edits/editors were mentioned as attributes of high-quality articles.},
   author = {E Yaari and Shifra Baruchson-Arbib and Judit Bar-Ilan},
   doi = {10.1177/0165551511416065},
   issue = {5},
   journal = {Journal of Information Science},
   keywords = {Wikipedia,information quality assessment,user-study},
   month = {10},
   pages = {487-498},
   title = {Information quality assessment of community generated content: A user study of Wikipedia},
   volume = {37},
   url = {https://journals.sagepub.com/doi/abs/10.1177/0165551511416065},
   year = {2011},
}
@article{Druck2010,
   abstract = {Although some have argued that Wikipedia’s open edit policy is one of the primary reasons for its success, it also raises
concerns about quality — vandalism, bias, and errors can be
problems. Despite these challenges, Wikipedia articles are
often (perhaps surprisingly) of high quality, which many attribute to both the dedicated Wikipedia community and “good
Samaritan” users. As Wikipedia continues to grow, however,
it becomes more difficult for these users to keep up with the
increasing number of articles and edits. This motivates the
development of tools to assist users in creating and maintaining quality. In this paper, we propose metrics that quantify the
quality of contributions to Wikipedia through implicit feedback from the community. We then learn discriminative probabilistic models that predict the quality of a new edit using
features of the changes made, the author of the edit, and the
article being edited. Through estimating parameters for these
models, we also gain an understanding of factors that influence quality. We advocate using edit quality predictions and
information gleaned from model analysis not to place restrictions on editing, but to instead alert users to potential quality
problems, and to facilitate the development of additional incentives for contributors. We evaluate the edit quality prediction models on the Spanish Wikipedia. Experiments demonstrate that the models perform better when given access to
content-based features of the edit, rather than only features of
contributing user. This suggests that a user-based solution to
the Wikipedia quality problem may not be sufficient},
   author = {G Druck and G Miklau},
   isbn = {9781605589404},
   journal = {aaai.org},
   keywords = {H53 [Information Systems]: INFORMATION INTER-FACES AND PRESENTATION-Group and Organization Interfaces General Terms Experimentation,Human Factors,Measurement Keywords Wikipedia,models,quality},
   title = {Learning to predict the quality of contributions to wikipedia},
   url = {https://www.aaai.org/Papers/Workshops/2008/WS-08-15/WS08-15-002.pdf},
   year = {2010},
}
@article{Lucassen2010,
   abstract = {The use of Wikipedia as an information source is becoming increasingly popular. Several studies have shown that its information quality is high. Normally, when considering information trust, the source of information is an important factor. However, because of the open-source nature of Wikipedia articles, their sources remain mostly unknown. This means that other features need to be used to assess the trustworthiness of the articles. We describe article features - such as images and references - which lay Wikipedia readers use to estimate trustworthiness. The quality and the topics of the articles are manipulated in an experiment to reproduce the varying quality on Wikipedia and the familiarity of the readers with the topics. We show that the three most important features are textual features, references and images. © 2010 ACM.},
   author = {Teun Lucassen and Jan Maarten Schraagen},
   doi = {10.1145/1772938.1772944},
   isbn = {9781605589404},
   journal = {Proceedings of the 4th Workshop on Information Credibility, WICOW '10},
   keywords = {models,quality,think-aloud,trustworthiness,wikipedia},
   pages = {19-26},
   title = {Trust in wikipedia: How users trust information from an unknown source},
   url = {https://www.researchgate.net/publication/221021979_Trust_in_wikipedia_how_users_trust_information_from_an_unknown_source},
   year = {2010},
}
@article{Stvilia2008,
   abstract = {The classic problem within the information quality (IQ) research and practice community has been the problem of defining IQ. It has been found repeatedly that IQ is context sensitive and cannot be described, measured, and assured with a single model. There is a need for empirical case studies of IQ work in different systems to develop a systematic knowledge that can then inform and guide the construction of context-specific IQ models. This article analyzes the organization of IQ assurance work in a large-scale, open, collaborative encyclopedia-Wikipedia. What is special about Wikipedia as a resource is that the quality discussions and processes are strongly connected to the data itself and are accessible to the general public. This openness makes it particularly easy for researchers to study a particular kind of collaborative work that is highly distributed and that has a particularly substantial focus, not just on error detection but also on error correction. We believe that the study of those evolving debates and processes and of the IQ assurance model as a whole has useful implications for the improvement of quality in other more conventional databases.},
   author = {Besiki Stvilia and Michael B. Twidale and Linda C. Smith and Les Gasser},
   doi = {10.1002/ASI.20813},
   issn = {15322882},
   issue = {6},
   journal = {Journal of the American Society for Information Science and Technology},
   month = {4},
   pages = {983-1001},
   title = {Information quality work organization in Wikipedia},
   volume = {59},
   year = {2008},
}
@article{Adler2008,
   abstract = {The Wikipedia is a collaborative encyclopedia: anyone can contribute to its articles simply by clicking on an "edit" button. The open nature of the Wikipedia has been key to its success, but has also created a challenge: how can readers develop an informed opinion on its reliability? We propose a system that computes quantitative values of trust for the text in Wikipedia articles; these trust values provide an indication of text reliability. The system uses as input the revision history of each article, as well as information about the reputation of the contributing authors, as provided by a reputation system. The trust of a word in an article is computed on the basis of the reputation of the original author of the word, as well as the reputation of all authors who edited text near the word. The algorithm computes word trust values that vary smoothly across the text; the trust values can be visualized using varying text-background colors. The algorithm ensures that all changes to an article's text are reflected in the trust values, preventing surreptitious content changes. We have implemented the proposed system, and we have used it to compute and display the trust of the text of thousands of articles of the English Wikipedia. To validate our trust-computation algorithms, we show that text labeled as low-trust has a significantly higher probability of being edited in the future than text labeled as high-trust. © 2008 ACM.},
   author = {B. Thomas Adler and Krishnendu Chatterjee and Luca De Alfaro and Marco Faella and Ian Pye and Vishwanath Raman},
   doi = {10.1145/1822258.1822293},
   journal = {WikiSym 2008 - The 4th International Symposium on Wikis, Proceedings},
   keywords = {K43 [Computers and Society],Organi-zational Impacts,Web-based interaction},
   title = {Assigning trust to Wikipedia content},
   url = {https://www.researchgate.net/publication/200773226_Assigning_Trust_to_Wikipedia_Content},
   year = {2008},
}
@article{,
   abstract = {This paper discusses an approach to modeling and measuring information quality of Wikipedia articles. The approach is based on the idea that the quality of Wikipedia articles with distinctly different profiles needs to be measured using different information quality models. We report on our initial study, which involved two categories of Wikipedia articles: " stabilized" (those, whose content has not undergone major changes for a significant period of time) and "controversial" (the articles, which have undergone vandalism, revert wars, or whose content is subject to internal discussions between Wikipedia editors). We present simple information quality models and compare their performance on a subset of Wikipedia articles with the information quality evaluations provided by human users. Our experiment shows, that using special-purpose models for information quality captures user sentiment about Wikipedia articles better than using a single model for both categories of articles. © 2010 ACM.},
   author = {Gabriel De La Calzada and Alex Dekhtyar},
   doi = {10.1145/1772938.1772943},
   isbn = {9781605589404},
   journal = {Proceedings of the 4th Workshop on Information Credibility, WICOW '10},
   keywords = {models,quality,wikipedia},
   pages = {11-18},
   title = {On measuring the quality of wikipedia articles},
   url = {https://dl.acm.org/doi/10.1145/1772938.1772943},
   year = {2010},
}
@article{Adler2010,
   abstract = {WikiTrust is a reputation system for Wikipedia authors and content. WikiTrust computes three main quantities: edit quality, author reputation, and content reputation. The edit quality measures how well each edit, that is, each change introduced in a revision, is preserved in subsequent revisions. Authors who perform good quality edits gain reputation, and text which is revised by sev- eral high-reputation authors gains reputation. Since vandalism on the Wikipedia is usually performed by anonymous or new users (not least because long-time vandals end up banned), and is usually reverted in a reasonably short span of time, edit quality, author reputation, and content reputation are obvious candi- dates as features to identify vandalism on the Wikipedia. Indeed, using the full set of features computed by WikiTrust, we have been able to construct classifiers that identify vandalism with a recall of 83.5%, a precision of 48.5%, and a false positive rate of 8%, for an area under the ROC curve of 93.4%. If we limit our- selves to the set of features available at the time an edit is made (when the edit quality is still unknown), the classifier achieves a recall of 77.1%, a precision of 36.9%, and a false positive rate of 12.2%, for an area under the ROC curve of 90.4%. Using these classifiers, we have implemented a simple Web API that provides the vandalism estimate for every revision of the English Wikipedia. The API can be used both to identify vandalism that needs to be reverted, and to select high- quality, non-vandalized recent revisions of any given Wikipedia article. These recent high-quality revisions can be included in static snapshots of the Wikipedia, or they can be used whenever tolerance to vandalism is low (as in a school setting, or whenever the material is widely disseminated).},
   author = {B Adler and L De Alfaro},
   journal = {uni-weimar.de},
   title = {Detecting wikipedia vandalism using wikitrust},
   url = {https://www.researchgate.net/publication/221159945_Detecting_wikipedia_vandalism_using_WikiTrust_Lab_report_for_PAN_at_CLEF_2010},
   year = {2010},
}
@article{Lim2006,
   abstract = {Using open source Web editing software (e.g., wiki), online community users can now easily edit, review and publish articles collaboratively. While much useful knowledge can be derived from these articles, content users and critics are often concerned about their qualities. In this paper, we develop two models, namely basic model and peer review model, for measuring the qualities of these articles and the authorities of their contributors. We represent collaboratively edited articles and their contributors in a bipartite graph. While the basic model measures an article's quality using both the authorities of contributors and the amount of contribution from each contributor, the peer review model extends the former by considering the review aspect of article content. We present results of experiments conducted on some Wikipedia pages and their contributors. Our result show that the two models can effectively determine the articles' qualities and contributors' authorities using the collaborative nature of online communities},
   author = {EP Lim and BQ Vuong},
   journal = {ieeexplore.ieee.org},
   title = {Measuring qualities of articles contributed by online communities},
   url = {https://ieeexplore.ieee.org/abstract/document/4061345/},
   year = {2006},
}
@article{Daxenberger2013,
   abstract = {In this paper, we analyze a novel set of features for the task of automatic edit category classification. Edit category classification assigns categories such as spelling error correction , paraphrase or vandalism to edits in a document. Our features are based on differences between two versions of a document including meta data, textual and language properties and markup. In a supervised machine learning experiment, we achieve a micro-averaged F1 score of .62 on a corpus of edits from the En-glish Wikipedia. In this corpus, each edit has been multi-labeled according to a 21-category taxonomy. A model trained on the same data achieves state-of-the-art performance on the related task of fluency edit classification. We apply pattern mining to automatically labeled edits in the revision histories of different Wikipedia articles. Our results suggest that high-quality articles show a higher degree of homogeneity with respect to their collaboration patterns as compared to random articles.},
   author = {J Daxenberger},
   journal = {aclweb.org},
   pages = {578-589},
   publisher = {Association for Computational Linguistics},
   title = {Automatically classifying edit categories in Wikipedia revisions},
   url = {https://www.aclweb.org/anthology/D13-1055.pdf},
   year = {2013},
}
@article{Kittur2008,
   abstract = {Wikipedia has become one of the most important information resources on the Web by promoting peer collaboration and enabling virtually anyone to edit anything. However, this mutability also leads many to distrust it as a reliable source of information. Although there have been many attempts at developing metrics to help users judge the trustworthiness of content, it is unknown how much impact such measures can have on a system that is perceived as inherently unstable. Here we examine whether a visualization that exposes hidden article information can impact readers' perceptions of trustworthiness in a wiki environment. Our results suggest that surfacing information relevant to the stability of the article and the patterns of editor behavior can have a significant impact on users' trust across a variety of page types.},
   author = {A Kittur and B Suh},
   isbn = {9781605580074},
   journal = {dl.acm.org},
   keywords = {Author Keywords Wikipedia,Computer-supported cooperative work,H35 [Information Storage and Retrieval]: Online Information Systems,K43 [Computers and Society]: Organizational Impacts-Computer-supported collaborative work,Web-based interaction,collaboration,social computing ACM Classification Keywords H53 [Information Interfaces]: Group and Organization Interfaces-Collaborative computing,stability,trust,visualization,wiki},
   title = {Can you ever trust a Wiki? Impacting perceived trustworthiness in Wikipedia},
   url = {https://dl.acm.org/doi/abs/10.1145/1460563.1460639},
   year = {2008},
}
@article{Khairova2017,
   abstract = {We present the method of estimating the quality of articles in Rus-sian Wikipedia that is based on counting the number of facts in the article. For calculating the number of facts we use our logical-linguistic model of fact extraction. Basic mathematical means of the model are logical-algebraic equations of the finite predicates algebra. The model allows extracting of simple and complex types of facts in Russian sentences. We experimentally compare the effect of the density of these types of facts on the quality of articles in Russian Wikipedia. Better articles tend to have a higher density of facts.},
   author = {N Khairova and W Lewoniewski},
   doi = {10.1007/978-3-319-59336-4_3},
   journal = {Springer},
   keywords = {Russian Wikipedia,article quality,fact extraction,logical equations},
   pages = {28-40},
   publisher = {Springer Verlag},
   title = {Estimating the quality of articles in Russian wikipedia using the logical-linguistic model of fact extraction},
   volume = {288},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-59336-4_3},
   year = {2017},
}
@article{Kumar2016,
   abstract = {Wikipedia is a major source of information for many people. However , false information on Wikipedia raises concerns about its credibility. One way in which false information may be presented on Wikipedia is in the form of hoax articles, i.e., articles containing fabricated facts about nonexistent entities or events. In this paper we study false information on Wikipedia by focusing on the hoax articles that have been created throughout its history. We make several contributions. First, we assess the real-world impact of hoax articles by measuring how long they survive before being de-bunked, how many pageviews they receive, and how heavily they are referred to by documents on the Web. We find that, while most hoaxes are detected quickly and have little impact on Wikipedia, a small number of hoaxes survive long and are well cited across the Web. Second, we characterize the nature of successful hoaxes by comparing them to legitimate articles and to failed hoaxes that were discovered shortly after being created. We find characteristic differences in terms of article structure and content, embeddedness into the rest of Wikipedia, and features of the editor who created the hoax. Third, we successfully apply our findings to address a series of classification tasks, most notably to determine whether a given article is a hoax. And finally, we describe and evaluate a task involving humans distinguishing hoaxes from non-hoaxes. We find that humans are not good at solving this task and that our automated classifier outperforms them by a big margin.},
   author = {S Kumar and R West},
   doi = {10.1145/2872427.2883085},
   isbn = {9781450341431},
   journal = {dl.acm.org},
   pages = {591-602},
   publisher = {International World Wide Web Conferences Steering Committee},
   title = {Disinformation on the web: Impact, characteristics, and detection of wikipedia hoaxes},
   url = {https://dl.acm.org/doi/abs/10.1145/2872427.2883085},
   year = {2016},
}
@article{Chin2010,
   abstract = {This paper proposes an active learning approach using language model statistics to detect Wikipedia vandalism. Wikipedia is a popular and influential collaborative information system. The collaborative nature of authoring, as well as the high visibility of its content, have exposed Wikipedia articles to vandalism. Vandalism is defined as malicious editing intended to compromise the integrity of the content of articles. Extensive manual efforts are being made to combat vandalism and an automated approach to alleviate the laborious process is needed. This paper builds statistical language models, constructing distributions of words from the revision history of Wikipedia articles. As vandalism often involves the use of unexpected words to draw attention, the fitness (or lack thereof) of a new edit when compared with language models built from previous versions may well indicate that an edit is a vandalism instance. In addition, the paper adopts an active learning model to solve the problem of noisy and incomplete labeling of Wikipedia vandalism. The Wikipedia domain with its revision histories offers a novel context in which to explore the potential of language models in characterizing author intention. As the experimental results presented in the paper demonstrate, these models hold promise for vandalism detection. © 2010 ACM.},
   author = {Si Chi Chin and W. Nick Street and Padmini Srinivasan and David Eichmann},
   doi = {10.1145/1772938.1772942},
   isbn = {9781605589404},
   journal = {Proceedings of the 4th Workshop on Information Credibility, WICOW '10},
   keywords = {active learning,classification,collaboration,social computing ACM Classification Keywords H53 [Information Interfaces]: Group and Organization Interfaces-Collaborative computing,stability,statistical language models,trust,vandalism,visualization,wiki,wikipedia},
   pages = {3-10},
   title = {Detecting wikipedia vandalism with active learning and statistical language models},
   url = {https://dl.acm.org/doi/10.1145/1772938.1772942},
   year = {2010},
}
@article{Kane2016,
   abstract = {The 15-year history of collaboration on Wikipedia offers insight into how peer production communities create knowledge. In this research, we combine disparate content and collaboration approaches through a social network analysis approach known as an affiliation network. It captures both how knowledge is transferred in a peer production network and also the underlying skills possessed by its contributors in a single methodological approach. We test this approach on the Wikipedia articles dedicated to medical information developed in a subcommunity known as a WikiProject. Overall, we find that the position of an article in the affiliation network is associated with the quality of the article. We further investigate information quality through additional qualitative and quantitative approaches including expert coders using medical students, crowdsourcing using Amazon Mechanical Turk, and visualization using network graphs. A review by fourth-year medical students indicates that the Wikipedia quality rating is a reliable measure of information quality. Amazon Mechanical Turk ratings, however, are a less reliable measure of information quality, reflecting observable content characteristics such as article length and the number of references.},
   author = {Gerald C. Kane and Sam Ransbotham},
   doi = {10.1287/ISRE.2016.0622},
   issn = {15265536},
   issue = {2},
   journal = {Information Systems Research},
   keywords = {Information quality,Network analysis,Social media},
   month = {6},
   pages = {424-439},
   publisher = {INFORMS Inst.for Operations Res.and the Management Sciences},
   title = {Content and collaboration: An affiliation network approach to information quality in online peer production communities},
   volume = {27},
   url = {https://www.researchgate.net/publication/299650698_Research_Note-Content_and_Collaboration_An_Affiliation_Network_Approach_to_Information_Quality_in_Online_Peer_Production_Communities},
   year = {2016},
}
@article{Thalhammer2016,
   abstract = {Link analysis methods are used to estimate importance in graph-structured data. In that realm, the PageRank algorithm has been used to analyze directed graphs, in particular the link structure of the Web. Recent developments in information retrieval focus on entities and their relations (i.e., knowledge graph panels). Many entities are documented in the popular knowledge base Wikipedia. The cross-references within Wikipedia exhibit a directed graph structure that is suitable for computing PageRank scores as importance indicators for entities. In this work, we present different PageRank-based analyses on the link graph of Wikipedia and according experiments. We focus on the question whether some links—based on their context/position in the article text—can be deemed more important than others. In our variants, we change the probabilistic impact of links in accordance to their context/position on the page and measure the effects on the output of the PageRank algorithm. We compare the resulting rankings and those of existing systems with page-view-based rankings and provide statistics on the pairwise computed Spearman and Kendall rank correlations.},
   author = {Andreas Thalhammer and Achim Rettinger},
   doi = {10.1007/978-3-319-47602-5_41},
   isbn = {9783319476018},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {DBpedia,Link analysis,Page views,PageRank,Rank correlation,Wikipedia},
   pages = {227-240},
   publisher = {Springer Verlag},
   title = {Pagerank on wikipedia: Towards general importance scores for entities},
   volume = {9989 LNCS},
   year = {2016},
}
@article{Reavley2021,
   abstract = {Background. Although mental health information on the internet is often of poor quality, relatively little is known about the quality of websites, such as Wikipedia, that involve participatory information sharing. The aim of this paper was to explore the quality of user-contributed mental health-related information on Wikipedia and compare this with centrally controlled information sources. Method. Content on 10 mental health-related topics was extracted from 14 frequently accessed websites (including Wikipedia) providing information about depression and schizophrenia, Encyclopaedia Britannica, and a psychiatry textbook. The content was rated by experts according to the following criteria : accuracy, up-to-dateness, breadth of coverage, referencing and readability. Results. Ratings varied significantly between resources according to topic. Across all topics, Wikipedia was the most highly rated in all domains except readability. Conclusions. The quality of information on depression and schizophrenia on Wikipedia is generally as good as, or better than, that provided by centrally controlled websites, Encyclopaedia Britannica and a psychiatry textbook.},
   author = {N J Reavley and A J Mackinnon and A J Morgan and M Alvarez-Jimenez and S E Hetrick and E Killackey and B Nelson and R Purcell and M B H Yap and A F Jorm},
   doi = {10.1017/S003329171100287X},
   journal = {cambridge.org},
   keywords = {Internet information,Wikipedia,mental disorders,quality},
   title = {Quality of information sources about mental disorders: a comparison of Wikipedia with centrally controlled web and printed sources},
   url = {https://www.cambridge.org/core/journals/psychological-medicine/article/quality-of-information-sources-about-mental-disorders-a-comparison-of-wikipedia-with-centrally-controlled-web-and-printed-sources/595CEE672BB7C503101FAF5A9E303673},
   year = {2021},
}
@article{Arazy2011,
   abstract = {The notion of information quality (IQ) has been investigated extensively in recent years. Much of this research has been aimed at conceptualizing IQ and its underlying dimensions (e.g., accuracy, completeness) and at developing instruments for measuring these quality dimensions. However, less attention has been given to the measurability of IQ. The objective of this study is to explore the extent to which a set of IQ dimensions (accuracy, completeness, objectivity, and representation) lend themselves to reliable measurement. By reliable measurement, we refer to the degree to which independent assessors are able to agree when rating objects on these various dimensions. Our study reveals that multiple assessors tend to agree more on certain dimensions (e.g., accuracy) while finding it more difficult to agree on others (e.g., completeness). We argue that differences in measurability stem from properties inherent to the quality dimension (i.e., the availability of heuristics that make the assessment more tangible) as well as on assessors' reliance on these cues. Implications for theory and practice are discussed. © 2010 ASIS&T.},
   author = {Ofer Arazy and Rick Kopak},
   doi = {10.1002/ASI.21447},
   issn = {15322882},
   issue = {1},
   journal = {Journal of the American Society for Information Science and Technology},
   month = {1},
   pages = {89-99},
   title = {On the measurability of information quality},
   volume = {62},
   year = {2011},
}
@article{Stvilia2005,
   abstract = {Effective information quality analysis needs powerful yet easy ways to obtain metrics. The English version of Wikipedia provides an extremely interesting yet challenging case for the study of Information Quality dynamics at both macro and micro levels. We propose seven IQ metrics which can be evaluated automatically and test the set on a representative sample of Wikipedia content. The methodology of the metrics construction and the results of tests, along with a number of statistical characterizations of Wikipedia articles, their content construction, process metadata and social context are reported.},
   author = {B Stvilia and MB Twidale and L Gasser Culture},
   journal = {World Scientific},
   title = {Information quality in a community-based encyclopedia},
   url = {https://www.worldscientific.com/doi/abs/10.1142/9789812701527_0009},
   year = {2005},
}
@article{Dondio2007,
   abstract = {The problem of identifying useful and trustworthy information on the World Wide Web is becoming increasingly acute as new tools such as wikis and blogs simplify and democratize publication. It is not hard to predict that in the future the direct reliance on this material will expand and the problem of evaluating the trustworthiness of this kind of content become crucial. The Wikipedia project represents the most successful and discussed example of such online resources. In this paper we present a method to predict Wikipedia articles trustworthiness based on computational trust techniques and a deep domain-specific analysis. Our assumption is that a deeper understanding of what in general defines high-standard and expertise in domains related to Wikipedia-i.e. content quality in a collaborative environment-mapped onto Wikipedia elements would lead to a complete set of mechanisms to sustain trust in Wikipedia context. We present a series of experiment. The first is a study-case over a specific category of articles; the second is an evaluation over 8 000 articles representing 65% of the overall Wikipedia editing activity. We report encouraging results on the automated evaluation of Wikipedia content using our domain-specific expertise method. Finally, in order to appraise the value added by using domain-specific expertise, we compare our results with the ones obtained with a pre-processed cluster analysis, where complex expertise is mostly replaced by training and automatic classification of common features. Povzetek: Ocenjena je stopnja zaupanja v strani v Wikipediji.},
   author = {P Dondio and S Barret},
   journal = {informatica.si},
   keywords = {Wikipedia,computational trust,content-quality},
   title = {Computational trust in Web content quality: a comparative evalutation on the Wikipedia project},
   url = {http://informatica.si/index.php/informatica/article/viewFile/137/129},
   year = {2007},
}
@article{Fahy2014,
   abstract = {Background: The popularity of the Internet has enabled unprecedented access to health information. As a largely unregulated source, there is potential for inconsistency in the quality of information that reaches the patient.

Aims: To review the literature relating to the quality indicators of health information for patients on the Internet.

Method: A search of English language literature was conducted using PubMed, Google Scholar and EMBASE databases.

Results: Many articles have been published which assess the quality of information relating to specific medical conditions. Indicators of quality have been defined in an attempt to predict higher quality health information on the Internet. Quality evaluation tools are scoring systems based on indicators of quality. Established tools such as the HONcode may help patients navigate to more reliable information. Google and Wikipedia are important emerging sources of patient health information.

Conclusion: The Internet is crucial for modern dissemination of health information, but it is clear that quality varies significantly between sources. Quality indicators for web-information have been developed but there is no agreed standard yet. We envisage that reliable rating tools, effective search engine ranking and progress in crowd-edited websites will enhance patient access to health information on the Internet.},
   author = {E Fahy and R Hardikar and A Fox and S Mackay - The Australasian medical and undefined 2014},
   journal = {ncbi.nlm.nih.gov},
   title = {Quality of patient health information on the Internet: reviewing a complex and evolving landscape},
   url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3920473/},
   year = {2014},
}
@article{Anthony2009,
   abstract = {An important organizational innovation enabled by the revolution in information technologies is 'open source' production which converts private commodities into essentially public goods. Similar to other public goods, incentives for reputation and group identity appear to motivate contributions to open source projects, overcoming the social dilemma inherent in producing such goods. In this paper we examine how contributor motivations affect the type of contributions made to the open source online encyclopedia Wikipedia. As expected, we find that registered participants, motivated by reputation and commitment to the Wikipedia community, make many contributions with high reliability. Surprisingly, however, we find the highest reliability from the vast numbers of anonymous 'Good Samaritans' who contribute only once. Our findings of high reliability in the contributions of both Good Samaritans and committed 'zealots' suggest that open source production succeeds by altering the scope of production such that a critical mass of contributors can participate. © 2009 The Author(s).},
   author = {Denise Anthony and Sean W. Smith and Timothy Williamson},
   doi = {10.1177/1043463109336804},
   issn = {10434631},
   issue = {3},
   journal = {Rationality and Society},
   keywords = {Collective action,Collective goods,Group identity,Open source production,Public goods,Reputation,Technology},
   month = {8},
   pages = {283-306},
   title = {Reputation and reliability in collective goods: The case of the online encyclopedia Wikipedia},
   volume = {21},
   url = {https://www.researchgate.net/publication/230709192_Reputation_and_Reliability_in_Collective_Goods_The_Case_of_the_Online_Encyclopedia_Wikipedia},
   year = {2009},
}
@article{Rad2012,
   abstract = {Wikipedia articles are the result of the collaborative editing of a diverse group of anonymous volunteer editors, who are passionate and knowledgeable about specific topics. One can argue that this plurality of perspectives leads to broader coverage of the topic, thus benefitting the reader. On the other hand, differences among editors on polarizing topics can lead to controversial or questionable content, where facts and arguments are presented and discussed to support a particular point of view. Controversial articles are manually tagged by Wikipedia editors, and span many interesting and popular topics, such as religion, history, and politics, to name a few. Recent works have been proposed on automatically identifying controversy within unmarked articles. However, to date, no systematic comparison of these efforts has been made. This is in part because the various methods are evaluated using different criteria and on different sets of articles by different authors, making it hard for anyone to verify the efficacy and compare all alternatives. We provide a first attempt at bridging this gap. We compare five different methods for modelling and identifying controversy, and discuss some of the unique difficulties and opportunities inherent to the way Wikipedia is produced. © 2012 ACM.},
   author = {Hoda Sepehri Rad and Denilson Barbosa},
   doi = {10.1145/2462932.2462942},
   isbn = {9781450316057},
   journal = {WikiSym 2012 Conference Proceedings - 8th Annual International Symposium on Wikis and Open Collaboration},
   keywords = {Mono-tonicity,Wikipedia,comparison,controversy,disagreement,monotonicity},
   title = {Identifying controversial articles in Wikipedia: A comparative study},
   url = {https://dl.acm.org/doi/10.1145/2462932.2462942},
   year = {2012},
}
@article{Stvilia2007,
   abstract = {One cannot manage information quality (IQ) without first being able to measure it meaningfully and establishing a causal connection between the source of IQ change, the IQ problem types, the types of activities affected, and their implications. In this article we propose a general IQ assessment framework. In contrast to context-specific IQ assessment models, which usually focus on a few variables determined by local needs, our framework consists of comprehensive typologies of IQ problems, related activities, and a taxonomy of IQ dimensions organized in a systematic way based on sound theories and practices. The framework can be used as a knowledge resource and as a guide for developing IQ measurement models for many different settings. The framework was validated and refined by developing specific IQ measurement models for two large-scale collections of two large classes of information objects: Simple Dublin Core records and online encyclopedia articles.},
   author = {Besiki Stvilia and Les Gasser and Michael B. Twidale and Linda C. Smith},
   doi = {10.1002/ASI.20652},
   issn = {15322882},
   issue = {12},
   journal = {Journal of the American Society for Information Science and Technology},
   month = {10},
   pages = {1720-1733},
   title = {A framework for information quality assessment},
   volume = {58},
   year = {2007},
}
@article{Royal2009,
   abstract = {The World Wide Web continues to grow closer to achieving the vision of becoming the repository of all human knowledge, as features and applications that support user-generated content become more prevalent. Wikipedia is fast becoming an important resource for news and information. It is an online information source that is increasingly used as the first, and sometimes only, stop for online encyclopedic information. Using a method employed by Tankard and Royal to judge completeness of Web content, completeness of information on Wikipedia is assessed. Some topics are covered more comprehensively than others, and the predictors of these biases include recency, importance, population, and financial wealth. Wikipedia is more a socially produced document than a value-free information source. It reflects the viewpoints, interests, and emphases of the people who use it. © 2009 Sage Publications.},
   author = {Cindy Royal and Deepina Kapila},
   doi = {10.1177/0894439308321890},
   issn = {08944393},
   issue = {1},
   journal = {Social Science Computer Review},
   keywords = {Completeness of information,Open source,Social network,Wiki,Wikipedia},
   month = {2},
   pages = {138-148},
   title = {What's on wikipedia, and what's not... ?: Assessing completeness of information},
   volume = {27},
   year = {2009},
}
