@article{Dang2017,
   abstract = {Wikipedia is considered as the largest knowledge repository in the history of humanity and plays a crucial role in modern daily life. Assigning the correct quality class to Wikipedia articles is an important task in order to provide guidance for both authors and readers of Wikipedia. The manual review cannot cope with the editing speed of Wikipedia. An automatic classification is required to classify the quality of Wikipedia articles. Most existing approaches rely on traditional machine learning with manual feature engineering, which requires a lot of expertise and effort. Furthermore, it is known that there is no general perfect feature set because information leak always occurs in feature extraction phase. Also, for each language of Wikipedia, a new feature set is required. In this paper, we present an approach relying on deep learning for quality classification of Wikipedia articles. Our solution relies on Recurrent Neural Networks (RNN) which is an endto-end learning technique that eliminates disadvantages of feature engineering. Our approach learns directly from raw data without human intervention and is language-neutral. Experimental results on English, French and Russian Wikipedia datasets show that our approach outperforms state-of-the-art solutions.},
   author = {Quang Vinh Dang and Claudia Lavinia Ignat},
   doi = {10.1145/3125433.3125448},
   isbn = {9781450351874},
   journal = {Proceedings of the 13th International Symposium on Open Collaboration, OpenSym 2017},
   keywords = {Deep learning,Document quality,End-to-end learning,Long-Short Term Memory (LSTM),Recurrent Neural Network (RNN),Wikipedia},
   month = {8},
   publisher = {Association for Computing Machinery, Inc},
   title = {An end-to-end learning solution for assessing the quality of Wikipedia articles},
   url = {https://doi.org/10.1145/3125433.3125448},
   year = {2017},
}
@article{Dalip2009,
   abstract = {The old dream of a universal repository containing all the human knowledge and culture is becoming possible through the Internet and the Web. Moreover, this is happening with the direct collaborative, participation of people. Wikipedia is a great example. It is an enormous repository of information with free access and edition, created by the community in a collaborative manner. However, this large amount of information, made available democratically and virtually without any control, raises questions about its relative quality. In this work we explore a significant number of quality indicators, some of them proposed by us and used here for the first time, and study their capability to assess the quality of Wikipedia articles. Furthermore, we explore machine learning techniques to combine these quality indicators into one single assessment judgment. Through experiments, we show that the most important quality indicators are the easiest ones to extract, namely, textual features related to length, structure and style. We were also able to determine which indicators did not contribute significantly to the quality assessment. These were, coincidentally, the most complex features, such as those based on link analysis. Finally, we compare our combination method with state-of-the-art solution and show significant improvements in terms of effective quality prediction. Copyright 2009 ACM.},
   author = {Daniel Hasan Dalip and Marcos André Gonçalves and Marco Cristo and Pável Calado},
   doi = {10.1145/1555400.1555449},
   isbn = {9781605586977},
   issn = {15525996},
   journal = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries},
   keywords = {Machine learning,Quality assessment,SVM,User Issues] General Terms Human Factors,Wikipedia},
   pages = {295-304},
   title = {Automatic quality assessment of content created collaboratively by web communities: A case study of wikipedia},
   url = {http://en.wikipedia.org/wiki/Wikipedia},
   year = {2009},
}
@article{Chevalier2010,
   abstract = {As Wikipedia has become one of the most used knowledge bases worldwide, the problem of the trustworthiness of the information it disseminates becomes central. With WikipediaViz, we introduce five visual indicators integrated to the Wikipedia layout that can keep casual Wikipedia readers aware of important meta-information about the articles they read. The design of WikipediaViz was inspired by two participatory design sessions with expert Wikipedia writers and sociologists who explained the clues they used to quickly assess the trustworthiness of articles. According to these results, we propose five metrics for Maturity and Quality assessment ofWikipedia articles and their accompanying visualizations to provide the readers with important clues about the editing process at a glance. We also report and discuss about the results of the user studies we conducted. Two preliminary pilot studies show that all our subjects trust Wikipedia articles almost blindly. With the third study, we show that WikipediaViz significantly reduces the time required to assess the quality of articles while maintaining a good accuracy.},
   author = {Fanny Chevalier and Stéphane Huot and Jean Daniel Fekete},
   doi = {10.1109/PACIFICVIS.2010.5429611},
   isbn = {9781424466849},
   journal = {IEEE Pacific Visualization Symposium 2010, PacificVis 2010 - Proceedings},
   keywords = {Collaborative knowledge,Encyclopedia,Information visualization,Participatory design,Wikipedia},
   pages = {49-56},
   title = {WikipediaViz: Conveying article quality for casual wikipedia readers},
   url = {https://www.researchgate.net/publication/221536224_WikipediaViz_Conveying_Article_Quality_for_Casual_Wikipedia_Readers},
   year = {2010},
}
@article{Saengthongpattana2014,
   abstract = {The quality evaluation of Thai Wikipedia articles relies on user consideration. There are increasing numbers of articles every day therefore the automatic evaluation method is needed for user. Components of Wikipedia articles such as headers, pictures, references, and links are useful to indicate the quality of articles. However readers need complete content to cover all of concepts in that article. The concept features are investigated in this work. The aim of this research is to classify Thai Wikipedia articles into two classes namely high-quality and low-quality class. Three article domains (Biography, Animal, and Place) are testes with decision tree and Naïve Bayes. We found that Naïve Bayes gets high TP Rate compared to decision tree in every domain. Moreover, we found that the concept feature plays an important role in quality classification of Thai Wikipedia articles. © Springer International Publishing Switzerland 2014.},
   author = {Kanchana Saengthongpattana and Nuanwan Soonthornphisaj},
   doi = {10.1007/978-3-319-05951-8_49},
   isbn = {9783319059501},
   issn = {21945357},
   issue = {VOLUME 1},
   journal = {Advances in Intelligent Systems and Computing},
   keywords = {Concept feature,Decision tree,Naïve Bayes,Quality of Thai Wikipedia articles,Statistical feature},
   pages = {513-523},
   publisher = {Springer Verlag},
   title = {Assessing the quality of Thai Wikipedia articles using concept and statistical features},
   volume = {275 AISC},
   url = {https://www.researchgate.net/publication/290603893_Assessing_the_Quality_of_Thai_Wikipedia_Articles_Using_Concept_and_Statistical_Features},
   year = {2014},
}
@article{Su2016,
   abstract = {The great popularity of Wikipedia makes it one of the dominant knowledge source around the World. However, since one of the core principles of Wikipedia is being open for anyone to maintain it, Wikipedia cannot fully ensure the reliability of its articles, and thus sometimes suffered criticism for containing low-quality information. It is therefore essential to assess the quality of Wikipedia articles automatically. In this paper we describe how we approach that problem by using a psycho-lexical resource, i.e., the Language Inquiry and Word Count (LIWC) dictionary. By training a classifier on different LIWC categories, we discuss the implications of each category for Wikipedia quality assessment.},
   author = {Qi Su and Pengyuan Liu},
   doi = {10.1109/WI-IAT.2015.23},
   isbn = {9781467396172},
   journal = {Proceedings - 2015 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, WI-IAT 2015},
   keywords = {Information Quality,LIWC,User-generated Content,Wikipedia},
   month = {2},
   pages = {184-187},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A psycho-lexical approach to the assessment of information quality on wikipedia},
   url = {https://www.researchgate.net/publication/221023929_On_Measuring_the_Quality_of_Wikipedia_Articles},
   year = {2016},
}
@article{,
   abstract = {Collaboratively edited articles such as in Wikipedia suffer from well-identified problems regarding their quality, e.g., information accuracy, reputability of third-party sources, vandalism. Due to the huge number of articles and the intensive edit rate, the manual evaluation of article content quality is inconceivable. In this paper, we tackle the problem of automatically establishing the quality of Wikipedia articles. Evidences are shown to consider the interactions between authors and articles to assess the quality score. Collaborations between authors and reviewers are also considered to reinforce the discriminative process. This work gives a generic formulation of the Mutual Reinforcement principle held between articles quality and authors authority and take explicitly advantage of the co-edits graph generated by individuals. Experiments conducted on a set of representative data from Wikipedia show the effectiveness of our approach.},
   author = {Baptiste De La Robertie and Yoann Pitarch and Olivier Teste},
   doi = {10.1145/2808797.2808895},
   isbn = {9781450338547},
   journal = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2015},
   month = {8},
   pages = {464-471},
   publisher = {Association for Computing Machinery, Inc},
   title = {Measuring article quality in Wikipedia using the collaboration network},
   url = {https://dl.acm.org/doi/10.1145/2808797.2808895},
   year = {2015},
}
@article{Dang2017,
   abstract = {Wikipedia is a great example of large scale collaboration, where people from all over the world together build the largest and maybe the most important human knowledge repository in the history. However, a number of studies showed that the quality of Wikipedia articles is not equally distributed. While many articles are of good quality, many others need to be improved. Assessing the quality of Wikipedia articles is very important for guiding readers towards articles of high quality and suggesting authors and reviewers which articles need to be improved. Due to the huge size of Wikipedia, an effective automatic assessment method to measure Wikipedia articles quality is needed. In this paper, we present an automatic assessment method of Wikipedia articles quality by analyzing their content in terms of their format features and readability scores. Our results show improvements both in terms of accuracy and information gain compared with other existing approaches.},
   author = {Quang Vinh Dang and Claudia Lavinia Ignat},
   doi = {10.1109/CIC.2016.42},
   isbn = {9781509046072},
   journal = {Proceedings - 2016 IEEE 2nd International Conference on Collaboration and Internet Computing, IEEE CIC 2016},
   month = {1},
   pages = {266-275},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Measuring quality of collaboratively edited documents: The case of Wikipedia},
   url = {https://www.semanticscholar.org/paper/Measuring-Quality-of-Collaboratively-Edited-The-of-Dang-Ignat/31f226174f2da4ceca288b93769371c388d6c4fb},
   year = {2017},
}
@article{Bassani2019,
   abstract = {With the development of Web 2.0 technologies, people have gone from being mere content users to content generators. In this context, the evaluation of the quality of (potential) information available online has become a crucial issue. Nowadays, one of the biggest online resources that users rely on as a knowledge base is Wikipedia. The collaborative aspect at the basis of Wikipedia can let to the possible creation of low-quality articles or even misinformation if the process of monitoring the generation and the revision of articles is not performed in a precise and timely way. For this reason, in this paper, the problem of automatically evaluating the quality of Wikipedia contents is considered, by proposing a supervised approach based on Machine Learning to perform the classification of articles on qualitative bases. With respect to prior literature, a wider set of features connected to Wikipedia articles has been taken into account, as well as previously unconsidered aspects connected to the generation of a labeled dataset to train the model, and the use of Gradient Boosting, which produced encouraging results.},
   author = {Elias Bassani and Marco Viviani},
   doi = {10.1145/3297280.3297357},
   isbn = {9781450359337},
   journal = {Proceedings of the ACM Symposium on Applied Computing},
   keywords = {Information Quality,Machine Learning,Social Media,Wikipedia},
   pages = {804-807},
   publisher = {Association for Computing Machinery},
   title = {Automatically assessing the quality of Wikipedia contents},
   volume = {Part F147772},
   year = {2019},
}
@article{Wang2021,
   abstract = {Wikipedia is becoming increasingly critical in helping people obtain information and knowledge. Its leading advantage is that users can not only access information but also modify it. However, this presents a challenging issue: how can we measure the quality of a Wikipedia article? The existing approaches assess Wikipedia quality by statistical models or traditional machine learning algorithms. However, their performance is not satisfactory. Moreover, most existing models fail to extract complete information from articles, which degrades the model’s performance. In this article, we first survey related works and summarise a comprehensive feature framework. Then, state-of-the-art deep learning models are introduced and applied to assess Wikipedia quality. Finally, a comparison among deep learning models and traditional machine learning models is conducted to validate the effectiveness of the proposed model. The models are compared extensively in terms of their training and classification performance. Moreover, the importance of each feature and the importance of different feature sets are analysed separately.},
   author = {Ping Wang and Xiaodan Li and Renli Wu},
   doi = {10.1177/0165551519877646},
   issn = {17416485},
   issue = {2},
   journal = {Journal of Information Science},
   keywords = {Deep learning,Wikipedia,feature framework,information quality assessment},
   month = {4},
   pages = {176-191},
   publisher = {SAGE Publications Ltd},
   title = {A deep learning-based quality assessment model of collaboratively edited documents: A case study of Wikipedia},
   volume = {47},
   url = {https://www.researchgate.net/publication/336145185_A_deep_learning-based_quality_assessment_model_of_collaboratively_edited_documents_A_case_study_of_Wikipedia},
   year = {2021},
}
@article{Velichety2019,
   abstract = {This research provides a method for quality assessment of peer-produced content in knowledge repositories using a complementary view of collaboration. Using the definition of collaboration as the action of working with someone to produce something, we identify the aspects of collaboration that the present research on online communities does not consider. To this end, we introduce and define the concept of implicit collaboration and then identify two dimensions and four possible areas of collaboration. In each area, we identify the relevant social network that captures collaboration. Using customized measures on each of the networks that capture various aspects of collaboration, we quantify the utility of implicit collaboration in assessing article quality. Experiments conducted on the complete population of graded English language Wikipedia articles show that all the identified measures improve the predictive accuracy of the existing models by 11.89 percent while improving the class-wise precision by 9-18 percent and the class-wise recall by 5-26 percent. We also find that our method complements the existing quality assessment approaches well. Our research has implications for developing automated quality assessment methods for peer-produced content using big data and social networks.},
   author = {Srikar Velichety},
   doi = {10.1145/3371041.3371045},
   issn = {00950033},
   issue = {4},
   journal = {Data Base for Advances in Information Systems},
   keywords = {Discussions,Edits,Implicit Collaboration,Social Networks,Wikipedia},
   month = {11},
   pages = {28-51},
   publisher = {Association for Computing Machinery},
   title = {Quality assessment of peer-produced content in knowledge repositories using big data and social networks: The case of implicit collaboration in wikipedia},
   volume = {50},
   year = {2019},
}
@article{Wang2020,
   abstract = {Currently, web document repositories have been collaboratively created and edited. One of these repositories, Wikipedia, is facing an important problem: assessing the quality of Wikipedia. Existing approaches exploit techniques such as statistical models or machine leaning algorithms to assess Wikipedia article quality. However, existing models do not provide satisfactory results. Furthermore, these models fail to adopt a comprehensive feature framework. In this article, we conduct an extensive survey of previous studies and summarize a comprehensive feature framework, including text statistics, writing style, readability, article structure, network, and editing history. Selected state-of-the-art deep-learning models, including the convolutional neural network (CNN), deep neural network (DNN), long short-term memory (LSTMs) network, CNN-LSTMs, bidirectional LSTMs, and stacked LSTMs, are applied to assess the quality of Wikipedia. A detailed comparison of deep-learning models is conducted with regard to different aspects: classification performance and training performance. We include an importance analysis of different features and feature sets to determine which features or feature sets are most effective in distinguishing Wikipedia article quality. This extensive experiment validates the effectiveness of the proposed model.},
   author = {Ping Wang and Xiaodan Li},
   doi = {10.1002/ASI.24210},
   issn = {23301643},
   issue = {1},
   journal = {Journal of the Association for Information Science and Technology},
   month = {1},
   pages = {16-28},
   publisher = {John Wiley and Sons Inc.},
   title = {Assessing the quality of information on wikipedia: A deep-learning approach},
   volume = {71},
   url = {https://www.researchgate.net/publication/332294515_Assessing_the_quality_of_information_on_wikipedia_A_deep-learning_approach},
   year = {2020},
}
@article{Couto2021,
   abstract = {Wikipedia is an online, free, multi-language, and collaborative encyclopedia, currently one of the most significant information sources on the web the open nature of Wikipedia contributions raises concerns about the quality of its information. Previous studies have addressed this issue using manual evaluations and proposing generic measures for quality assessment. In this work, we focus on the quality of health-related content. For this purpose, we use general and health-specific features from Wikipedia articles to propose health-specific metrics. We evaluate these metrics using a set of Wikipedia articles previously assessed by WikiProject Medicine. We conclude that it is possible to combine generic and specific metrics to determine health-related content's information quality these metrics are computed automatically and can be used by curators to identify quality issues. Along with the explored features, these metrics can also be used in approaches that automatically classify the quality of Wikipedia health-related articles.},
   author = {Luís Couto and Carla Teixeira Lopes},
   doi = {10.1145/3442442.3452355},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Health-related Content,Information Quality,Wikipedia},
   month = {4},
   pages = {640-647},
   publisher = {Association for Computing Machinery, Inc},
   title = {Assessing the quality of health-related Wikipedia articles with generic and specific metrics},
   url = {https://doi.org/10.1145/3442442.3452355},
   year = {2021},
}
@article{Teblunthuis2021,
   abstract = {Organizing complex peer production projects and advancing scientific knowledge of open collaboration each depend on the ability to measure quality. Wikipedia community members and academic researchers have used article quality ratings for purposes like tracking knowledge gaps and studying how political polarization shapes collaboration. Even so, measuring quality presents many methodological challenges. The most widely used systems use quality assesements on discrete ordinal scales, but such labels can be inconvenient for statistics and machine learning. Prior work handles this by assuming that different levels of quality are "evenly spaced"from one another. This assumption runs counter to intuitions about degrees of effort needed to raise Wikipedia articles to different quality levels. I describe a technique extending the Wikimedia Foundations' ORES article quality model to address these limitations. My method uses weighted ordinal regression models to construct one-dimensional continuous measures of quality. While scores from my technique and from prior approaches are correlated, my approach improves accuracy for research datasets and provides evidence that the "evenly spaced"assumption is unfounded in practice on English Wikipedia. I conclude with recommendations for using quality scores in future research and include the full code, data, and models.},
   author = {Nathan Teblunthuis},
   doi = {10.1145/3479986.3479991},
   isbn = {9781450385008},
   keywords = {Wikipedia,datasets,machine learning,measurement,methods,online communities,peer production,quality,sociotechnical systems,statistics},
   month = {9},
   pages = {1-10},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Measuring Wikipedia Article Quality in One Dimension by Extending ORES with Ordinal Regression},
   url = {https://doi.org/10.1145/3479986.3479991},
   year = {2021},
}
@article{Hu2016,
   abstract = {This study attempts to investigate to what extent indicators of academic writing and cognitive thinking can help measure the writing quality of group collaborative writings on Wikis. Particularly, comparisons were made on Wiki content in different stages of the projects. Preliminary results from a multiple linear regression analysis reveal that linguistic indicators such as engagement markers and self-mention were significant predictors in earlier stages to the projects, whereas verbs indicating cognitive thinking in the evaluation level were significant in later project stages.},
   author = {Xiao Hu and Tzi Dong Jeremy Ng and Lu Tian and Chi Un Lei},
   doi = {10.1145/2883851.2883963},
   isbn = {9781450341905},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Automated assessment,Metadiscourse,Wiki},
   month = {4},
   pages = {518-519},
   publisher = {Association for Computing Machinery},
   title = {Automating assessment of collaborative writing quality in multiple stages: The case of wiki},
   volume = {25-29-April-2016},
   url = {http://dx.doi.org/10.1145/2883851.2883963},
   year = {2016},
}
@article{Dang2016,
   abstract = {As Wikipedia became the largest human knowledge repository, quality measurement of its articles received a lot of attention during the last decade. Most research efforts focused on classification of Wikipedia articles quality by using a different feature set. However, so far, no 'golden feature set' was proposed. In this paper, we present a novel approach for classifying Wikipedia articles by analysing their content rather than by considering a feature set. Our approach uses recent techniques in natural language processing and deep learning, and achieved a comparable result with the state-of-the-art.},
   author = {Quang Vinh Dang and Claudia Lavinia Ignat},
   doi = {10.1145/2910896.2910917},
   isbn = {9781450342292},
   issn = {15525996},
   journal = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries},
   keywords = {Wikipedia,deep learning,document representation,feature engineering,quality assessment},
   month = {9},
   pages = {27-30},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Quality assessment of Wikipedia articles without feature engineering},
   volume = {2016-September},
   url = {https://dl.acm.org/doi/10.1145/2910896.2910917},
   year = {2016},
}
@article{Lex2012,
   abstract = {Nowadays, many decisions are based on information found in the Web. For the most part, the disseminating sources are not certified, and hence an assessment of the quality and credibility of Web content became more important than ever. With factual density we present a simple statistical quality measure that is based on facts extracted from Web content using Open Information Extraction. In a first case study, we use this measure to identify featured/good articles in Wikipedia. We compare the factual density measure with word count, a measure that has successfully been applied to this task in the past. Our evaluation corroborates the good performance of word count in Wikipedia since featured/good articles are often longer than non-featured. However, for articles of similar lengths the word count measure fails while factual density can separate between them with an F-measure of 90.4%. We also investigate the use of relational features for categorizing Wikipedia articles into featured/good versus non-featured ones. If articles have similar lengths, we achieve an F-measure of 86.7% and 84% otherwise. © 2012 ACM.},
   author = {Elisabeth Lex and Michael Voelske and Marcelo Errecalde and Edgardo Ferretti and Leticia Cagnina and Christopher Horn and Benno Stein and Michael Granitzer},
   doi = {10.1145/2184305.2184308},
   isbn = {9781450312370},
   journal = {ACM International Conference Proceeding Series},
   pages = {7-10},
   title = {Measuring the quality of web content using factual information},
   url = {https://dl.acm.org/doi/10.1145/2184305.2184308},
   year = {2012},
}
@article{Biancani2014,
   abstract = {Wikipedia is unique among reference works both in its scale and in the openness of its editing interface. The question of how it can achieve and maintain high-quality encyclopedic articles is an area of active research. In order to address this question, researchers need to build consensus around a sensible metric to assess the quality of contributions to articles. This measure must not only reflect an intuitive concept of "quality, " but must also be scalable and run efficiently. Building on prior work in this area, this paper uses human raters through Amazon Mechanical Turk to validate an efficient, automated quality metric.},
   author = {Susan Biancani},
   doi = {10.1145/2641580.2641621},
   isbn = {9781450330169},
   journal = {Proceedings of the 10th International Symposium on Open Collaboration, OpenSym 2014},
   keywords = {Collaboration,Experience,Ownership,Peer,Peer Review,Quality,WikiWork,Wikipedia,web-based interaction H12 [Models and Principles]: User/Machine Systems General Terms Measurement},
   pages = {G4},
   publisher = {Association for Computing Machinery, Inc},
   title = {Measuring the quality of edits to Wikipedia},
   url = {http://dx.doi.org/10.1145/2641580.2641621},
   year = {2014},
}
@article{Hu2007,
   abstract = {Wikipedia has grown to be the world largest and busiest free encyclopedia, in which articles are collaboratively written and maintained by volunteers online. Despite its success as a means of knowledge sharing and collaboration, the public has never stopped criticizing the quality of Wikipedia articles edited by non-experts and inexperienced contributors. In this paper, we investigate the problem of assessing the quality of articles in collaborative authoring of Wikipedia. We propose three article quality measurement models that make use of the interaction data between articles and their contributors derived from the article edit history. Our basic model is designed based on the mutual dependency between article quality and their author authority. The PeerReview model introduces the review behavior into measuring article quality. Finally, our ProbReview models extend PeerReview with partial reviewership of contributors as they edit various portions of the articles. We conduct experiments on a set of well-labeled Wikipedia articles to evaluate the effectiveness of our quality measurement models in resembling human judgement. Copyright 2007 ACM.},
   author = {Meiqun Hu and Ee Peng Lim and Aixin Sun and Hady W. Lauw and Ba Quy Vuong},
   doi = {10.1145/1321440.1321476},
   isbn = {9781595938039},
   journal = {International Conference on Information and Knowledge Management, Proceedings},
   keywords = {Article quality,Authority,Collaborative authoring,Peer review,Wikipedia,quality assessment,transient contribution,web-based interaction; K43 [Computers and Society]: Organizational Impacts-Computer-supported collaborative work General Terms Measurement},
   pages = {243-252},
   title = {Measuring article quality in wikipedia: Models and evaluation},
   url = {https://dl.acm.org/doi/10.1145/1321440.1321476},
   year = {2007},
}
@article{Antunes2020,
   abstract = {Looking for health information is one of the most popular activities online. However, the specificity of language on this domain is frequently an obstacle to comprehension, especially for the ones with lower levels of health literacy. For this reason, search engines should consider the readability of health content and, if possible, adapt it to the user behind the search. In this work, we explore methods to assess the readability of health content automatically. We propose features capable of measuring the specificity of a medical text and estimate the knowledge necessary to comprehend it. The features are based on information retrieval metrics and the log-likelihood of a text with lay and medico-scientific language models. To evaluate our methods, we built and used a dataset composed of health articles of Simple English Wikipedia and the respective documents in ordinary Wikipedia. We achieved a maximum accuracy of 88% in binary classifications (easy versus hard-to-read). We found out that the machine learning algorithm does not significantly interfere with performance. We also experimented and compared different features combinations. The features using the values of the log-likelihood of a text with lay and medico-scientific language models perform better than all the others.},
   author = {Hélder Antunes and Carla Teixeira Lopes},
   doi = {10.1145/3397271.3401187},
   isbn = {9781450380164},
   journal = {SIGIR 2020 - Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {consumer health search,machine learning,natural language processing,readability},
   month = {7},
   pages = {1973-1976},
   publisher = {Association for Computing Machinery, Inc},
   title = {Proposal and Comparison of Health Specific Features for the Automatic Assessment of Readability},
   url = {https://doi.org/10.1145/3397271.3401187},
   year = {2020},
}
@article{Banerjee2011,
   abstract = {This paper proposes a novel bio-inspired model that quantifies the quality aspect of Wiki content. Unlike the statistical measures, the proposed system automates the quality dispersion mechanism using ant colony's pheromone artifacts over Wiki content. The inclusion of artificial ant agents is relevant to the heuristic behavior of Wiki content management and reputation paradigm of Wiki. The proposed model generates substantial empirical and graphical evidences, which could be timely boosted to enhance Wiki culture and editing process of Wiki in order to be theoretically trusted and validated across the users. Observed results present evidences that despite of users' attribution, registered or anonymous, the proposed agent based model provides quantifying editing in content of Wiki and accordingly both commercial viability, in terms of quality, and vandalism can be ensured. Copyright © 2011 ACM.},
   author = {Soumya Banerjee and Nashwa El-Bendary and Hameed Al-Qaheri},
   doi = {10.1145/2077489.2077545},
   isbn = {9781450310475},
   journal = {Proceedings of the International Conference on Management of Emergent Digital EcoSystems, MEDES'11},
   keywords = {Ant colony optimization,Bio-inspired,Content management,Quality,Quality measurement,Reputation paradigm,Wiki,WikiWork,Wikipedia,web-based interaction H12 [Models and Principles]: User/Machine Systems General Terms Measurement},
   pages = {305-312},
   title = {Exploring wiki: Measuring the quality of social media using ant colony metaphor},
   url = {http://dx.doi.org/10.1145/2641580.2641621},
   year = {2011},
}
@article{Blumenstock2008,
   abstract = {Wikipedia, "the free encyclopedia", now contains over two million English articles, and is widely regarded as a high-quality, authoritative encyclopedia. Some Wikipedia articles, however, are of questionable quality, and it is not always apparent to the visitor which articles are good and which are bad. We propose a simple metric - word count - for measuring article quality. In spite of its striking simplicity, we show that this metric significantly outperforms the more complex methods described in related work.},
   author = {Joshua E. Blumenstock},
   doi = {10.1145/1367497.1367673},
   isbn = {9781605580852},
   journal = {Proceeding of the 17th International Conference on World Wide Web 2008, WWW'08},
   keywords = {Information quality,Wikipedia,Word count,word count},
   pages = {1095-1096},
   title = {Size matters: Word count as a measure of quality on Wikipedia},
   url = {https://www.researchgate.net/publication/221023915_Size_matters_Word_count_as_a_measure_of_quality_on_Wikipedia},
   year = {2008},
}
@article{Wilkinson2007,
   abstract = {The rise of the Internet has enabled collaboration and cooperation on anunprecedentedly large scale. The online encyclopedia Wikipedia, which presently comprises 7.2 million articles created by 7.04 million distinct editors, provides a consummate example. We examined all 50 million edits made to the 1.5 million English-language Wikipedia articles and found that the high-quality articles are distinguished by a marked increase in number of edits, number of editors, and intensity of cooperative behavior, as compared to other articles of similar visibility and age. This is significant because in other domains, fruitful cooperation has proven to be difficult to sustain as the size of the collaboration increases. Furthermore, in spite of the vagaries of human behavior, we show that Wikipedia articles accrete edits according to a simple stochastic mechanism in which edits beget edits. Topics of high interest or relevance are thus naturally brought to the forefront of quality. Copyright © 2007 ACM.},
   author = {Dennis M. Wilkinson and Bernardo A. Huberman},
   doi = {10.1145/1296951.1296968},
   isbn = {9781595938619},
   journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
   keywords = {Cooperation,Web-based interaction; K43 [Comput-ers and Society]: Organizational Impacts-Computer-supported collaborative work General Terms Collaborative authoring,Wikipedia,groupware Keywords Cooperation},
   pages = {157-164},
   title = {Cooperation and quality in Wikipedia},
   url = {https://www.researchgate.net/publication/200772402_Cooperation_and_quality_in_Wikipedia},
   year = {2007},
}
@article{Halfaker2009,
   abstract = {Wikipedia is a highly successful example of what mass collaboration in an informal peer review system can accomplish. In this paper, we examine the role that the quality of the contributions, the experience of the contributors and the ownership of the content play in the decisions over which contributions become part of Wikipedia and which ones are rejected by the community. We introduce and justify a versatile metric for automatically measuring the quality of a contribution. We find little evidence that experience helps contributors avoid rejection. In fact, as they gain experience , contributors are even more likely to have their work rejected. We also find strong evidence of ownership behaviors in practice despite the fact that ownership of content is discouraged within Wikipedia.},
   author = {A Halfaker and A Kittur and R Kraut and J Riedl - Proceedings of the 5th and undefined 2009},
   isbn = {9781605587301},
   journal = {dl.acm.org},
   keywords = {Experience,H12 [Models and Principles]: User/Machine Systems Keywords Wikipedia,Own-ership,Peer,Peer Review,Quality,WikiWork},
   title = {A jury of your peers: quality, experience and ownership in Wikipedia},
   url = {https://dl.acm.org/doi/abs/10.1145/1641309.1641332},
   year = {2009},
}
@article{Stein2007,
   abstract = {The considerable high quality of Wikipedia articles is often accredited to the large number of users who contribute to Wikipedia's encyclopedia articles, who watch articles and correct errors immediately. In this paper, we are in particular interested in a certain type of Wikipedia articles, namely, the featured articles - articles marked by a community's vote as being of outstanding quality. The German Wikipedia has the nice property that it has two types of featured articles: excellent and worth reading. We explore on the German Wikipedia whether only the mere number of contributors makes the difference or whether the high quality of featured articles results from having experienced authors contributing with a reputation for high quality contributions. Our results indicate that it does matter who contributes. Copyright 2007 ACM.},
   author = {Klaus Stein and Claudia Hess},
   doi = {10.1145/1286240.1286290},
   isbn = {1595938206},
   journal = {Hypertext 2007: Proceedings of the Eighteenth ACM Conference on Hypertext and Hypermedia, HT'07},
   keywords = {Collaborative working,Measures of quality and reputation,Statistical analysis of Wikipedia,Wiki,Wikipedia},
   pages = {171-174},
   title = {Does it matter who contributes - A study on featured articles in the german wikipedia},
   url = {https://dl.acm.org/doi/10.1145/1286240.1286290},
   year = {2007},
}
@article{,
   abstract = {The main feature of the free online-encyclopedia Wikipedia is the wiki-tool, which allows viewers to edit the articles directly in the web browser. As a weakness of this openness for example the possibility of manipulation and vandalism cannot be ruled out, so that the quality of any given Wikipedia article is not guaranteed. Hence the automatic quality assessment has been becoming a high active research field. In this paper we offer new metrics for an efficient quality measurement. The metrics are based on the lifecycles of low and high quality articles, which refer to the changes of the persistent and transient contributions throughout the entire life span. Copyright © 2009 ACM.},
   author = {Thomas Wöhner and Ralf Peters},
   doi = {10.1145/1641309.1641333},
   isbn = {9781605587301},
   journal = {Proceedings of the 5th International Symposium on Wikis and Open Collaboration, WiKiSym 2009},
   keywords = {Persistent contribution,Quality assessment,Transient contribution,Wikipedia,Wikipedia lifecycle,quality assessment,transient contribution,web-based interaction; K43 [Computers and Society]: Organizational Impacts-Computer-supported collaborative work General Terms Measurement},
   title = {Assessing the quality of Wikipedia articles with lifecycle based metrics},
   url = {https://dl.acm.org/doi/10.1145/1641309.1641333},
   year = {2009},
}
@article{Adler2008,
   abstract = {We consider the problem of measuring user contributions to ver-sioned, collaborative bodies of information, such as wikis. Measuring the contributions of individual authors can be used to divide revenue, to recognize merit, to award status promotions, and to choose the order of authors when citing the content. In the context of the Wikipedia, previous works on author contribution estimation have focused on two criteria: the total text created, and the total number of edits performed. We show that neither of these criteria work well: both techniques are vulnerable to manipulation, and the total-text criterion fails to reward people who polish or re-arrange the content. We consider and compare various alternative criteria that take into account the quality of a contribution, in addition to the quantity, and we analyze how the criteria differ in the way they rank authors according to their contributions. As an outcome of this study, we propose to adopt total edit longevity as a measure of author contribution. Edit longevity is resistant to simple attacks, since edits are counted towards an author's contribution only if other authors accept the contribution. Edit longevity equally rewards people who create content, and people who rearrange or polish the content. Finally, edit longevity distinguishes the people who contribute little (who have contribution close to zero) from spammers or vandals, whose contribution quickly grows negative. © 2008 ACM.},
   author = {B. Thomas Adler and Luca De Alfaro and Ian Pye and Vishwanath Raman},
   doi = {10.1145/1822258.1822279},
   journal = {WikiSym 2008 - The 4th International Symposium on Wikis, Proceedings},
   keywords = {H53 [Information Interfaces and Presentation]: Group and Organization Interfaces-Computer-supported cooperative work, Web-based interaction,J4 [Social and Behavioral Sciences]: Miscellaneous *,K43 [Computers and Society]: Orga-nizational Impacts-Computer-supported collaborative work},
   title = {Measuring author contributions to the Wikipedia},
   year = {2008},
}
@article{Lipka2010,
   abstract = {Wikipedia provides an information quality assessment model with criteria for human peer reviewers to identify featured articles. For this classification task "Is an article featured or not?" we present a machine learning approach that exploits an article's character trigram distribution. Our approach differs from existing research in that it aims to writing style rather than evaluating meta features like the edit history. The approach is robust, straightforward to implement, and outperforms existing solutions. We underpin these claims by an experiment design where, among others, the domain transferability is analyzed. The achieved performances in terms of the F-measure for featured articles are 0.964 within a single Wikipedia domain and 0.880 in a domain transfer situation. © 2010 Copyright is held by the author/owner(s).},
   author = {Nedim Lipka and Benno Stein},
   doi = {10.1145/1772690.1772847},
   isbn = {9781605587998},
   journal = {Proceedings of the 19th International Conference on World Wide Web, WWW '10},
   keywords = {Information Quality,domain transfer,information quality,wikipedia},
   pages = {1147-1148},
   title = {Identifying featured articles in Wikipedia: Writing style matters},
   url = {https://dl.acm.org/doi/10.1145/1772690.1772847},
   year = {2010},
}
@article{Druck2010,
   abstract = {Although some have argued that Wikipedia’s open edit policy is one of the primary reasons for its success, it also raises
concerns about quality — vandalism, bias, and errors can be
problems. Despite these challenges, Wikipedia articles are
often (perhaps surprisingly) of high quality, which many attribute to both the dedicated Wikipedia community and “good
Samaritan” users. As Wikipedia continues to grow, however,
it becomes more difficult for these users to keep up with the
increasing number of articles and edits. This motivates the
development of tools to assist users in creating and maintaining quality. In this paper, we propose metrics that quantify the
quality of contributions to Wikipedia through implicit feedback from the community. We then learn discriminative probabilistic models that predict the quality of a new edit using
features of the changes made, the author of the edit, and the
article being edited. Through estimating parameters for these
models, we also gain an understanding of factors that influence quality. We advocate using edit quality predictions and
information gleaned from model analysis not to place restrictions on editing, but to instead alert users to potential quality
problems, and to facilitate the development of additional incentives for contributors. We evaluate the edit quality prediction models on the Spanish Wikipedia. Experiments demonstrate that the models perform better when given access to
content-based features of the edit, rather than only features of
contributing user. This suggests that a user-based solution to
the Wikipedia quality problem may not be sufficient},
   author = {G Druck and G Miklau},
   isbn = {9781605589404},
   journal = {aaai.org},
   keywords = {H53 [Information Systems]: INFORMATION INTER-FACES AND PRESENTATION-Group and Organization Interfaces General Terms Experimentation,Human Factors,Measurement Keywords Wikipedia,models,quality},
   title = {Learning to predict the quality of contributions to wikipedia},
   url = {https://www.aaai.org/Papers/Workshops/2008/WS-08-15/WS08-15-002.pdf},
   year = {2010},
}
@article{Lucassen2010,
   abstract = {The use of Wikipedia as an information source is becoming increasingly popular. Several studies have shown that its information quality is high. Normally, when considering information trust, the source of information is an important factor. However, because of the open-source nature of Wikipedia articles, their sources remain mostly unknown. This means that other features need to be used to assess the trustworthiness of the articles. We describe article features - such as images and references - which lay Wikipedia readers use to estimate trustworthiness. The quality and the topics of the articles are manipulated in an experiment to reproduce the varying quality on Wikipedia and the familiarity of the readers with the topics. We show that the three most important features are textual features, references and images. © 2010 ACM.},
   author = {Teun Lucassen and Jan Maarten Schraagen},
   doi = {10.1145/1772938.1772944},
   isbn = {9781605589404},
   journal = {Proceedings of the 4th Workshop on Information Credibility, WICOW '10},
   keywords = {models,quality,think-aloud,trustworthiness,wikipedia},
   pages = {19-26},
   title = {Trust in wikipedia: How users trust information from an unknown source},
   url = {https://www.researchgate.net/publication/221021979_Trust_in_wikipedia_how_users_trust_information_from_an_unknown_source},
   year = {2010},
}
@article{Adler2008,
   abstract = {The Wikipedia is a collaborative encyclopedia: anyone can contribute to its articles simply by clicking on an "edit" button. The open nature of the Wikipedia has been key to its success, but has also created a challenge: how can readers develop an informed opinion on its reliability? We propose a system that computes quantitative values of trust for the text in Wikipedia articles; these trust values provide an indication of text reliability. The system uses as input the revision history of each article, as well as information about the reputation of the contributing authors, as provided by a reputation system. The trust of a word in an article is computed on the basis of the reputation of the original author of the word, as well as the reputation of all authors who edited text near the word. The algorithm computes word trust values that vary smoothly across the text; the trust values can be visualized using varying text-background colors. The algorithm ensures that all changes to an article's text are reflected in the trust values, preventing surreptitious content changes. We have implemented the proposed system, and we have used it to compute and display the trust of the text of thousands of articles of the English Wikipedia. To validate our trust-computation algorithms, we show that text labeled as low-trust has a significantly higher probability of being edited in the future than text labeled as high-trust. © 2008 ACM.},
   author = {B. Thomas Adler and Krishnendu Chatterjee and Luca De Alfaro and Marco Faella and Ian Pye and Vishwanath Raman},
   doi = {10.1145/1822258.1822293},
   journal = {WikiSym 2008 - The 4th International Symposium on Wikis, Proceedings},
   keywords = {K43 [Computers and Society],Organi-zational Impacts,Web-based interaction},
   title = {Assigning trust to Wikipedia content},
   url = {https://www.researchgate.net/publication/200773226_Assigning_Trust_to_Wikipedia_Content},
   year = {2008},
}
@article{,
   abstract = {This paper discusses an approach to modeling and measuring information quality of Wikipedia articles. The approach is based on the idea that the quality of Wikipedia articles with distinctly different profiles needs to be measured using different information quality models. We report on our initial study, which involved two categories of Wikipedia articles: " stabilized" (those, whose content has not undergone major changes for a significant period of time) and "controversial" (the articles, which have undergone vandalism, revert wars, or whose content is subject to internal discussions between Wikipedia editors). We present simple information quality models and compare their performance on a subset of Wikipedia articles with the information quality evaluations provided by human users. Our experiment shows, that using special-purpose models for information quality captures user sentiment about Wikipedia articles better than using a single model for both categories of articles. © 2010 ACM.},
   author = {Gabriel De La Calzada and Alex Dekhtyar},
   doi = {10.1145/1772938.1772943},
   isbn = {9781605589404},
   journal = {Proceedings of the 4th Workshop on Information Credibility, WICOW '10},
   keywords = {models,quality,wikipedia},
   pages = {11-18},
   title = {On measuring the quality of wikipedia articles},
   url = {https://dl.acm.org/doi/10.1145/1772938.1772943},
   year = {2010},
}
@article{Adler2010,
   abstract = {WikiTrust is a reputation system for Wikipedia authors and content. WikiTrust computes three main quantities: edit quality, author reputation, and content reputation. The edit quality measures how well each edit, that is, each change introduced in a revision, is preserved in subsequent revisions. Authors who perform good quality edits gain reputation, and text which is revised by sev- eral high-reputation authors gains reputation. Since vandalism on the Wikipedia is usually performed by anonymous or new users (not least because long-time vandals end up banned), and is usually reverted in a reasonably short span of time, edit quality, author reputation, and content reputation are obvious candi- dates as features to identify vandalism on the Wikipedia. Indeed, using the full set of features computed by WikiTrust, we have been able to construct classifiers that identify vandalism with a recall of 83.5%, a precision of 48.5%, and a false positive rate of 8%, for an area under the ROC curve of 93.4%. If we limit our- selves to the set of features available at the time an edit is made (when the edit quality is still unknown), the classifier achieves a recall of 77.1%, a precision of 36.9%, and a false positive rate of 12.2%, for an area under the ROC curve of 90.4%. Using these classifiers, we have implemented a simple Web API that provides the vandalism estimate for every revision of the English Wikipedia. The API can be used both to identify vandalism that needs to be reverted, and to select high- quality, non-vandalized recent revisions of any given Wikipedia article. These recent high-quality revisions can be included in static snapshots of the Wikipedia, or they can be used whenever tolerance to vandalism is low (as in a school setting, or whenever the material is widely disseminated).},
   author = {B Adler and L De Alfaro},
   journal = {uni-weimar.de},
   title = {Detecting wikipedia vandalism using wikitrust},
   url = {https://www.researchgate.net/publication/221159945_Detecting_wikipedia_vandalism_using_WikiTrust_Lab_report_for_PAN_at_CLEF_2010},
   year = {2010},
}
@article{Kittur2008,
   abstract = {Wikipedia has become one of the most important information resources on the Web by promoting peer collaboration and enabling virtually anyone to edit anything. However, this mutability also leads many to distrust it as a reliable source of information. Although there have been many attempts at developing metrics to help users judge the trustworthiness of content, it is unknown how much impact such measures can have on a system that is perceived as inherently unstable. Here we examine whether a visualization that exposes hidden article information can impact readers' perceptions of trustworthiness in a wiki environment. Our results suggest that surfacing information relevant to the stability of the article and the patterns of editor behavior can have a significant impact on users' trust across a variety of page types.},
   author = {A Kittur and B Suh},
   isbn = {9781605580074},
   journal = {dl.acm.org},
   keywords = {Author Keywords Wikipedia,Computer-supported cooperative work,H35 [Information Storage and Retrieval]: Online Information Systems,K43 [Computers and Society]: Organizational Impacts-Computer-supported collaborative work,Web-based interaction,collaboration,social computing ACM Classification Keywords H53 [Information Interfaces]: Group and Organization Interfaces-Collaborative computing,stability,trust,visualization,wiki},
   title = {Can you ever trust a Wiki? Impacting perceived trustworthiness in Wikipedia},
   url = {https://dl.acm.org/doi/abs/10.1145/1460563.1460639},
   year = {2008},
}
@article{Khairova2017,
   abstract = {We present the method of estimating the quality of articles in Rus-sian Wikipedia that is based on counting the number of facts in the article. For calculating the number of facts we use our logical-linguistic model of fact extraction. Basic mathematical means of the model are logical-algebraic equations of the finite predicates algebra. The model allows extracting of simple and complex types of facts in Russian sentences. We experimentally compare the effect of the density of these types of facts on the quality of articles in Russian Wikipedia. Better articles tend to have a higher density of facts.},
   author = {N Khairova and W Lewoniewski},
   doi = {10.1007/978-3-319-59336-4_3},
   journal = {Springer},
   keywords = {Russian Wikipedia,article quality,fact extraction,logical equations},
   pages = {28-40},
   publisher = {Springer Verlag},
   title = {Estimating the quality of articles in Russian wikipedia using the logical-linguistic model of fact extraction},
   volume = {288},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-59336-4_3},
   year = {2017},
}
@article{Arazy2011,
   abstract = {The notion of information quality (IQ) has been investigated extensively in recent years. Much of this research has been aimed at conceptualizing IQ and its underlying dimensions (e.g., accuracy, completeness) and at developing instruments for measuring these quality dimensions. However, less attention has been given to the measurability of IQ. The objective of this study is to explore the extent to which a set of IQ dimensions (accuracy, completeness, objectivity, and representation) lend themselves to reliable measurement. By reliable measurement, we refer to the degree to which independent assessors are able to agree when rating objects on these various dimensions. Our study reveals that multiple assessors tend to agree more on certain dimensions (e.g., accuracy) while finding it more difficult to agree on others (e.g., completeness). We argue that differences in measurability stem from properties inherent to the quality dimension (i.e., the availability of heuristics that make the assessment more tangible) as well as on assessors' reliance on these cues. Implications for theory and practice are discussed. © 2010 ASIS&T.},
   author = {Ofer Arazy and Rick Kopak},
   doi = {10.1002/ASI.21447},
   issn = {15322882},
   issue = {1},
   journal = {Journal of the American Society for Information Science and Technology},
   month = {1},
   pages = {89-99},
   title = {On the measurability of information quality},
   volume = {62},
   year = {2011},
}
@article{Stvilia2005,
   abstract = {Effective information quality analysis needs powerful yet easy ways to obtain metrics. The English version of Wikipedia provides an extremely interesting yet challenging case for the study of Information Quality dynamics at both macro and micro levels. We propose seven IQ metrics which can be evaluated automatically and test the set on a representative sample of Wikipedia content. The methodology of the metrics construction and the results of tests, along with a number of statistical characterizations of Wikipedia articles, their content construction, process metadata and social context are reported.},
   author = {B Stvilia and MB Twidale and L Gasser Culture},
   journal = {World Scientific},
   title = {Information quality in a community-based encyclopedia},
   url = {https://www.worldscientific.com/doi/abs/10.1142/9789812701527_0009},
   year = {2005},
}
@article{Dondio2007,
   abstract = {The problem of identifying useful and trustworthy information on the World Wide Web is becoming increasingly acute as new tools such as wikis and blogs simplify and democratize publication. It is not hard to predict that in the future the direct reliance on this material will expand and the problem of evaluating the trustworthiness of this kind of content become crucial. The Wikipedia project represents the most successful and discussed example of such online resources. In this paper we present a method to predict Wikipedia articles trustworthiness based on computational trust techniques and a deep domain-specific analysis. Our assumption is that a deeper understanding of what in general defines high-standard and expertise in domains related to Wikipedia-i.e. content quality in a collaborative environment-mapped onto Wikipedia elements would lead to a complete set of mechanisms to sustain trust in Wikipedia context. We present a series of experiment. The first is a study-case over a specific category of articles; the second is an evaluation over 8 000 articles representing 65% of the overall Wikipedia editing activity. We report encouraging results on the automated evaluation of Wikipedia content using our domain-specific expertise method. Finally, in order to appraise the value added by using domain-specific expertise, we compare our results with the ones obtained with a pre-processed cluster analysis, where complex expertise is mostly replaced by training and automatic classification of common features. Povzetek: Ocenjena je stopnja zaupanja v strani v Wikipediji.},
   author = {P Dondio and S Barret},
   journal = {informatica.si},
   keywords = {Wikipedia,computational trust,content-quality},
   title = {Computational trust in Web content quality: a comparative evalutation on the Wikipedia project},
   url = {http://informatica.si/index.php/informatica/article/viewFile/137/129},
   year = {2007},
}
@article{Rad2012,
   abstract = {Wikipedia articles are the result of the collaborative editing of a diverse group of anonymous volunteer editors, who are passionate and knowledgeable about specific topics. One can argue that this plurality of perspectives leads to broader coverage of the topic, thus benefitting the reader. On the other hand, differences among editors on polarizing topics can lead to controversial or questionable content, where facts and arguments are presented and discussed to support a particular point of view. Controversial articles are manually tagged by Wikipedia editors, and span many interesting and popular topics, such as religion, history, and politics, to name a few. Recent works have been proposed on automatically identifying controversy within unmarked articles. However, to date, no systematic comparison of these efforts has been made. This is in part because the various methods are evaluated using different criteria and on different sets of articles by different authors, making it hard for anyone to verify the efficacy and compare all alternatives. We provide a first attempt at bridging this gap. We compare five different methods for modelling and identifying controversy, and discuss some of the unique difficulties and opportunities inherent to the way Wikipedia is produced. © 2012 ACM.},
   author = {Hoda Sepehri Rad and Denilson Barbosa},
   doi = {10.1145/2462932.2462942},
   isbn = {9781450316057},
   journal = {WikiSym 2012 Conference Proceedings - 8th Annual International Symposium on Wikis and Open Collaboration},
   keywords = {Mono-tonicity,Wikipedia,comparison,controversy,disagreement,monotonicity},
   title = {Identifying controversial articles in Wikipedia: A comparative study},
   url = {https://dl.acm.org/doi/10.1145/2462932.2462942},
   year = {2012},
}
@article{Stvilia2007,
   abstract = {One cannot manage information quality (IQ) without first being able to measure it meaningfully and establishing a causal connection between the source of IQ change, the IQ problem types, the types of activities affected, and their implications. In this article we propose a general IQ assessment framework. In contrast to context-specific IQ assessment models, which usually focus on a few variables determined by local needs, our framework consists of comprehensive typologies of IQ problems, related activities, and a taxonomy of IQ dimensions organized in a systematic way based on sound theories and practices. The framework can be used as a knowledge resource and as a guide for developing IQ measurement models for many different settings. The framework was validated and refined by developing specific IQ measurement models for two large-scale collections of two large classes of information objects: Simple Dublin Core records and online encyclopedia articles.},
   author = {Besiki Stvilia and Les Gasser and Michael B. Twidale and Linda C. Smith},
   doi = {10.1002/ASI.20652},
   issn = {15322882},
   issue = {12},
   journal = {Journal of the American Society for Information Science and Technology},
   month = {10},
   pages = {1720-1733},
   title = {A framework for information quality assessment},
   volume = {58},
   year = {2007},
}
