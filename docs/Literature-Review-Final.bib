@article{Dang2017,
   abstract = {Wikipedia is considered as the largest knowledge repository in the history of humanity and plays a crucial role in modern daily life. Assigning the correct quality class to Wikipedia articles is an important task in order to provide guidance for both authors and readers of Wikipedia. The manual review cannot cope with the editing speed of Wikipedia. An automatic classification is required to classify the quality of Wikipedia articles. Most existing approaches rely on traditional machine learning with manual feature engineering, which requires a lot of expertise and effort. Furthermore, it is known that there is no general perfect feature set because information leak always occurs in feature extraction phase. Also, for each language of Wikipedia, a new feature set is required. In this paper, we present an approach relying on deep learning for quality classification of Wikipedia articles. Our solution relies on Recurrent Neural Networks (RNN) which is an endto-end learning technique that eliminates disadvantages of feature engineering. Our approach learns directly from raw data without human intervention and is language-neutral. Experimental results on English, French and Russian Wikipedia datasets show that our approach outperforms state-of-the-art solutions.},
   author = {Quang Vinh Dang and Claudia Lavinia Ignat},
   doi = {10.1145/3125433.3125448},
   isbn = {9781450351874},
   journal = {Proceedings of the 13th International Symposium on Open Collaboration, OpenSym 2017},
   keywords = {Deep learning,Document quality,End-to-end learning,Long-Short Term Memory (LSTM),Recurrent Neural Network (RNN),Wikipedia},
   month = {8},
   publisher = {Association for Computing Machinery, Inc},
   title = {An end-to-end learning solution for assessing the quality of Wikipedia articles},
   url = {https://doi.org/10.1145/3125433.3125448},
   year = {2017},
}
@article{Dalip2009,
   abstract = {The old dream of a universal repository containing all the human knowledge and culture is becoming possible through the Internet and the Web. Moreover, this is happening with the direct collaborative, participation of people. Wikipedia is a great example. It is an enormous repository of information with free access and edition, created by the community in a collaborative manner. However, this large amount of information, made available democratically and virtually without any control, raises questions about its relative quality. In this work we explore a significant number of quality indicators, some of them proposed by us and used here for the first time, and study their capability to assess the quality of Wikipedia articles. Furthermore, we explore machine learning techniques to combine these quality indicators into one single assessment judgment. Through experiments, we show that the most important quality indicators are the easiest ones to extract, namely, textual features related to length, structure and style. We were also able to determine which indicators did not contribute significantly to the quality assessment. These were, coincidentally, the most complex features, such as those based on link analysis. Finally, we compare our combination method with state-of-the-art solution and show significant improvements in terms of effective quality prediction. Copyright 2009 ACM.},
   author = {Daniel Hasan Dalip and Marcos André Gonçalves and Marco Cristo and Pável Calado},
   doi = {10.1145/1555400.1555449},
   isbn = {9781605586977},
   issn = {15525996},
   journal = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries},
   keywords = {Machine learning,Quality assessment,SVM,User Issues] General Terms Human Factors,Wikipedia},
   pages = {295-304},
   title = {Automatic quality assessment of content created collaboratively by web communities: A case study of wikipedia},
   url = {http://en.wikipedia.org/wiki/Wikipedia},
   year = {2009},
}
@article{Chevalier2010,
   abstract = {As Wikipedia has become one of the most used knowledge bases worldwide, the problem of the trustworthiness of the information it disseminates becomes central. With WikipediaViz, we introduce five visual indicators integrated to the Wikipedia layout that can keep casual Wikipedia readers aware of important meta-information about the articles they read. The design of WikipediaViz was inspired by two participatory design sessions with expert Wikipedia writers and sociologists who explained the clues they used to quickly assess the trustworthiness of articles. According to these results, we propose five metrics for Maturity and Quality assessment ofWikipedia articles and their accompanying visualizations to provide the readers with important clues about the editing process at a glance. We also report and discuss about the results of the user studies we conducted. Two preliminary pilot studies show that all our subjects trust Wikipedia articles almost blindly. With the third study, we show that WikipediaViz significantly reduces the time required to assess the quality of articles while maintaining a good accuracy.},
   author = {Fanny Chevalier and Stéphane Huot and Jean Daniel Fekete},
   doi = {10.1109/PACIFICVIS.2010.5429611},
   isbn = {9781424466849},
   journal = {IEEE Pacific Visualization Symposium 2010, PacificVis 2010 - Proceedings},
   keywords = {Collaborative knowledge,Encyclopedia,Information visualization,Participatory design,Wikipedia},
   pages = {49-56},
   title = {WikipediaViz: Conveying article quality for casual wikipedia readers},
   url = {https://www.researchgate.net/publication/221536224_WikipediaViz_Conveying_Article_Quality_for_Casual_Wikipedia_Readers},
   year = {2010},
}
@article{Saengthongpattana2014,
   abstract = {The quality evaluation of Thai Wikipedia articles relies on user consideration. There are increasing numbers of articles every day therefore the automatic evaluation method is needed for user. Components of Wikipedia articles such as headers, pictures, references, and links are useful to indicate the quality of articles. However readers need complete content to cover all of concepts in that article. The concept features are investigated in this work. The aim of this research is to classify Thai Wikipedia articles into two classes namely high-quality and low-quality class. Three article domains (Biography, Animal, and Place) are testes with decision tree and Naïve Bayes. We found that Naïve Bayes gets high TP Rate compared to decision tree in every domain. Moreover, we found that the concept feature plays an important role in quality classification of Thai Wikipedia articles. © Springer International Publishing Switzerland 2014.},
   author = {Kanchana Saengthongpattana and Nuanwan Soonthornphisaj},
   doi = {10.1007/978-3-319-05951-8_49},
   isbn = {9783319059501},
   issn = {21945357},
   issue = {VOLUME 1},
   journal = {Advances in Intelligent Systems and Computing},
   keywords = {Concept feature,Decision tree,Naïve Bayes,Quality of Thai Wikipedia articles,Statistical feature},
   pages = {513-523},
   publisher = {Springer Verlag},
   title = {Assessing the quality of Thai Wikipedia articles using concept and statistical features},
   volume = {275 AISC},
   url = {https://www.researchgate.net/publication/290603893_Assessing_the_Quality_of_Thai_Wikipedia_Articles_Using_Concept_and_Statistical_Features},
   year = {2014},
}
@article{Su2016,
   abstract = {The great popularity of Wikipedia makes it one of the dominant knowledge source around the World. However, since one of the core principles of Wikipedia is being open for anyone to maintain it, Wikipedia cannot fully ensure the reliability of its articles, and thus sometimes suffered criticism for containing low-quality information. It is therefore essential to assess the quality of Wikipedia articles automatically. In this paper we describe how we approach that problem by using a psycho-lexical resource, i.e., the Language Inquiry and Word Count (LIWC) dictionary. By training a classifier on different LIWC categories, we discuss the implications of each category for Wikipedia quality assessment.},
   author = {Qi Su and Pengyuan Liu},
   doi = {10.1109/WI-IAT.2015.23},
   isbn = {9781467396172},
   journal = {Proceedings - 2015 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, WI-IAT 2015},
   keywords = {Information Quality,LIWC,User-generated Content,Wikipedia},
   month = {2},
   pages = {184-187},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A psycho-lexical approach to the assessment of information quality on wikipedia},
   url = {https://www.researchgate.net/publication/221023929_On_Measuring_the_Quality_of_Wikipedia_Articles},
   year = {2016},
}
@article{Dang2017,
   abstract = {Wikipedia is a great example of large scale collaboration, where people from all over the world together build the largest and maybe the most important human knowledge repository in the history. However, a number of studies showed that the quality of Wikipedia articles is not equally distributed. While many articles are of good quality, many others need to be improved. Assessing the quality of Wikipedia articles is very important for guiding readers towards articles of high quality and suggesting authors and reviewers which articles need to be improved. Due to the huge size of Wikipedia, an effective automatic assessment method to measure Wikipedia articles quality is needed. In this paper, we present an automatic assessment method of Wikipedia articles quality by analyzing their content in terms of their format features and readability scores. Our results show improvements both in terms of accuracy and information gain compared with other existing approaches.},
   author = {Quang Vinh Dang and Claudia Lavinia Ignat},
   doi = {10.1109/CIC.2016.42},
   isbn = {9781509046072},
   journal = {Proceedings - 2016 IEEE 2nd International Conference on Collaboration and Internet Computing, IEEE CIC 2016},
   month = {1},
   pages = {266-275},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Measuring quality of collaboratively edited documents: The case of Wikipedia},
   url = {https://www.semanticscholar.org/paper/Measuring-Quality-of-Collaboratively-Edited-The-of-Dang-Ignat/31f226174f2da4ceca288b93769371c388d6c4fb},
   year = {2017},
}
@article{Bassani2019,
   abstract = {With the development of Web 2.0 technologies, people have gone from being mere content users to content generators. In this context, the evaluation of the quality of (potential) information available online has become a crucial issue. Nowadays, one of the biggest online resources that users rely on as a knowledge base is Wikipedia. The collaborative aspect at the basis of Wikipedia can let to the possible creation of low-quality articles or even misinformation if the process of monitoring the generation and the revision of articles is not performed in a precise and timely way. For this reason, in this paper, the problem of automatically evaluating the quality of Wikipedia contents is considered, by proposing a supervised approach based on Machine Learning to perform the classification of articles on qualitative bases. With respect to prior literature, a wider set of features connected to Wikipedia articles has been taken into account, as well as previously unconsidered aspects connected to the generation of a labeled dataset to train the model, and the use of Gradient Boosting, which produced encouraging results.},
   author = {Elias Bassani and Marco Viviani},
   doi = {10.1145/3297280.3297357},
   isbn = {9781450359337},
   journal = {Proceedings of the ACM Symposium on Applied Computing},
   keywords = {Information Quality,Machine Learning,Social Media,Wikipedia},
   pages = {804-807},
   publisher = {Association for Computing Machinery},
   title = {Automatically assessing the quality of Wikipedia contents},
   volume = {Part F147772},
   year = {2019},
}
@article{Wang2021,
   abstract = {Wikipedia is becoming increasingly critical in helping people obtain information and knowledge. Its leading advantage is that users can not only access information but also modify it. However, this presents a challenging issue: how can we measure the quality of a Wikipedia article? The existing approaches assess Wikipedia quality by statistical models or traditional machine learning algorithms. However, their performance is not satisfactory. Moreover, most existing models fail to extract complete information from articles, which degrades the model’s performance. In this article, we first survey related works and summarise a comprehensive feature framework. Then, state-of-the-art deep learning models are introduced and applied to assess Wikipedia quality. Finally, a comparison among deep learning models and traditional machine learning models is conducted to validate the effectiveness of the proposed model. The models are compared extensively in terms of their training and classification performance. Moreover, the importance of each feature and the importance of different feature sets are analysed separately.},
   author = {Ping Wang and Xiaodan Li and Renli Wu},
   doi = {10.1177/0165551519877646},
   issn = {17416485},
   issue = {2},
   journal = {Journal of Information Science},
   keywords = {Deep learning,Wikipedia,feature framework,information quality assessment},
   month = {4},
   pages = {176-191},
   publisher = {SAGE Publications Ltd},
   title = {A deep learning-based quality assessment model of collaboratively edited documents: A case study of Wikipedia},
   volume = {47},
   url = {https://www.researchgate.net/publication/336145185_A_deep_learning-based_quality_assessment_model_of_collaboratively_edited_documents_A_case_study_of_Wikipedia},
   year = {2021},
}
@article{Velichety2019,
   abstract = {This research provides a method for quality assessment of peer-produced content in knowledge repositories using a complementary view of collaboration. Using the definition of collaboration as the action of working with someone to produce something, we identify the aspects of collaboration that the present research on online communities does not consider. To this end, we introduce and define the concept of implicit collaboration and then identify two dimensions and four possible areas of collaboration. In each area, we identify the relevant social network that captures collaboration. Using customized measures on each of the networks that capture various aspects of collaboration, we quantify the utility of implicit collaboration in assessing article quality. Experiments conducted on the complete population of graded English language Wikipedia articles show that all the identified measures improve the predictive accuracy of the existing models by 11.89 percent while improving the class-wise precision by 9-18 percent and the class-wise recall by 5-26 percent. We also find that our method complements the existing quality assessment approaches well. Our research has implications for developing automated quality assessment methods for peer-produced content using big data and social networks.},
   author = {Srikar Velichety},
   doi = {10.1145/3371041.3371045},
   issn = {00950033},
   issue = {4},
   journal = {Data Base for Advances in Information Systems},
   keywords = {Discussions,Edits,Implicit Collaboration,Social Networks,Wikipedia},
   month = {11},
   pages = {28-51},
   publisher = {Association for Computing Machinery},
   title = {Quality assessment of peer-produced content in knowledge repositories using big data and social networks: The case of implicit collaboration in wikipedia},
   volume = {50},
   year = {2019},
}
@article{Couto2021,
   abstract = {Wikipedia is an online, free, multi-language, and collaborative encyclopedia, currently one of the most significant information sources on the web the open nature of Wikipedia contributions raises concerns about the quality of its information. Previous studies have addressed this issue using manual evaluations and proposing generic measures for quality assessment. In this work, we focus on the quality of health-related content. For this purpose, we use general and health-specific features from Wikipedia articles to propose health-specific metrics. We evaluate these metrics using a set of Wikipedia articles previously assessed by WikiProject Medicine. We conclude that it is possible to combine generic and specific metrics to determine health-related content's information quality these metrics are computed automatically and can be used by curators to identify quality issues. Along with the explored features, these metrics can also be used in approaches that automatically classify the quality of Wikipedia health-related articles.},
   author = {Luís Couto and Carla Teixeira Lopes},
   doi = {10.1145/3442442.3452355},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Health-related Content,Information Quality,Wikipedia},
   month = {4},
   pages = {640-647},
   publisher = {Association for Computing Machinery, Inc},
   title = {Assessing the quality of health-related Wikipedia articles with generic and specific metrics},
   url = {https://doi.org/10.1145/3442442.3452355},
   year = {2021},
}
@article{Dang2016,
   abstract = {As Wikipedia became the largest human knowledge repository, quality measurement of its articles received a lot of attention during the last decade. Most research efforts focused on classification of Wikipedia articles quality by using a different feature set. However, so far, no 'golden feature set' was proposed. In this paper, we present a novel approach for classifying Wikipedia articles by analysing their content rather than by considering a feature set. Our approach uses recent techniques in natural language processing and deep learning, and achieved a comparable result with the state-of-the-art.},
   author = {Quang Vinh Dang and Claudia Lavinia Ignat},
   doi = {10.1145/2910896.2910917},
   isbn = {9781450342292},
   issn = {15525996},
   journal = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries},
   keywords = {Wikipedia,deep learning,document representation,feature engineering,quality assessment},
   month = {9},
   pages = {27-30},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Quality assessment of Wikipedia articles without feature engineering},
   volume = {2016-September},
   url = {https://dl.acm.org/doi/10.1145/2910896.2910917},
   year = {2016},
}
@article{Blumenstock2008,
   abstract = {Wikipedia, "the free encyclopedia", now contains over two million English articles, and is widely regarded as a high-quality, authoritative encyclopedia. Some Wikipedia articles, however, are of questionable quality, and it is not always apparent to the visitor which articles are good and which are bad. We propose a simple metric - word count - for measuring article quality. In spite of its striking simplicity, we show that this metric significantly outperforms the more complex methods described in related work.},
   author = {Joshua E. Blumenstock},
   doi = {10.1145/1367497.1367673},
   isbn = {9781605580852},
   journal = {Proceeding of the 17th International Conference on World Wide Web 2008, WWW'08},
   keywords = {Information quality,Wikipedia,Word count,word count},
   pages = {1095-1096},
   title = {Size matters: Word count as a measure of quality on Wikipedia},
   url = {https://www.researchgate.net/publication/221023915_Size_matters_Word_count_as_a_measure_of_quality_on_Wikipedia},
   year = {2008},
}
@article{Stein2007,
   abstract = {The considerable high quality of Wikipedia articles is often accredited to the large number of users who contribute to Wikipedia's encyclopedia articles, who watch articles and correct errors immediately. In this paper, we are in particular interested in a certain type of Wikipedia articles, namely, the featured articles - articles marked by a community's vote as being of outstanding quality. The German Wikipedia has the nice property that it has two types of featured articles: excellent and worth reading. We explore on the German Wikipedia whether only the mere number of contributors makes the difference or whether the high quality of featured articles results from having experienced authors contributing with a reputation for high quality contributions. Our results indicate that it does matter who contributes. Copyright 2007 ACM.},
   author = {Klaus Stein and Claudia Hess},
   doi = {10.1145/1286240.1286290},
   isbn = {1595938206},
   journal = {Hypertext 2007: Proceedings of the Eighteenth ACM Conference on Hypertext and Hypermedia, HT'07},
   keywords = {Collaborative working,Measures of quality and reputation,Statistical analysis of Wikipedia,Wiki,Wikipedia},
   pages = {171-174},
   title = {Does it matter who contributes - A study on featured articles in the german wikipedia},
   url = {https://dl.acm.org/doi/10.1145/1286240.1286290},
   year = {2007},
}
@article{,
   abstract = {The main feature of the free online-encyclopedia Wikipedia is the wiki-tool, which allows viewers to edit the articles directly in the web browser. As a weakness of this openness for example the possibility of manipulation and vandalism cannot be ruled out, so that the quality of any given Wikipedia article is not guaranteed. Hence the automatic quality assessment has been becoming a high active research field. In this paper we offer new metrics for an efficient quality measurement. The metrics are based on the lifecycles of low and high quality articles, which refer to the changes of the persistent and transient contributions throughout the entire life span. Copyright © 2009 ACM.},
   author = {Thomas Wöhner and Ralf Peters},
   doi = {10.1145/1641309.1641333},
   isbn = {9781605587301},
   journal = {Proceedings of the 5th International Symposium on Wikis and Open Collaboration, WiKiSym 2009},
   keywords = {Persistent contribution,Quality assessment,Transient contribution,Wikipedia,Wikipedia lifecycle,quality assessment,transient contribution,web-based interaction; K43 [Computers and Society]: Organizational Impacts-Computer-supported collaborative work General Terms Measurement},
   title = {Assessing the quality of Wikipedia articles with lifecycle based metrics},
   url = {https://dl.acm.org/doi/10.1145/1641309.1641333},
   year = {2009},
}
@article{Lipka2010,
   abstract = {Wikipedia provides an information quality assessment model with criteria for human peer reviewers to identify featured articles. For this classification task "Is an article featured or not?" we present a machine learning approach that exploits an article's character trigram distribution. Our approach differs from existing research in that it aims to writing style rather than evaluating meta features like the edit history. The approach is robust, straightforward to implement, and outperforms existing solutions. We underpin these claims by an experiment design where, among others, the domain transferability is analyzed. The achieved performances in terms of the F-measure for featured articles are 0.964 within a single Wikipedia domain and 0.880 in a domain transfer situation. © 2010 Copyright is held by the author/owner(s).},
   author = {Nedim Lipka and Benno Stein},
   doi = {10.1145/1772690.1772847},
   isbn = {9781605587998},
   journal = {Proceedings of the 19th International Conference on World Wide Web, WWW '10},
   keywords = {Information Quality,domain transfer,information quality,wikipedia},
   pages = {1147-1148},
   title = {Identifying featured articles in Wikipedia: Writing style matters},
   url = {https://dl.acm.org/doi/10.1145/1772690.1772847},
   year = {2010},
}
@article{,
   abstract = {This paper discusses an approach to modeling and measuring information quality of Wikipedia articles. The approach is based on the idea that the quality of Wikipedia articles with distinctly different profiles needs to be measured using different information quality models. We report on our initial study, which involved two categories of Wikipedia articles: " stabilized" (those, whose content has not undergone major changes for a significant period of time) and "controversial" (the articles, which have undergone vandalism, revert wars, or whose content is subject to internal discussions between Wikipedia editors). We present simple information quality models and compare their performance on a subset of Wikipedia articles with the information quality evaluations provided by human users. Our experiment shows, that using special-purpose models for information quality captures user sentiment about Wikipedia articles better than using a single model for both categories of articles. © 2010 ACM.},
   author = {Gabriel De La Calzada and Alex Dekhtyar},
   doi = {10.1145/1772938.1772943},
   isbn = {9781605589404},
   journal = {Proceedings of the 4th Workshop on Information Credibility, WICOW '10},
   keywords = {models,quality,wikipedia},
   pages = {11-18},
   title = {On measuring the quality of wikipedia articles},
   url = {https://dl.acm.org/doi/10.1145/1772938.1772943},
   year = {2010},
}
@article{Stvilia2005,
   abstract = {Effective information quality analysis needs powerful yet easy ways to obtain metrics. The English version of Wikipedia provides an extremely interesting yet challenging case for the study of Information Quality dynamics at both macro and micro levels. We propose seven IQ metrics which can be evaluated automatically and test the set on a representative sample of Wikipedia content. The methodology of the metrics construction and the results of tests, along with a number of statistical characterizations of Wikipedia articles, their content construction, process metadata and social context are reported.},
   author = {B Stvilia and MB Twidale and L Gasser Culture},
   journal = {World Scientific},
   title = {Information quality in a community-based encyclopedia},
   url = {https://www.worldscientific.com/doi/abs/10.1142/9789812701527_0009},
   year = {2005},
}
